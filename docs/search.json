[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accessing and Using Historical Newspaper Data",
    "section": "",
    "text": "Welcome!\nThis book is a guide to accessing and analysing newspaper data, mostly using the programming language R. I hope it is of use to people interested in working with newspaper data but a bit lost on how to get started.\nIt uses freely-available newspaper data from collections held by the British Library and digitised by the Living with Machines and Heritage Made Digital projects, and aims to focus on, through examples, the kinds of issues and questions one might have as a researcher or student working with newspapers as sources.\nThrough the book, you’ll learn about the key sources of digitised newspapers in the UK (Chapter 2), where to find and download newspaper data (Chapter 8), generate plain text files to work with (Chapter 9), and do everything from simple word statistics (Chapter 10) to building your own machine learning model in Chapter 13.\nThe book could also form the basis for a course, probably at Masters level. Each chapter ends with a few recommended readings, mostly academic papers, all focused on computational analysis of text, where possible specifically look at work done with or on newspapers. If you’d like to use this as a teacher, you are free to reuse any parts of the publication in anyway you like, to the extent to which I am entitled to grant that licence."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Accessing and Using Historical Newspaper Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI’m hugely grateful to the Living with Machines project for supporting me with a Digital Residency, which gave me the time to write up a new version of this book, and port it to the Quarto format.\nI’m also grateful to the British Library for their advice and information."
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Accessing and Using Historical Newspaper Data",
    "section": "Contact Me",
    "text": "Contact Me\nIf you spot any mistakes, would like to give me feedback, or have found any of this book useful, I’d love to hear from you. Feel free to get in touch at y.c.ryan@hum.leidenuniv.nl. You can also post an issue on the book Github repository."
  },
  {
    "objectID": "intro.html#why-newspaper-data",
    "href": "intro.html#why-newspaper-data",
    "title": "1  Introduction",
    "section": "Why Newspaper Data?",
    "text": "Why Newspaper Data?\nMore and more newspaper data is becoming available for researchers. Most news digitisation projects now carry out Optical Character Recognition and segmentation, meaning that the digitised images are processed so that the text is machine readable, and then divided into articles. It’s far from perfect, but it does generate a large amount of data: both the digitised images, and the underlying text and information about the structure.\nAll in all, it represents a very extensive source of historical text data, one which is ripe for analysis. Newspapers are particularly compelling as evidence because they can be a window into cultures and discourses which are not necessarily represented in other historical sources, such as printed books. The regularity and periodicity of newspapers mean they are a key source for studying events, trends, and patterns in history. To name a few projects, researchers have used newspaper data to understand Victorian jokes, understand the effects of industrialisation, track the meetings of radical groups, trace global information networks, and look at the history of pandemic reporting."
  },
  {
    "objectID": "intro.html#what-is-this-book",
    "href": "intro.html#what-is-this-book",
    "title": "1  Introduction",
    "section": "What is this book?",
    "text": "What is this book?\nTo a new researcher, working with newspaper data can be daunting. As well as the sheer size of the datasets, digitised newspapers are often confusingly scattered across many collections and repositories and stored in what might seem like—to a newcomer—complicated, even esoteric, formats.\nThis book aims to demystify some of these issues, and to provide a set of very practical tools and tutorials which should allow someone with little experience to work with newspaper data in a meaningful way. It will also reference many of the exemplary projects and papers which have worked with this kind of material, hopefully to serve as some kind of inspiration.\nThis book is aimed at researchers, teachers, and and other interested individuals who would like to access and analyse data from newspapers. In this book, the term newspaper data analysis is taken to mean approaches which look beyond the close reading of individual digitised newspapers, and instead look at some element of the underlying data at scale. In this book, this analysis primarily means working with the text data derived from processed newspapers and metadata from collections of newspapers held by libraries and archives, but also associated data such as press directories. Other types of newspaper data, such as images, are important but beyond the scope of this book.\n\nGoals\nIn short, this book hopes to help you:\n\nKnow what British Library newspaper data is openly available, where it is, and where to look for more coming onstream.\nUnderstand something of the XML format which make up the Library’s current crop of openly available newspapers.\nHave a process for extracting the plain text of the newspaper in a format which is easy to use.\nHave been introduced to a number of tools which are particularly useful for large-scale text mining of huge corpora: n-gram counters, topic modelling, text re-use.\nUnderstand how the tools can be used to answer some basic historical questions (whether they provide answers, I’ll leave for the reader and historian to decide)\n\n\n\nStructure\nThe book is divided into two parts, sources and methods.\n\nSources\nThe first section is a series of ‘code-free’ chapters, which aim to give an overview of available newspaper data, how to access it, and some caveats to watch out for when using it for your own research. It will give brief introductions to some tools made available by the Living with Machines project to download and work with newspapers. This section is suitable for anyone, though in some cases it will require some use of the command line.\n\n\nMethods\nThe second section is more specific: a series of tutorials using the programming language R to process and analyse newspaper data. These tutorials include examples and fully worked-out code which takes the reader from the ‘raw’ newspaper data available online, through to downloading, extracting, and analysing the text within it. These tutorials are most suited for researchers who have a little bit of programming experience, and may be useful for teachers of courses in digital humanities or data science.\nFor this section, it’ll be useful to have at least basic experience with the coding language R, and most likely, its very widely-used IDE, R-Studio. The tutorials will assume you have managed to install R and R Studio, and know how to install packages and use it for basic data wrangling. If you want to learn how to use R, R-Studio and the Tidyverse, there are lots of existing resources available. At the same time, the tutorials are entirely self-contained, and if you are careful and willing to very closely follow the steps, you should be able to make them run even without any coding experience."
  },
  {
    "objectID": "intro.html#recommended-reading",
    "href": "intro.html#recommended-reading",
    "title": "1  Introduction",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nEach chapter is accompanied by a couple of pieces of recommended reading. These are generally academic articles, related to the computation technique of the chapter. Wherever possible, they will specifically use newspaper data in their methods."
  },
  {
    "objectID": "intro.html#contribute",
    "href": "intro.html#contribute",
    "title": "1  Introduction",
    "section": "Contribute",
    "text": "Contribute\nThe book has been written using Quarto, which turns pages of code into figures and text, exposing the parts of the code when necessary, and hiding it in other cases. It lives on a GitHub repository, here: and the underlying code can be freely downloaded and re-created or altered. If you’d like to contribute, anything from a few corrections to an entire new chapter, please feel free to get in touch via the Github issues page or just fork the repository and request a merge when you’re done."
  },
  {
    "objectID": "intro.html#recommended-reading-1",
    "href": "intro.html#recommended-reading-1",
    "title": "1  Introduction",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nNicholson, Bob. “The Digital Turn: Exploring the Methodological Possibilities of Digital Newspaper Archives.” Media History 19, no. 1 (February 2013): 59–73. https://doi.org/10.1080/13688804.2012.752963."
  },
  {
    "objectID": "sources.html",
    "href": "sources.html",
    "title": "Sources",
    "section": "",
    "text": "This part of the book deals with sources - mostly explaining and accessing them."
  },
  {
    "objectID": "news-sources-uk.html#what-is-available-in-a-nutshell",
    "href": "news-sources-uk.html#what-is-available-in-a-nutshell",
    "title": "2  Accessing Newspaper Data in the UK",
    "section": "What is Available? In a Nutshell:",
    "text": "What is Available? In a Nutshell:\nMost of the available newspaper data in the UK is based on the collections of the British Library (though other libraries do hold significant collections). This is a huge collection, but getting access to it is not straightforward. In a nutshell:\n\nIf your research is more methods-based or doesn’t necessarily need full coverage, you could use the freely-available, public domain Heritage Made Digital and Living with Machines titles. These are used in the second section of the book as the basis for the coding tutorials.\nIf you would like to do analysis on something slightly more representative, you could contact Gale and ask them to supply you with the JISC Historical Newspaper Collection. Hopefully, these titles will be made freely available in the coming years.\nTo work with the largest dataset of newspapers, you would need an agreement with a commercial company, Find My Past."
  },
  {
    "objectID": "news-sources-uk.html#british-library-newspapers---the-physical-collection",
    "href": "news-sources-uk.html#british-library-newspapers---the-physical-collection",
    "title": "2  Accessing Newspaper Data in the UK",
    "section": "British Library Newspapers - The Physical Collection",
    "text": "British Library Newspapers - The Physical Collection\nThe British Library holds about 60 million issues, or 450 million pages of newspapers. They now span over 400 years, but the coverage prior to the nineteenth century is very partial (King 2007). Prior to 1800, the collection has serious gaps, and many issues of many titles have simply not survived. This means that only a fraction of what was published has been preserved or collected, and only a fraction of that which has been collected has been digitised.\nIt’s actually surprisingly difficult to know exactly what has been digitised, but a rough count is possible, using the ‘newspaper year’ as a unit. This is all the issues for one title, for one year. Its not perfect, because a newspaper-year of a weekly looks the same as a newspaper-year for a daily, but it’s an easy unit to count. There are currently about 40,000 newspaper-years on the British Newspaper Archive. The entire collection of British and Irish Newspapers is probably about 350,000 newspaper-years.\nIt’s all very well being able to access the content, but for the purposes of the kind of things we’d like to do, access to the data is needed. The following are the main British Library digitised newspaper sources. The following is organised into two time periods: before and after 1800."
  },
  {
    "objectID": "news-sources-uk.html#pre-1800",
    "href": "news-sources-uk.html#pre-1800",
    "title": "2  Accessing Newspaper Data in the UK",
    "section": "Pre-1800",
    "text": "Pre-1800\nNewspapers before 1800 are treated different to later ones and even constitute a different collection in the British Library. First of all, the collection of pre-1800 newspapers is much smaller and much less complete. This is partially because the British Library did not collect newspapers systematically until well into the nineteenth century. Therefore, even though they are out of copyright, there are no large-scale, centralised collection of early newspapers and newsbooks (their precursors).\nClose to the entirety of the surviving collection of newspapers (called newsbooks from the seventeenth century, by and large) has been digitised and made available online, through resources such as EEBO, JISC Historical Texts, and Gale Primary Sources. Many universities and libraries will provide access to these resources, allowing you to browse, search and read through the texts.\nGetting access to the underlying text data is less straightforward, however. Where there is OCR data of these titles, the age and quality of these texts mean it is not particularly suited to data analysis. Secondly, the images and texts themselves are mostly behind paywalls and there is no easy provision for bulk downloading of either. Two sources of newspapers for those interested in pre-1800 are worth mentioning: 1) the Burney Collection and 2) the Thomason Newsbook Collection.\n\nThe Thomason Newsbook Collection\nThe Thomason Tracts are a collection of pamphlets, periodicals and broadsides held at the British Library. Part of this are approximately 7,200 serials (or periodicals). These were digitised from microfilm scans in the 1990s, and made available online through the resource Early English Books Online (EEBO). A separate project the Text Creation Partnership (EEBO-TCP), created double-keyed transcriptions of many EEBO texts, but, sadly, not including serials (though it does include news pamphlets). These transcribed texts, in TEI-encoded format, are freely available online.\nSome smaller corpora of Thomason newsbooks have been created. These include the Lancaster Newsbook Corpus, a collection of several hundred manually-transcribed serials from the 1650s, and ‘George Thomason’s Newsbooks’, which is a corpus of all newsbooks printed in the year 1649, as well as the entire run of a single title, Mercurius Politicus. Neither of these may be suited to the kind of large-scale analysis proposed in this book, but they may anyway be of interest.\n\n\nBurney Collection\nThe Burney Collection contains about one million pages, from the very earliest newspapers in the seventeenth century to the beginning of the nineteenth, collected by Rev. Charles Burney in the eighteenth century, and purchased by the Library in 1818(Prescott 2018). It’s actually a mixture of both Burney’s own collection and material inserted afterwards. It was microfilmed in its entirety in the 1970s and imaged in the 90s but because of technological restrictions it wasn’t until 2007 when, with Gale, the British Library released the Burney Collection as a digital resource.\nIt’s not generally available as a data download, but the raw OCR would be of limited use anyway. Older OCR for early modern print is not very good, and it was certainly worse ten years ago when the collection was processed. The accuracy of the OCR has been measured, and Prescott (2018) found that the ocr for the Burney newspapers offered character accuracy of 75.6% and word accuracy of 65%.\nHowever, this is still a useful collection for browsing and keyword searching, which can usually be accessed through an institutional subscription to Gale, packaged as the Seventeenth and Eighteenth Century Burney Newspapers Collection."
  },
  {
    "objectID": "news-sources-uk.html#after-1800",
    "href": "news-sources-uk.html#after-1800",
    "title": "2  Accessing Newspaper Data in the UK",
    "section": "After 1800",
    "text": "After 1800\nThe British Library newspapers after 1800 is a much larger collection, and from the middle of the century, is quite comprehensive. Some more on the history of the collection is can be found in Chapter 4. A number of resources make digital surrogates of these newspapers available.\n\nJISC Newspaper digitisation projects\nMost of the academic projects in the UK which have used newspaper data, from the Political Meetings Mapper, to the Victorian Meme Machine have used the British Library’s 19th Century Newspapers collection, published by the Joint Information Systems Committee (JISC).\n\nWhat is available?\nJISC is mostly a collection of large regional titles from across the nineteenth century, which were picked for a variety of reasons, outlined below. There are titles from many major UK towns and cities. Figure Figure 2.1 charts the number of titles per decade: the coverage of the nineteenth century is more weighted towards the end, but this also reflects the increase in the newspaper collection as a whole.\n\n\n\n\n\nFigure 2.1: Very approximate chart of JISC titles, assuming that we had complete runs for all. Counted by year rather than number of pages digitised.\n\n\n\n\n\n\nHistory\nThe JISC newspaper digitisation program began in 2004, when The British Library received two million pounds from the Joint Information Systems Committee (JISC) to complete a newspaper digitisation project. A plan was made to digitise up to two million pages, across 49 titles.(King 2007) A second phase of the project digitised a further 22 titles.(Shaw 2007, 2005)\n\n\nCoverage\nThe titles cover England, Scotland, Wales and Ireland, and it should be noted that the latter is underrepresented although it was obviously an integral part of the United Kingdom at the time of the publication of these newspapers - something that’s often overlooked in projects using the JISC data. They cover about 40 cities @ref(fig:jisc-points), and are spread across 24 counties within Great Britain @ref(fig:jisc-map), plus Dublin and Belfast. To quote the then curator of the newspapers, Ed King:\n\nThe forty-eight titles chosen represent a very large cross-section of 19th century press and publishing history. Three principles guided the work of the selection panel: firstly, that newspapers from all over the UK would be represented in the database; in practice, this meant selecting a significant regional or city title, from a large number of potential candidate titles. Secondly, the whole of the nineteenth century would be covered; and thirdly, that, once a newspaper title was selected, all of the issues available at the British Library would be digitised. To maximise content, only the last timed edition was digitised. No variant editions were included. Thirdly, once a newspaper was selected, all of its run of issue would be digitised.(King 2008)\n\nFurther information on the selection process comes from Jane Shaw, who wrote in 2007 that:\n\nThe academic panel made their selection using the following eligibility criteria:\n\n\n\nTo ensure that complete runs of newspapers are scanned\nTo have the most complete date range, 1800-1900, covered by the titles selected\nTo have the greatest UK-wide coverage as possible To include the specialist area of Chartism (many of which are short runs)\nTo consider the coverage of the title: e.g., the London area; a large urban area (e.g., Birmingham); a larger regional/rural area To consider the numbers printed - a large circulation\nThe paper was successful in its time via its sales\nTo consider the different editions for dailies and weeklies and their importance for article inclusion or exclusion To consider special content, e.g., the newspaper espoused a certain political viewpoint (radical/conservative)\nThe paper was influential via its editorials. (Shaw 2007)\n\n\nThe result was a heavily curated collection, which has been scrutinised by historians (Fyfe 2016 for example).\nRecently, the Living with Machines project has done the most thorough assessment of the specific biases of the collection. Doing what has been termed an ‘environmental scan’ of the newspaper collection, and linking it to press directories containing information on geographic coverage, political leanings, and price, that team has shown that it had specific biases in some areas (Beelen et al. 2022).\nThis is all covered in lots of detail elsewhere, including some really interesting critiques of the access and so forth. Smits (2016) and Mussell (2014) both include some discussion and critique of the British Library Newspaper Collection.\nRegardless of its representativeness, it only contains a tiny fraction of the newspaper collection, and by being relevant and restricted to ‘important’ titles, it does of course miss other voices. For example, much of the Library’s collection consists of short runs, and much of it has not been microfilmed, which means it won’t have been selected for digitisation. This means that 2019 digitisation selection policies are indirectly greatly influenced by microfilm selection policies of earlier decades. Subsequent digitisation projects are trying to rectify these imbalances.\nThe map below is interactive and shows the locations of the JISC 1 and 2 collections. Clicking on a point will bring up a list of the newspapers digitised in that place; clicking on these links will bring you to the British Library’s catalogue page for that title.\n\n\n\n\nInteractive map of the JISC 1 & 2 digitised collections\n\n\n\n\nAccess\nCurrently researchers access this either through Gale, or through the British Library as an external researcher. Many researchers have requested access to the collection through Gale, which they will apparently do in exchange for a ‘cost recovery’ fee. These files were digitised using a specific format, and the text from them can be extracted using a tool developed by Living with Machines, which I’ll explore in a future chapter.\n\n\n\nBritish Newspaper Archive\nMost of the British Library’s digitised newspaper collection is available on the British Newspaper Archive (BNA). The BNA is a commercial product run by a family history company called FindMyPast. FindMyPast is now responsible for digitising large amounts of the Library’s newspapers, mostly through microfilm. As such, they have a very different focus to the JISC digitisation projects. The BNA is constantly growing, and it already dwarfs the JISC projects by number of pages: the BNA currently hosts nearly 70 million pages, against the 3 million or so of the two JISC projects. A recent project, thanks to an agreement between the supplier and the British Library, means that one million pages per year are being made freely available to read through the platform, if you register for a free account.\n\nCoverage\nThe BNA collection is very large, and is particularly focused on local newspapers. At time of writing, the BNA website (linked above) contains over 68 million pages, with more added on a weekly basis.\nThe titles it contains span the entire nineteenth century, peaking from 1850 - 1900. Figure Figure 2.2 charts the volume of material on the British Newspaper Archive, per year. It is derived from the metadata rather than the actual volume of data. The ‘unit’ is the newspaper year, meaning each year of a newspaper’s run is counted as a single data point. It doesn’t account for weekly versus daily titles, or the number of pages, but it gives an idea of the temporal shape of the coverage.\n\n\n\n\n\nFigure 2.2: Newspaper-years on the British Newspaper Archive. Note that this includes the JISC content too\n\n\n\n\nGeographically, the BNA collection is also widely spread. The most important distinction between it and JISC is that the BNA covers a much wider range of smaller, local titles. Figure Figure 2.3 maps all the titles digitised and available on the British Newspaper Archive. In this case, clicking a link will bring you to the page for that title on the resource. You’ll need a subscription (or a free account if they happen to be free-to-view titles) to view it in its entirety.\nThe map is not complete - titles from Ireland are not shown at the moment because of an issue with the metadata, but will be added in a later update. However the map gives a good sense of the geographic coverage, which is particularly clustered in parts of the Midlands and North, as well as London. Many smaller villages and towns are represented. Coverage in Wales is still patchy, but most big population centres are represented.\n\n\n\n\n\n\nFigure 2.3: Newspaper-years on the British Newspaper Archive. Note that this includes the JISC content too\n\n\n\n\n\nAccess\nThe BNA’s primary focus is on providing search access to individual researchers. Sadly, there is little provision made for data access, or even institutional subscriptions for researchers. There are some exceptions: the Bristol N-Gram and named entity datasets project used the BNA data, processing the top phrases and words from about 16 million pages. The collection has tripled in size since then: it’s likely that were it to be run again the results would be different.\nThe Living with Machines project negotiated a full data transfer directly from Find My Past (the owners of the British Newspaper Archive) for the first time. The story of this is written in some detail in the book Collaborative Historical Research in the Age of Big Data (Ahnert et al. 2023). In a nutshell, because of copyright issues, the project had to deal directly with Find My Past, and there were a set of detailed restrictions on what could be done with the data, such as having to keep it in secure storage, and delete it after two years.\nThat is all to say that working with ‘all’ the UK’s available digitised newspaper data is probably not feasible for most individual researchers or even small or medium-sized research projects, at the present time. While hopefully this will change in the future, what it means for now is that most data-driven historical work carried out on newspapers has used the JISC data, rather than the much larger BNA collection, because of relative ease-of-access.\n\n\n\nBritish Library Open Research Repository (bl.iro.bl.uk)\nHowever, there is some positive news in accessing UK newspaper data, thanks to a new resource which has become available in the last couple of years: the Shared Research Repository, a resource maintained by the British Library. Newspaper digitisation projects carried out internally by the British Library do not have the same copyright or commercial restrictions as JISC and the BNA, meaning in theory the data from them can be opely shared with no license restrictions. Two recent projects, Living with Machines and Heritage Made Digital, have carried out digitisation in this way, and so the data can be much more easily and openly shared. It is still a tiny collection in comparison to the entire BNA, but it is a step in the right direction.\nThe BL open repository is a centralised repository for all sorts of research outputs and data, and the Library has been using it as a storage space for newspaper data from various newspaper digitisation projects. The text data (not the images) have been made freely available, with no license restrictions, meaning they can be used for any purpose whatsoever.\n\nCoverage\nAs of publication, a total of 57 titles, or 437 ‘newspaper years’ are currently available.\nThese titles currently come from two sources:\n\nThe Living with Machines project\nHeritage Made Digital\n\nBecause the information on the motivations behind these projects is somewhat new, more transparent, and easier to access, I’ll explain them in a bit more detail.\n\n\nLiving with Machines\nThe Living with Machines project, a collaboration between the Alan Turing Institute and the British Library (as well as other partners), has digitised and made available a new selection of newspapers. According to that project, the aim of the digitisation was research-led rather than curatorial, meaning they chose their titles according to flexible research needs. According to the project, the key aims for the digitisation were to respond to specific research questions, and to rebalance the bias in the existing newspaper corpus (Tolfo et al. 2022). The eventual selections were a mix of research interests and ‘practical factors’.\nTo help with choosing the most useful and optimal set of newspapers to digitise with limited resources, the team developed a number of tools, including a user interface called Press Picker, as well as digitising a set of press directories, which hold information on the newspapers which could then be used in making choices.\nThe project’s historical research interest focused on industrialisation and its impact, and as a result many of the titles digitised are specifically related to that subject, for example by the topic or area of coverage of the newspaper.\n\n\nHeritage Made Digital\nHeritage Made Digital was a project within the British Library to digitise up to 1.3 million pages of 19th century newspapers. It has a specific curatorial focus: it picked titles which are completely out of copyright, which means that they all finished publication before 1879. It also had preservation aims: because of this, it chose titles which were not on microfilm, and were also in poor or unfit condition. Newspaper volumes in unfit condition cannot be called up by readers: this meant that if a volume is not microfilmed and is in this state, it can’t be read by anyone.\nThe other curatorial goal was to focus on ‘national’ titles. In practice this meant choosing titles printed in London, but without a specific London focus. The JISC digitisation projects focused on regional titles, then local, and all the ‘big’ nationals like the Times or the Manchester Guardian have been digitised by their current owners. This means that a group of historically important titles may have fallen through the cracks, and this projects is digitising some of those.1\nAs can be seen in Figure Figure 2.4, the years digitised so far cover most of the century, with the Living with Machines titles focused on the second half, and Heritage Made Digital slightly earlier.\n\n\n\n\n\nFigure 2.4: Time-series graph of the newspapers digitised by Living with Machines and Heritage Made Digital, and made available on the Open Research Repository.\n\n\n\n\nTaken together, the collection spans much of the UK, with an emphasis on London and industrial areas, because of the focus of the projects.\nThis map in Figure Figure 2.5 displays them. A few are currently missing from the map because the relevant ID numbers to link them to the title list and ultimately, geographic coordinates, were not found. The links in the pop-up take you to the download page for that title on the Open Research Repository.\n\n\n\n\n\nFigure 2.5: Newspaper-years on the British Newspaper Archive. Note that this includes the JISC content too.\n\n\n\n\n\nAccess\nThe good news is that as these newspapers are deemed out of copyright, the text data can be made freely downloadable. Currently the first batch of newspapers, in METS/ALTO format, are available on the British Library’s Open Repository. They have a CC-0 licence, which means they can be used for any purpose whatsoever. The code examples in the following chapters will use this data, and chapter six will show you how to download the titles using a bulk downloader, instead of going through them one-by-one.\nThe titles are primarily available as individual downloads through the web interface of the repository. Downloading them is as simple as clicking a link to a .zip file. The easiest way to find all of them is to use the collections organisational system of the repository. The collection British Library News Datasets contains a number of newspaper datasets, including metadata and newspapers. The newspapers themselves are contained with the sub-collection Newspapers. If you click on this second link, you’ll see a list of datasets. Each dataset corresponds to a single title (actually, often a group of titles if they changed name or merged). Clicking on a title will display a page containing links to .zip files, one for each year of the title. Note that there may be multiple pages.\nDownloading the data one year of a title at a time can be a time-consuming process. In (Chapter 8), I’ll demonstrate some methods for downloading titles in bulk.\n\n\n\nOther data sources\nAs a final note, there are also a number of really useful metadata files available through the British Library research repository. Firstly, two title-level lists of all British and Irish Newspapers held by the library: one for Britain and Ireland here, and an updated, world-wide one, here. A later chapter Chapter 7 describes these in some more detail.\nMost recently, the Living with Machines project has published two digitised press directories, available here and here, plus a dataset of extracted structured data. All of these are have a Public Domain licence."
  },
  {
    "objectID": "news-sources-uk.html#recommended-reading",
    "href": "news-sources-uk.html#recommended-reading",
    "title": "2  Accessing Newspaper Data in the UK",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nBeelen, Kaspar, Jon Lawrence, Daniel C S Wilson, and David Beavan. “Bias and Representativeness in Digitized Newspaper Collections: Introducing the Environmental Scan.” Digital Scholarship in the Humanities 38, no. 1 (April 3, 2023): 1–22. https://doi.org/10.1093/llc/fqac037.\nFyfe, Paul. “An Archaeology of Victorian Newspapers.” Victorian Periodicals Review 49, no. 4 (2016): 546–77. https://www.jstor.org/stable/26166577.\n\n\n\n\nAhnert, Ruth, Emma Griffin, Mia Ridge, and Giorgia Tolfo. 2023. “Collaborative Historical Research in the Age of Big Data,” January. https://doi.org/10.1017/9781009175548.\n\n\nBeelen, Kaspar, Jon Lawrence, Daniel C S Wilson, and David Beavan. 2022. “Bias and Representativeness in Digitized Newspaper Collections: Introducing the Environmental Scan.” Digital Scholarship in the Humanities 38 (1): 1–22. https://doi.org/10.1093/llc/fqac037.\n\n\nFyfe, Paul. 2016. “An Archaeology of Victorian Newspapers.” Victorian Periodicals Review 49 (4): 546–77. https://doi.org/10.1353/vpr.2016.0039.\n\n\nKing, Ed. 2007. “Digitisation of British Newspapers 1800-1900.” https://www.gale.com/intl/essays/ed-king-digitisation-of-british-newspapers-1800-1900.\n\n\n———. 2008. “British Library Digitisation: Access and Copyright.”\n\n\nMussell, James. 2014. “Elemental Forms: Elemental Forms: The Newspaper as Popular Genre in the Nineteenth Century.” Media History 20 (1): 4–20. https://doi.org/10.1080/13688804.2014.880264.\n\n\nPrescott, Andrew. 2018. “Travelling Chronicles: News and Newspapers from the Early Modern Period to the Eighteenth Century.” In, edited by Siv Gøril Brandtzæg, Paul Goring, and Christine Watson, 51–71. Leiden, The Netherlands: Brill.\n\n\nShaw, Jane. 2005. “10 Billion Words: The British Library British Newspapers 1800-1900 Project: Some Guidelines for Large-Scale Newspaper Digitisation.” https://archive.ifla.org/IV/ifla71/papers/154e-Shaw.pdf.\n\n\n———. 2007. “Selection of Newspapers.” British Library Newspapers. https://www.gale.com/intl/essays/jane-shaw-selection-of-newspapers.\n\n\nSmits, Thomas. 2016. “Making the News National: Using Digitized Newspapers to Study the Distribution of the Queen’s Speech by W. H. Smith & Son, 1846–1858.” Victorian Periodicals Review 49 (4): 598–625. https://doi.org/10.1353/vpr.2016.0041.\n\n\nTolfo, Giorgia, Olivia Vane, Kaspar Beelen, Kasra Hosseini, Jon Lawrence, David Beavan, and Katherine McDonough. 2022. “Hunting for Treasure: Living with Machines and the British Library Newspaper Collection.” In, 23–46. De Gruyter. https://doi.org/10.1515/9783110729214-002."
  },
  {
    "objectID": "news-sources-uk.html#footnotes",
    "href": "news-sources-uk.html#footnotes",
    "title": "2  Accessing Newspaper Data in the UK",
    "section": "",
    "text": "The term national is debatable, but it’s been used to try and distinguish from titles which clearly had a focus on one region. Even this is difficult: regionals would have often had a national focus, and were in any case reprinting many national stories. But their audience would have been primarily in a limited geographical area, unlike many London-based titles, which were printed and sent out across the country, first by train, then the stories themselves by telegraph.↩︎"
  },
  {
    "objectID": "news-sources-int.html#united-states",
    "href": "news-sources-int.html#united-states",
    "title": "3  Accessing Newspaper Data Internationally",
    "section": "United States",
    "text": "United States\nThe Library of Congress in the U.S. sponsored a project called ‘Chronicling America’, which has created a newspaper dataset and interface which currently has about 16 million pages. All the titles are freely available through the website without a paywall. To access the data itself, the CA database has an API, with instructions here. As with the UK titles, Chronicling America newspapers use the METS/ALTO format. However it may be in a slightly different format and require adjusting the method by which you extract the text from the .xml files.\nYou can also download all the OCR results directly for each title on this page. Each newspaper contains a list of folders for each issue, and within that can be found a single file for the OCR results (ocr.xml) and a single file for the plain text (ocr.txt).\nChronicling America publish a simple tab-separated-values list of the titles currently in the database here: https://chroniclingamerica.loc.gov/newspapers.txt\nWe can easily use this information to produce a State-level map of the newspaper titles:\n\n\n\n\nInteractive map of Library of Congress Newspapers"
  },
  {
    "objectID": "news-sources-int.html#australia",
    "href": "news-sources-int.html#australia",
    "title": "3  Accessing Newspaper Data Internationally",
    "section": "Australia",
    "text": "Australia\nTrove is a centralised data store of cultural heritage items from the National Library of Australia and other partners. Newspapers published between 1803 and 1955 are freely available through Trove. As well as browsing and searching, Trove has an API which allows you to download newspaper text and image data, and associated metadata. See the documentation for more details. Tim Sherratt publishes a large number of guides to using Trove, including live code tutorials."
  },
  {
    "objectID": "news-sources-int.html#the-netherlands",
    "href": "news-sources-int.html#the-netherlands",
    "title": "3  Accessing Newspaper Data Internationally",
    "section": "The Netherlands",
    "text": "The Netherlands\nHistorical newspapers in the Netherlands are available through a resource and interface called Delpher, maintained by the Koninklijke Bibliotheek, the Dutch Royal (e.g. National) Library. As well as browsing/searching, users can download in bulk all newspapers published between 1618 and 1879. A full list of the available titles can be found here.\nNewspaper data is available in a series of .zip files, and is published in METS/ALTO format. Page scans are not included, but can be retrieved manually from the web using a unique identifier and a URL. A full description of the data format and structure is available on the Delpher website. There is also an API available: users need to apply directly to Delpher for access. See here."
  },
  {
    "objectID": "news-sources-int.html#finland",
    "href": "news-sources-int.html#finland",
    "title": "3  Accessing Newspaper Data Internationally",
    "section": "Finland",
    "text": "Finland\nMany of Finland’s newspapers have been digitised, and are available through a standard web interface. All the OCR results (not images) of newspapers published between 1771 and 1874 are available as a single bulk download through the Language Bank of Finland. The file is 13GB and the newspaper OCR results are in METS/ALTO format."
  },
  {
    "objectID": "news-sources-int.html#luxembourg",
    "href": "news-sources-int.html#luxembourg",
    "title": "3  Accessing Newspaper Data Internationally",
    "section": "Luxembourg",
    "text": "Luxembourg\nLuxembourg have digitised and made available about 800,000 pages of digitised newspapers, and made them available through the National Library’s Open Data service. The data is presented in a number of different ‘packs’, each made with different user needs in mind. The data is in METS/ALTO format. The website contains extensive documetation on the format used, and a tool for processing the files can be found on the organisation’s Github page.\nHelpfully, they also make available a ‘text analysis pack’ where the plain text has been extracted from the METS/ALTO and made available either in a series of simplified .xml files, or as a .json file with one line per article."
  },
  {
    "objectID": "news-sources-int.html#pan-european-collections",
    "href": "news-sources-int.html#pan-european-collections",
    "title": "3  Accessing Newspaper Data Internationally",
    "section": "Pan-European Collections",
    "text": "Pan-European Collections\nA number of European projects have worked to make newspapers from multiple countries available through a single repository and interface. Europeana Newspapers makes available, for browsing and keyword searching, about 20 million pages of newspapers from 18 partner libaries. You can view the final report, including links to tools and further information, here.\nAnother project worth mentioning is Impresso.. Impresso is a database and interface combining newspapers from multiple European countries. The data is not all freely available, but the interface allows for a number of text analysis tasks (such as topic modelling and text reuse) to be carried out on a large corpus, once a non-disclosure agreement has been signed. Users can also create a search query and export the resulting articles as a dataset. A new version is currently in the pipeline."
  },
  {
    "objectID": "ocr.html#conclusion",
    "href": "ocr.html#conclusion",
    "title": "4  History of the British Library Collection",
    "section": "Conclusion",
    "text": "Conclusion\nAs this is meant as a practical guide,it is worth reflecting on how this may effect your own experience of working with newspaper data. First of all, the complex history of the collection means that depending on the time period and the type of material, it may be available from very different sources, or in very different formats. It is possible that material of interest is spread across multiple collections, and that some of it has been digitised and others not, often for arbitrary reasons. A second takeaway is that these collection, microfilm, and digitisation practices have a huge effect on the resulting newspaper data available today, and are worth understanding before undertaking any research project which hopes to use newspaper data in any kind of representative way."
  },
  {
    "objectID": "ocr.html#recommended-reading",
    "href": "ocr.html#recommended-reading",
    "title": "4  History of the British Library Collection",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nTorget, Andrew J. “Mapping Texts: Examining the Effects of OCR Noise on Historical Newspaper Collections.” In Digitised Newspapers – A New Eldorado for Historians?, edited by Estelle Bunout, Maud Ehrmann, and Frédéric Clavert, 47–66. De Gruyter, 2022. https://doi.org/10.1515/9783110729214-003.\nVan Strien, Daniel, Kaspar Beelen, Mariona Ardanuy, Kasra Hosseini, Barbara McGillivray, and Giovanni Colavizza. “Assessing the Impact of OCR Quality on Downstream NLP Tasks:” In Proceedings of the 12th International Conference on Agents and Artificial Intelligence, 484–96. Valletta, Malta: SCITEPRESS - Science and Technology Publications, 2020. https://doi.org/10.5220/0009169004840496.\nWhy You (A Humanist) Should Care About Optical Character Recognition\n\n\n\n\nFyfe, Paul. 2016. “An Archaeology of Victorian Newspapers.” Victorian Periodicals Review 49 (4): 546–77. https://doi.org/10.1353/vpr.2016.0039.\n\n\nHarris, P. R. 1998. A History of the British Museum Library, 1753-1973. London: British Library.\n\n\nPrescott, Andrew. 2018. “Travelling Chronicles: News and Newspapers from the Early Modern Period to the Eighteenth Century.” In, edited by Siv Gøril Brandtzæg, Paul Goring, and Christine Watson, 51–71. Leiden, The Netherlands: Brill."
  },
  {
    "objectID": "mets-alto.html#how-to-work-with-these",
    "href": "mets-alto.html#how-to-work-with-these",
    "title": "5  Working with METS/ALTO",
    "section": "How to work with these",
    "text": "How to work with these\nThe structure of these files can be daunting, and it’s true that it is not straightforward to extract the text in any meaningful way. In many cases, you’ll find that others have already created plain-text versions of newspapers, or tools to make them yourself, such as the resources created by the National Library of Luxembourg, the Impresso project, and Living with Machines. The latter, alto2text, is a command line tool which converts METS/ALTO into plain text.\nThe full documentation for alto2text is available via Github. It uses the python programming language, and expects the newspapers the folder format in which they can be downloaded from the Shared Research Repository.\n\nUsing R\nIn Chapter 9, you’ll find a series of steps in order to extract text from METS/ALTO. It’s much slower and a little clunkier than the method above, but it may be an alternative if you want to do everything within a single coding language."
  },
  {
    "objectID": "mets-alto.html#recommended-reading",
    "href": "mets-alto.html#recommended-reading",
    "title": "5  Working with METS/ALTO",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nMaking sense of the METS and ALTO XML standards"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "This part of the book is a series of practical tutorials, written in R, which teach how to download, extract, and analyse newspaper data, using openly available newspaper data from the Shared Research Repository, digitised by Heritage Made Digital and Living with Machines."
  },
  {
    "objectID": "using-r-tidyverse.html#getting-started",
    "href": "using-r-tidyverse.html#getting-started",
    "title": "6  Using R and the tidyverse",
    "section": "Getting started",
    "text": "Getting started\nThe only requirements to get through these tutorials are to install R and R-Studio, as well as some data which needs to be downloaded separately.\n\nDownload R and R-Studio\nR and R-Studio are two separate things. R will work without R-studio, but not the other way around, and so it should be downloaded first. Go to the download page, select a download mirror, and download the correct version for your operating system. Follow the installation instructions if you get stuck.\nNext, download R-Studio. You’ll want the desktop version and again, download the correct version and follow the instructions. When both of these are installed, open R-Studio, and it should run the underlying software R automatically.\nAt this point, I would highly recommend reading a beginners guide to R and R-studio, such as this one, to familiarise yourself with the layout and some of the basic functionality of R-Studio. Once you understand where to type and save code, where your files and dataframes live, and how to import data from spreadsheets, you should be good to start experimenting with newspaper data.\nR relies on lots of additional packages for full functionality, and you’ll need to install these by using the function install.packages(), followed by the package name, in inverted commas. I recommend doing this to install the tidyverse suite of packages by running install.packages('tidyverse') in the Console window (the bottom-left of the screen in R-Studio) as you’ll end up using this all the time."
  },
  {
    "objectID": "using-r-tidyverse.html#using-r",
    "href": "using-r-tidyverse.html#using-r",
    "title": "6  Using R and the tidyverse",
    "section": "Using R",
    "text": "Using R\n\n‘Base’ R.\nCommands using R without needing any additional packages are often called ‘base’ R. Here are some important ones to know:\nYou can assign a value to an object using = or &lt;-:\n\nx = 1\n\ny &lt;- 4\n\nEntering the name of a variable in the console and pressing return will return that value in the console. The same will happen if you enter it in a notebook cell (like here below), and run the cell. This is also true of any R object, such as a dataframe, vector, or list.\n\ny\n\n[1] 4\n\n\nYou can do basic calculations with +, -, * and /:\n\nx = 1+1\n\ny = 4 - 2\n\nz = x * y\n\nz\n\n[1] 4\n\n\nYou can compare numbers or variables using == (equals), &gt; (greater than), &lt;, (less than) != (not equal to). These return either TRUE or FALSE:\n\n1 == 1\n\n[1] TRUE\n\nx &gt; y\n\n[1] FALSE\n\nx != z\n\n[1] TRUE\n\n\n\n\nBasic R data structures\nIt is worth understanding the main types of data that you’ll come across, in your environment window.\nA variable is a piece of data stored with a name, which can then be used for various purposes. The simplest of these are single elements, such as a number:\n\nx = 1\n\nx\n\n[1] 1\n\n\nNext is a vector. A vector is a list of elements. A vector is created with the command c(), with each item in the vector placed between the brackets, and followed by a comma. If your vector is a vector of words, the words need to be in inverted commas or quotation marks.\n\nfruit = c(\"apples\", \"bananas\", \"oranges\", \"apples\")\ncolour = c(\"green\", \"yellow\", \"orange\", \"red\")\namount = c(2,5,10,8)\n\nNext are dataframes. These are the spreadsheet-like objects, with rows and columns, which you’ll use in most analyses.\nYou can create a dataframe using the data.frame() command. You just need to pass the function each of your vectors, which will become your columns.\nWe can also use the glimpse() or str() commands to view some basic information on the dataframe (particularly useful with longer data).\n\nfruit_data = data.frame(fruit, colour, amount, stringsAsFactors = FALSE)\n\nstr(fruit_data)\n\n'data.frame':   4 obs. of  3 variables:\n $ fruit : chr  \"apples\" \"bananas\" \"oranges\" \"apples\"\n $ colour: chr  \"green\" \"yellow\" \"orange\" \"red\"\n $ amount: num  2 5 10 8\n\n\n\n\nData types\nNotice that to the right of the third column, the amount, has &lt;dbl&gt;under it, whereas the other two have `. That’s because R is treating the third as a number and others as a string of characters. It’s often important to know which data type your data is in: you can’t do arithmetic on characters, for example. R has 6 data types:\n\ncharacter\nnumeric (real or decimal)\ninteger\nlogical\ncomplex\nRaw\n\nThe most commonly-used ones you’ll come across are character, numeric, and logical. logical is data which is either TRUE or FALSE. In R, all the items in a vector are coerced to the same type. So if you try to make a vector with a combination of numbers and strings, the numbers will be converted to strings, as in the example below:\n\nfruit = c(\"apples\", 5, \"oranges\", 3)\n\nstr(fruit)\n\n chr [1:4] \"apples\" \"5\" \"oranges\" \"3\"\n\n\n\n\nInstalling and loading packages:\nR is extended through the use of ‘packages’: pre-made sets of functions, usually with a particular task or theme in mind. To work with networks, for example, we’ll use a set of third-party packages. If you complete the exercises using the CSC cloud notebooks, these are already installed for you in most cases. To install a package, use the command install.packages(), and include the package name within quotation marks:\n\ninstall.packages('igraph')\n\nTo load a package, use the command library(). This time, the package name is not within quotation marks\n\nlibrary(igraph)\n\nWarning: package 'igraph' was built under R version 4.0.5\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union"
  },
  {
    "objectID": "using-r-tidyverse.html#tidyverse",
    "href": "using-r-tidyverse.html#tidyverse",
    "title": "6  Using R and the tidyverse",
    "section": "Tidyverse",
    "text": "Tidyverse\nMost of the work in these notebooks is done using a set of packages developed for R called the ‘tidyverse’. These enhance and improve a large range of R functions, with a more intuitive nicer syntax. It’s really a bunch of individual packages for sorting, filtering and plotting data frames. They can be divided into a number of different categories.\nAll these functions work in the same way. The first argument is the thing you want to operate on. This is nearly always a data frame. After come other arguments, which are often specific columns, or certain variables you want to do something with.\n\nlibrary(tidyverse)\n\nHere are a couple of the most important ones\n\nselect(), pull()\nselect() allows you to select columns. You can use names or numbers to pick the columns, and you can use a - sign to select everything but a given column.\nUsing the fruit data frame we created above: We can select just the fruit and colour columns:\n\nselect(fruit_data, fruit, colour)\n\n    fruit colour\n1  apples  green\n2 bananas yellow\n3 oranges orange\n4  apples    red\n\n\nSelect everything but the colour column:\n\nselect(fruit_data, -colour)\n\n    fruit amount\n1  apples      2\n2 bananas      5\n3 oranges     10\n4  apples      8\n\n\nSelect the first two columns:\n\nselect(fruit_data, 1:2)\n\n    fruit colour\n1  apples  green\n2 bananas yellow\n3 oranges orange\n4  apples    red\n\n\n\n\ngroup_by(), tally(), summarise()\nThe next group of functions group things together and count them. Sounds boring but you would be amazed by how much of data science just seems to be doing those two things in various combinations.\ngroup_by() puts rows with the same value in a column of your dataframe into a group. Once they’re in a group, you can count them or summarise them by another variable.\nFirst you need to create a new dataframe with the grouped fruit.\n\ngrouped_fruit = group_by(fruit_data, fruit)\n\nNext we use tally(). This counts all the instances of each fruit group.\n\ntally(grouped_fruit)\n\n# A tibble: 3 × 2\n  fruit       n\n  &lt;chr&gt;   &lt;int&gt;\n1 apples      2\n2 bananas     1\n3 oranges     1\n\n\nSee? Now the apples are grouped together rather than being two separate rows, and there’s a new column called n, which contains the result of the count.\nIf we specify that we want to count by something else, we can add that in as a ‘weight’, by adding wt = as an argument in the function.\n\ntally(grouped_fruit, wt = amount)\n\n# A tibble: 3 × 2\n  fruit       n\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apples     10\n2 bananas     5\n3 oranges    10\n\n\nThat counts the amounts of each fruit, ignoring the colour.\n\n\nfilter()\nAnother quite obviously useful function. This filters the dataframe based on a condition which you set within the function. The first argument is the data to be filtered. The second is a condition (or multiple condition). The function will return every row where that condition is true.\nJust red fruit:\n\nfilter(fruit_data, colour == 'red')\n\n   fruit colour amount\n1 apples    red      8\n\n\nJust fruit with at least 5 pieces:\n\nfilter(fruit_data, amount &gt;=5)\n\n    fruit colour amount\n1 bananas yellow      5\n2 oranges orange     10\n3  apples    red      8\n\n\nYou can also filter with multiple terms by using a vector (as above), and the special command %in%:\n\nfilter(fruit_data, colour %in% c('red', 'green'))\n\n   fruit colour amount\n1 apples  green      2\n2 apples    red      8\n\n\n\n\nslice_max(), slice_min()\nThese functions return the top or bottom number of rows, ordered by the data in a particular column.\n\nfruit_data %&gt;% slice_max(order_by = amount, n = 1)\n\n    fruit colour amount\n1 oranges orange     10\n\nfruit_data %&gt;% slice_min(order_by = amount, n = 1)\n\n   fruit colour amount\n1 apples  green      2\n\n\nThese can also be used with group_by(), to give the top rows for each group:\n\nfruit_data %&gt;% group_by(fruit) %&gt;% slice_max(order_by = amount, n  =  1)\n\n# A tibble: 3 × 3\n# Groups:   fruit [3]\n  fruit   colour amount\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 apples  red         8\n2 bananas yellow      5\n3 oranges orange     10\n\n\nNotice it has kept only one row per fruit type, meaning it has kept only the apple row with the highest amount?\n\n\nsort(), arrange()\nAnother useful set of functions, often you want to sort things. The function arrange() does this very nicely. You specify the data frame, and the variable you would like to sort by.\n\narrange(fruit_data, amount)\n\n    fruit colour amount\n1  apples  green      2\n2 bananas yellow      5\n3  apples    red      8\n4 oranges orange     10\n\n\nSorting is ascending by default, but you can specify descending using desc():\n\narrange(fruit_data, desc(amount))\n\n    fruit colour amount\n1 oranges orange     10\n2  apples    red      8\n3 bananas yellow      5\n4  apples  green      2\n\n\nIf you `sortarrange() by a list of characters, you’ll get alphabetical order:\n\narrange(fruit_data, fruit)\n\n    fruit colour amount\n1  apples  green      2\n2  apples    red      8\n3 bananas yellow      5\n4 oranges orange     10\n\n\nYou can sort by multiple things:\n\narrange(fruit_data, fruit, desc(amount))\n\n    fruit colour amount\n1  apples    red      8\n2  apples  green      2\n3 bananas yellow      5\n4 oranges orange     10\n\n\nNotice that now red apples are first.\n\n\nleft_join(), inner_join(), anti_join()\nAnother set of commands we’ll use quite often in this course are the join() ‘family’. Joins are a very powerful but simple way of selecting certain subsets of data, and adding information from multiple tables together.\nLet’s make a second table of information giving the delivery day for each fruit type:\n\nfruit_type = c('apples', 'bananas','oranges')\nweekday = c('Monday', 'Wednesday', 'Friday')\n\nfruit_days = data.frame(fruit_type, weekday, stringsAsFactors = FALSE)\n\nfruit_days\n\n  fruit_type   weekday\n1     apples    Monday\n2    bananas Wednesday\n3    oranges    Friday\n\n\nThis can be ‘joined’ to the fruit information, to add the new data on the delivery day, without having to edit the original table (or repeat the information for apples twice). This is done using left_join.\nJoins need a common key, a column which allows the join to match the data tables up. It’s important that these are unique (a person’s name makes a bad key by itself, for example, because it’s likely more than one person will share the same name). Usually, we use codes as the join keys. If the columns containing the join keys have different names (as ours do), specify them using the syntax below:\n\njoined_fruit = left_join(fruit_data, fruit_days, by = c(\"fruit\" = \"fruit_type\"))\n\njoined_fruit\n\n    fruit colour amount   weekday\n1  apples  green      2    Monday\n2 bananas yellow      5 Wednesday\n3 oranges orange     10    Friday\n4  apples    red      8    Monday\n\n\nIn this new dataframe, the correct weekday is now listed beside the relevant fruit type.\n\n\nPiping\nAnother useful feature of the tidyverse is that you can ‘pipe’ commands through a bunch of functions, making it easier to follow the logical order of the code. This means that you can do one operation, and pass the result to another operation. The previous dataframe is passed as the first argument of the next function by using the pipe %&gt;% command. It works like this:\n\nfruit_data %&gt;% \n  filter(colour != 'yellow') %&gt;% # remove any yellow colour fruit\n  group_by(fruit) %&gt;% # group the fruit by type\n  tally(amount) %&gt;% # count each group\n  arrange(desc(n)) # arrange in descending order of the count\n\n# A tibble: 2 × 2\n  fruit       n\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apples     10\n2 oranges    10\n\n\nThat code block, written in prose: “take fruit data, remove any yellow colour fruit, count the fruits by type and amount, and arrange in descending order of the total”\n\n\nPlotting using ggplot()\nThe tidyverse includes a plotting library called ggplot2. To use it, first use the function ggplot() and specify the dataset you wish to graph using data =. Next, add what is known as a ‘geom’: a function which tells the package to represent the data using a particular geometric form (such as a bar, or a line). These functions begin with the standard form geom_.\nWithin this geom, you’ll add ‘aesthetics’, which specify to the package which part of the data needs to be mapped to which particular element of the geom. The most common ones include x and y for the x and y axes, color or fill to map colors in your plot to particular data.\nggplot is an advanced package with many options and extensions, which cannot be covered here.\nSome examples using the fruit data:\nBar chart of different types of fruit (one each of bananas and oranges, two types of apple)\n\nggplot(data = fruit_data) + geom_col(aes(x = fruit, y = amount))\n\n\n\n\nCounting the total amount of fruit:\n\nggplot(fruit_data) + geom_col(aes(x = fruit, y = amount))\n\n\n\n\nCharting amounts and fruit colours:\n\nggplot(data = fruit_data) + geom_bar(aes(x = fruit, weight = amount, fill = colour))"
  },
  {
    "objectID": "using-r-tidyverse.html#reading-in-external-data",
    "href": "using-r-tidyverse.html#reading-in-external-data",
    "title": "6  Using R and the tidyverse",
    "section": "Reading in external data",
    "text": "Reading in external data\nMost of the time, you’ll be working with external data sources. These most commonly come in the form of comma separated values (.csv) or tab separated values (.tsv). The tidyverse commands to read these are read_csv() and read_tsv. You can also use read_delim(), and specify the type of delimited using delim = ',' or delim = '/t. The path to the file is given as a string to the argument file=.\n\ndf = read_csv(file = 'aus_titles.csv') # Read a .csv file as a network, specify the path to the file here.\n\nRows: 2137 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): newspaper_title, state, place_id, place\ndbl (3): title_id, latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf\n\n# A tibble: 2,137 × 7\n   title_id newspaper_title              state place_id place latitude longitude\n      &lt;dbl&gt; &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1      984 Adelaide Chronicle and Sout… SA    SA00558… Adel…    -34.9      139.\n 2      986 Adelaide Chronicle and Sout… SA    SA00558… Adel…    -34.9      139.\n 3      174 Adelaide Morning Chronicle … SA    SA00558… Adel…    -34.9      139.\n 4      821 Adelaide Observer (SA : 184… SA    SA00558… Adel…    -34.9      139.\n 5     1100 Adelaide Times (SA : 1848 -… SA    SA00558… Adel…    -34.9      139.\n 6      277 Adelaider Deutsche Zeitung … SA    SA00558… Adel…    -34.9      139.\n 7      434 Adelong and Tumut Express a… NSW   NSW81112 Adel…    -35.3      148.\n 8      434 Adelong and Tumut Express a… NSW   NSW60433 Tumb…    -35.8      148.\n 9      434 Adelong and Tumut Express a… NSW   NSW79906 Tumut    -35.3      148.\n10      625 Adelong and Tumut Express (… NSW   NSW81112 Adel…    -35.3      148.\n# ℹ 2,127 more rows\n\n\nNotice that each column has a data type beside it, either  for text or  for numbers. This is important if you want to sort or run calculations on the data.\n\nDoing this with newspaper data\nLet’s load a dataset of metadata for all the titles held by the library, and practise some counting and sorting on real-world data.\nDownload from here: British Library Research Repository\nYou would need to extract into your project folder first, if you’re following along:\nread_csv reads the csv from file.\n\ntitle_list = read_csv('data/BritishAndIrishNewspapersTitleList_20191118.csv')\n\nRows: 24927 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): publication_title, edition, preceding_titles, succeeding_titles, p...\ndbl  (6): title_id, nid, nlp, first_date_held, publication_date_one, publica...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSelect some particularly relevant columns:\n\ntitle_list %&gt;% \n  select(publication_title, \n         first_date_held, \n         last_date_held, \n         country_of_publication)\n\n# A tibble: 24,927 × 4\n   publication_title       first_date_held last_date_held country_of_publication\n   &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                 \n 1 \"Corante, or, Newes fr…            1621 1621           The Netherlands       \n 2 \"Corante, or, Newes fr…            1621 1621           The Netherlands       \n 3 \"Corante, or, Newes fr…            1621 1621           The Netherlands       \n 4 \"Corante, or, Newes fr…            1621 1621           England               \n 5 \"Courant Newes out of …            1621 1621           The Netherlands       \n 6 \"A Relation of the lat…            1622 1622           England               \n 7 \"A Relation of the lat…            1622 1622           England               \n 8 \"A Relation of the lat…            1622 1622           England               \n 9 \"A Relation of the lat…            1622 1622           England               \n10 \"A Relation of the lat…            1622 1622           England               \n# ℹ 24,917 more rows\n\n\nArrange in order of the latest date of publication, and then by the first date of publication:\n\ntitle_list %&gt;% \n  select(publication_title, \n         first_date_held, \n         last_date_held, \n         country_of_publication) %&gt;%\n  arrange(desc(last_date_held), first_date_held)\n\n# A tibble: 24,927 × 4\n   publication_title       first_date_held last_date_held country_of_publication\n   &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                 \n 1 Shrewsbury chronicle               1773 Continuing     England               \n 2 London times|The Times…            1788 Continuing     England               \n 3 Observer (London)|Obse…            1791 Continuing     England               \n 4 Limerick chronicle                 1800 Continuing     Ireland               \n 5 Hampshire chronicle|Th…            1816 Continuing     England               \n 6 The Inverness Courier,…            1817 Continuing     Scotland              \n 7 Sunday times (London)|…            1822 Continuing     England               \n 8 The Impartial Reporter…            1825 Continuing     Northern Ireland      \n 9 Impartial reporter and…            1825 Continuing     Northern Ireland      \n10 Aberdeen observer                  1829 Continuing     Scotland              \n# ℹ 24,917 more rows\n\n\nGroup and count by country of publication:\n\ntitle_list %&gt;% \n  select(publication_title, \n         first_date_held, \n         last_date_held, \n         country_of_publication) %&gt;%\n  arrange(desc(last_date_held)) %&gt;% \n  group_by(country_of_publication) %&gt;%\n  tally()\n\n# A tibble: 40 × 2\n   country_of_publication               n\n   &lt;chr&gt;                            &lt;int&gt;\n 1 Bermuda Islands                     24\n 2 Cayman Islands                       1\n 3 England                          20465\n 4 England|Hong Kong                    1\n 5 England|India                        2\n 6 England|Iran                         2\n 7 England|Ireland                     10\n 8 England|Ireland|Northern Ireland    10\n 9 England|Jamaica                      7\n10 England|Malta                        2\n# ℹ 30 more rows\n\n\nArrange again, this time in descending order of number of titles for each country:\n\ntitle_list %&gt;% \n  select(publication_title, \n         first_date_held, \n         last_date_held, \n         country_of_publication) %&gt;%\n  arrange(desc(last_date_held)) %&gt;% \n  group_by(country_of_publication) %&gt;%\n  tally() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 40 × 2\n   country_of_publication               n\n   &lt;chr&gt;                            &lt;int&gt;\n 1 England                          20465\n 2 Scotland                          1778\n 3 Ireland                           1050\n 4 Wales                             1019\n 5 Northern Ireland                   415\n 6 England|Wales                       58\n 7 Bermuda Islands                     24\n 8 England|Scotland                    13\n 9 England|Ireland                     10\n10 England|Ireland|Northern Ireland    10\n# ℹ 30 more rows\n\n\nFilter only those with more than 100 titles:\n\ntitle_list %&gt;% \n  select(publication_title, \n         first_date_held, \n         last_date_held, \n         country_of_publication) %&gt;%\n  arrange(desc(last_date_held)) %&gt;% \n  group_by(country_of_publication) %&gt;%\n  tally() %&gt;%\n  arrange(desc(n)) %&gt;% \n  filter(n&gt;=100)\n\n# A tibble: 5 × 2\n  country_of_publication     n\n  &lt;chr&gt;                  &lt;int&gt;\n1 England                20465\n2 Scotland                1778\n3 Ireland                 1050\n4 Wales                   1019\n5 Northern Ireland         415\n\n\nMake a simple bar chart:\n\ntitle_list %&gt;% \n  select(publication_title, \n         first_date_held, \n         last_date_held, \n         country_of_publication) %&gt;%\n  arrange(desc(last_date_held)) %&gt;% \n  group_by(country_of_publication) %&gt;%\n  tally() %&gt;%\n  arrange(desc(n)) %&gt;% \n  filter(n&gt;=100) %&gt;% \n  ggplot() + \n  geom_bar(aes(x = country_of_publication, weight = n))\n\n\n\n\nbarchart"
  },
  {
    "objectID": "using-r-tidyverse.html#recommended-reading",
    "href": "using-r-tidyverse.html#recommended-reading",
    "title": "6  Using R and the tidyverse",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThis has been a very quick introduction to R. There are lots of resources available to learn more, including:\nR-studio cheat sheets\nThe Pirate’s Guide to R, a good beginners guide to base R\nR for data science, which teaches the tidyverse in detail\nLearn how to make a book like this using Bookdown"
  },
  {
    "objectID": "map-title-list.html#news-metadata",
    "href": "map-title-list.html#news-metadata",
    "title": "7  Working with Metadata: Mapping the British Library Newspaper Collection",
    "section": "News Metadata",
    "text": "News Metadata\nWhile much of the rest of this book deals with text data, that is only one type of what might be considered newspaper data. Also important to consider is newspaper metadata, which can be used to understand and analyse newspaper collections and is valuable in its own right.\nKey metadata sources for the British Library newspaper collection are a series of title-level lists covering UK, worldwide, and even television news, and a series of press directories, containing information on the newspapers from contemporary sources.\n\nTitle-level lists\nThe title list has been produced by the British Library and is published on their institutional repository. It contains metadata taken from the Library’s catalogue of every newspaper published in Britain and Ireland up until the year 2019, a total of about 24,000 titles. There is more information available in a published data paper (Ryan and McKernan 2021).\nThis list contains a number of fields for each title, including the name of the publication, subsequent and previous title names, several fields for geographic coverage, the first and last dates held, and some other information. Where digitised versions have been created, these are noted and links are provided. It can be used to get statistical information about the Library’s collection, such as the number of new titles per year, the proportion of material which has been digitised, and how the collection is spread geographically.\n\n\nPress Directories\nThe Living with Machines project has digitised a number of press directories, between 1846 and 1920. These directories were produced by a number of companies, and were intended for advertisers and others as a guide to the content and coverage of printed newspapers. They contain information on the newspaper owners, geographic coverage, political leaning, and circulation. This, along with the title list, can serve as an excellent source for mapping, analysing, and visualising the newspaper collection at scale.\nThe directories have been digitised and the ‘raw’ .xml from the OCR process has been made available. Additionally, a single file where the information has been extracted and standardised, can also be downloaded."
  },
  {
    "objectID": "map-title-list.html#producing-maps-with-metadata-and-r",
    "href": "map-title-list.html#producing-maps-with-metadata-and-r",
    "title": "7  Working with Metadata: Mapping the British Library Newspaper Collection",
    "section": "Producing Maps With Metadata and R",
    "text": "Producing Maps With Metadata and R\nThe following two short tutorials aim to show how this kind of metadata can also be used to produce visualisations, and use the title list to produce a data map of the newspaper collection in Great Britain. They also aim to teach the reader how R can be used a GIS (Geographic Information System), and produce high-quality maps.\nThere are two, related tutorials. First, the title list is used to draw a map of points, relating to the number of newspaper titles published in each location. The second tutorial will guide you through creating a thematic, or choropleth map, using the same data."
  },
  {
    "objectID": "map-title-list.html#points",
    "href": "map-title-list.html#points",
    "title": "7  Working with Metadata: Mapping the British Library Newspaper Collection",
    "section": "Mapping Data as Points",
    "text": "Mapping Data as Points\n\nLesson Steps\nThe basic steps to create a map of points are as follows: 1) download a base map of the UK and Ireland, 2) count the total number of titles of each city and merge this with coordinate information on those cities and 3) create a shapefile out of that information, 4) draw both the basic map and the points on top of it.\nTo do this we’ll need three elements:\n\nA background map of the UK and Ireland\nA count of the total titles for each city\nA list of coordinates for each place mentioned in the title list.\n\n\n\nRequirements\nThis lesson assumes that you have some basic knowledge of R, and in particular, the package ggplot2. Not every step using this package is fully explained. You might want to refresh your knowledge by reading over Chapter 6, which gives a very short introduction.\nOn top of the packages used already, most importantly tidyverse, you’ll need to install a few more packages if you don’t have them already:\n\nthe sf package, which we’ll use to create and work with the geographic data\nThe rnaturalearth package. This will be used to download the underlying map of the UK and Ireland, on which the points will be overlaid.\n\nTo install all the necessary packages, you can use the following code:\n\ninstall.packages('tidyverse')\ninstall.packages('rnaturalearth')\ninstall.packages('sf')\n\nLastly, you’ll need to download a file containing the coordinate locations for the places in the title list, because this information is not included. This file can be downloaded here.\n\n\nDownload a ‘basemap’\nThe first step is to download a basemap: data containing the shape of the background area onto which the points will be overlaid. The easiest way to do this in R is to install the rnaturalearth package. This package makes it easy to download shapefiles: data containing polygons of various maps of the earth, containing shapes for everything from political borders to lakes and rivers.\nOnce you have installed the rnaturalearth and sf packages, load them as well as the tidyverse\n\nlibrary(tidyverse)\nlibrary(rnaturalearth)\nlibrary(sf)\n\nNext, use the rnaturalearth package to download a map of the UK and Ireland. There are a number of different shapefiles in the Natural Earth database we could choose from. One is ‘countries’, which is a set of connected shapes, one for each country on Earth. The other is ‘coastline’, which you could think of as a single continuous shape tracing the entire coastline of the planet.\nBoth have advantages and disadvantages. Using ‘countries’ makes it easy to just download the information for specific countries. However, it does mean that the shapes contain modern internal borders, which might not be suitable in all cases (if you are making a history map of Europe, for example).\nIn this case, we’ll use the ‘countries’ database. Use the ne_countries function from the rnaturalearth package to download the relevant shapes and save to a variable called uk_ireland_sf. We have to set a number of parameters in the function:\nscale is set to \"medium\": this is the resolution of the shapes (how detailed they will be). Medium is fine at this level.\ncountry is set to a vector of the country names we want, in this case Ireland and the UK.\nreturnclass is set to sf. This is the format for the geographic information which we’ll use with the sf package. sf stands for ‘shapefile’, and is a standardised way of representing geographic data, which is also easy to query as a dataset, for example to filter or summarise the data. rnaturalearth can return a file in this format automatically.\n\nuk_ireland_sf = ne_countries(scale = 'medium', \n                             country = c(\"United Kingdom\", \"Ireland\"), \n                             returnclass = 'sf')\n\nOnce you run this, you should have a new variable called uk_ireland_sf. We can draw this to check exactly what has been downloaded. To do this, we can use a function in ggplot2 called geom_sf(). This geom draws geographic information from shapefiles.\n\nggplot() + geom_sf(data = uk_ireland_sf)\n\n\n\n\nNow is a good moment to demonstrate some features of ggplot2 which are particularly relevant for mapping. This map is a good starting point, but some parts of it should be improved to make it more readable: we don’t need the border in Northern Ireland to be drawn, we don’t want the coordinates written on the x and y axes, and we want a simpler, single colour background.\nLet’s start by making some changes to the colour. We can change both the line and the fill colours. To do this, we add some parameters to the geom_sf() function. The color parameter will set the outer borders, and the fill parameter the inside of the shape. To make the differences obvious I’ll use two very distinct colours:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'blue', fill = 'green') \n\n\n\n\nRemember we chose ‘countries’ rather than ‘coastline’ above? One small drawback is that we can’t draw a map using the countries data where a distinct line surrounds both islands. One quite legible way to draw the map is to have a lightly-coloured background and render the entire map shape, both internal and external lines, in a single colour. To do this, we’ll set both color and fill to a medium/light gray:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') \n\n\n\n\nNext, we’ll get rid of the background clutter. The following code will remove the background and axis text:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') + \n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(),\n        panel.background = element_blank()\n        ) \n\n\n\n\nNow we have a map onto which we can draw the points.\n\n\nCropping the Map\nIn some cases, you may want to crop a map further. We can do that with coord_sf(). coord_fixed() is used to fix the aspect ratio of a coordinate system, but can be used to specify a bounding box by using two of its arguments: xlim= and ylim=.\nThese each take a vector (a series of numbers) with two items A vector is created using c(). Each item in the vector specifies the limits for that axis. So xlim = c(0,10) means restrict the x-axis to 0 and 10.\nThe axes correspond to the lines of longitude (x) and latitude (y). We’ll restrict the x-axis to c(-10, 4) and the y-axis to c(50.3, 60) which should just about cover the UK and Ireland.\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') + \n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(),\n        panel.background = element_blank()\n        ) +\n  coord_sf(xlim = c(-10,3), \n              ylim = c(50.3, 59))\n\n\n\n\nEmpty Map\n\n\n\n\n\n\nAdd Sized Points By Coordinates\nOn top of this basemap, we will draw a set of points, sized by the count of newspapers from each place in a metadata list of newspaper titles from the British Library.\nDoing this involves a few steps:\n\nDownload a metadata list of newspaper titles held by the British Library\nGenerate a new dataset, containing a total count the newspapers from each place in this title list.\nMerge this with a dataset of coordinates for each place\nTurn this information into a shapefile, as the map downloaded above.\nDraw the points on top of the base map.\n\n\nDownload the metadata list\nTo start with, we’ll need a complete list of the newspaper titles held in by in the British Library’s collection. This is available as a free download through the Open Research Repository. Go to the repository page and download the file BritishAndIrishNewspapersTitleList_20191118.zip (or just click the link to directly download).\nUnzip this file and note where you save it. It will contain a .csv file with the metadata list. Import this to R with the following (replace the 'data/BritishAndIrishNewspapersTitleList_20191118.csv' with the path to your file):\n\ntitle_list = read_csv('data/BritishAndIrishNewspapersTitleList_20191118.csv')\n\nNext, create a new dataframe, which will simply be count of the appearances of each place in the metadata. Use the dplyr commands group_by() and tally(), which will count the instances of each different combination of the three geographic information columns:\n\nlocation_counts = title_list %&gt;% \n  group_by(country_of_publication, \n           general_area_of_coverage, \n           coverage_city) %&gt;% \n  tally()\n\nThis new dataframe, location_counts, will have one row for each combination of country/general area/city in the data, along with a new column, n, containing the count:\n\nlocation_counts %&gt;% head(5) %&gt;% kableExtra::kbl() \n\n\n\n\ncountry_of_publication\ngeneral_area_of_coverage\ncoverage_city\nn\n\n\n\n\nBermuda Islands\nNA\nHamilton\n23\n\n\nBermuda Islands\nNA\nSaint George\n1\n\n\nCayman Islands\nNA\nGeorgetown\n1\n\n\nEngland\nAberdeenshire (Scotland)\nPeterhead\n1\n\n\nEngland\nActon (London, England)\nEaling (London, England)\n1\n\n\n\n\n\n\n\n\n\n\nGet hold of a list of geocoordinates\nTo create the shapefile and visualise the data, the next step is to merge this text information on the places, to a dataset of coordinates. For this, you’ll need existing coordinate information, which has been created separately. This file is available online from the following source. If you run this code, it will import the coordinate file directly into R from its storage place on Github.\n\ngeocorrected = read_csv('https://raw.githubusercontent.com/yann-ryan/r-for-news-data/master/data/geocorrected.csv')\n\nThis file also needs some pre-processing. A preliminary step here is to change the column names so that they match those found in the metadata:\n\nlibrary(snakecase)\ncolnames(geocorrected) = to_snake_case(colnames(geocorrected))\n\nThere are a few further pieces of pre-processing to do before we can merge this to the location_counts dataframe. We’ll change some of the column names and remove some unnecessary ones, and change the na values to NA, which is properly recognised by R. Last, we’ll change the coordinate information to numeric values, and then filter out any rows with missing coordinate information:\n\ncolnames(geocorrected)[7:8] = c('lat', \n                                'lng')\n\ngeocorrected = geocorrected %&gt;% \n  select(-1, -5,-9,-10, -11, -12)\n\ngeocorrected = geocorrected %&gt;% \n  mutate(country_of_publication = replace(country_of_publication, \n                                          country_of_publication == 'na', NA)) %&gt;%\n  mutate(general_area_of_coverage = replace(general_area_of_coverage,\n                                            general_area_of_coverage == 'na', NA)) %&gt;%\n  mutate(coverage_city = replace(coverage_city,\n                                 coverage_city == 'na', NA))\n\ngeocorrected = geocorrected %&gt;%\n  mutate(lat = as.numeric(lat)) %&gt;% \n  mutate(lng = as.numeric(lng)) %&gt;% \n  filter(!is.na(lat)) %&gt;% filter(!is.na(lng))\n\ngeocorrected %&gt;% head(5)%&gt;% kableExtra::kbl() \n\n\n\n\ncoverage_city\ngeneral_area_of_coverage\ncountry_of_publication\nwikititle\nlat\nlng\n\n\n\n\nAfghanistan\nAfghanistan\nEngland\nAfghanistan\n33.0000\n65.00\n\n\nAirdrie\nAirdrie\nEngland\nAirdrie,_North_Lanarkshire\n55.8600\n-3.98\n\n\nAlbania\nAlbania\nEngland\nAlbania\n41.0000\n20.00\n\n\nAustralia\nAustralia\nEngland\nAustralia\n-25.0000\n133.00\n\n\nBahrain\nBahrain\nEngland\nBahrain\n26.0275\n50.55\n\n\n\n\n\n\n\nThe result is a dataframe with a set of longitude and latitude points (they come from Wikipedia, which is why they are prefixed with wiki) for every combination of city/county/country in the list of titles. These can be joined to the full title list with the following method:\nUsing left_join() we will merge these dataframes, joining up each set of location information to its coordinates and standardised name. left_join() is a very common command in data analysis. It merges two sets of data by matching a value known as a key.\nHere the key is actually a combination of three values - city, county and country, and it matches up the two sets of data by ‘joining’ two rows together, if they share all three of these values. Store this is a new variable called lc_with_geo.\n\nlc_with_geo = location_counts %&gt;% \n  left_join(geocorrected, \n            by = c('coverage_city' ,\n                   'general_area_of_coverage',\n                   'country_of_publication')) %&gt;% \n  filter(!is.na(lat))\n\nIf you look at this new dataset, you’ll see that now the counts of locations have merged with the geocorrected data. Now we have an amount and coordinates for each place.\n\nhead(lc_with_geo, 10)%&gt;% kableExtra::kbl() \n\n\n\n\ncountry_of_publication\ngeneral_area_of_coverage\ncoverage_city\nn\nwikititle\nlat\nlng\n\n\n\n\nEngland\nAvon\nBath\n88\nUNKNOWN\n51.3811\n2.35900\n\n\nEngland\nAvon\nBristol\n175\nBristol\n51.4500\n-2.58333\n\n\nEngland\nAvon\nClevedon\n14\nClevedon\n51.4380\n-2.85400\n\n\nEngland\nAvon\nKeynsham\n4\nKeynsham\n51.4135\n-2.49680\n\n\nEngland\nAvon\nNailsea\n2\nNailsea\n51.4300\n-2.76000\n\n\nEngland\nAvon\nNorton\n4\nUNKNOWN\n51.2842\n-2.48170\n\n\nEngland\nAvon\nPortishead\n2\nPortishead,_Somerset\n51.4840\n-2.76260\n\n\nEngland\nAvon\nRadstock\n6\nRadstock\n51.2927\n-2.44770\n\n\nEngland\nAvon\nThornbury\n3\nThornbury,_Gloucestershire\n51.6094\n-2.52490\n\n\nEngland\nAvon\nWeston-super-Mare\n23\nWeston-super-Mare\n51.3460\n-2.97700\n\n\n\n\n\n\n\nA next step is to use group_by() and tally() again, this time on the the wikititle, lat and lng columns. This is because the wikititle is a standardised title, which means it will group together cities properly, rather than giving a different row for slightly different combinations of the three geographic information columns (incidentally, it could also be used to link to wikidata). At the same time, filter again to ensure no rows are missing latitude or longitude information:\n\nlc_with_geo_counts = lc_with_geo %&gt;% \n  group_by(wikititle, lat, lng) %&gt;% \n  tally(n) %&gt;% filter(!is.na(lat)& !is.na(lng))\n\nNow we’ve got a dataframe with counts of total newspapers, for each standardised wikipedia title in the dataset.\n\nknitr::kable(head(lc_with_geo_counts,10))\n\n\n\n\nwikititle\nlat\nlng\nn\n\n\n\n\nAbbots_Langley\n51.7010\n-0.4160\n1\n\n\nAberavon_(UK_Parliament_constituency)\n51.6000\n-3.8120\n1\n\n\nAberdare\n51.7130\n-3.4450\n20\n\n\nAberdeen\n57.1500\n-2.1100\n82\n\n\nAbergavenny\n51.8240\n-3.0167\n9\n\n\nAbergele\n53.2800\n-3.5800\n8\n\n\nAbersychan\n51.7239\n-3.0587\n2\n\n\nAbertillery\n51.7300\n-3.1300\n2\n\n\nAberystwyth\n52.4140\n-4.0810\n31\n\n\nAbingdon-on-Thames\n51.6670\n-1.2830\n23\n\n\n\n\n\n\n\nFinally, create the ‘shapefile’ object. To do this from a dataframe, we’ll use a function st_as_sf from the sf package. Also specify the columns this function should use as the coordinates for the shapefile, using the coords = parameter, and specify in a vector the longitude and latitude columns from the data:\n\nlc_with_geo_counts_sf = lc_with_geo_counts %&gt;% \n  st_as_sf(coords = c('lng', 'lat'))\n\nWe can draw this shapefile using geom_sf, to check and see that it looks reasonable:\n\nggplot() + geom_sf(data = lc_with_geo_counts_sf)\n\n\n\n\n\n\nSetting a Coordinate Reference System (CRS)\nTo create the final map, there’s one more important step. A shapefile is not just a list of coordinates, but also includes a ‘coordinate reference system’ (CRS), which tells how the coordinates in that shapefile should be interpreted. As a first step, we’ll set both our shapefiles to have the same CRS:\n\nuk_ireland_sf = uk_ireland_sf %&gt;% st_set_crs(4326)\n\nlc_with_geo_counts_sf = lc_with_geo_counts_sf %&gt;% st_set_crs(4326)\n\n\n\nCreating the Final Map\nFirst we’ll start with the base map from the first steps:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') + \n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(),\n        panel.background = element_blank()\n        )\n\n\n\n\nBlank Map of UK and Ireland\n\n\n\n\nNext, plot the newspaper place information on top of this using another geom_sf. The second one is added to the first layer using a + sign:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') + \n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(),\n        panel.background = element_blank()\n        ) + \n  geom_sf(data = lc_with_geo_counts_sf)\n\n\n\n\nBlank Map of UK and Ireland\n\n\n\n\nWe see that the points are drawn on top of the base map. To make the map more informative and legible, there are a few more things we can do. First, size the points by the count of the instances (column ‘n’). This is done by setting the size aesthetic in ggplot to the column name, inside the aes(). We’ll also add the command scale_size_area() to the code, which better represents the relationship between the numeric value and the circle size:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') + \n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(),\n        panel.background = element_blank()\n        ) + \n  geom_sf(data = lc_with_geo_counts_sf, aes(size = n)) + scale_size_area()\n\n\n\n\nWe can also reduce the transparency of the points to make them more readable, using the alpha aesthetic:\n\nggplot() + \n  geom_sf(data = uk_ireland_sf, color = 'gray80', fill = 'gray80') + \n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank(),\n        panel.background = element_blank()\n        ) + \n  geom_sf(data = lc_with_geo_counts_sf, aes(size = n), alpha = .6) + \n  scale_size_area() \n\n\n\n\nUsing labs(), add a title, and with scale_size_area() and scale_color_viridis_c(), make some changes to the size and colours, respectively."
  },
  {
    "objectID": "map-title-list.html#choro",
    "href": "map-title-list.html#choro",
    "title": "7  Working with Metadata: Mapping the British Library Newspaper Collection",
    "section": "Drawing a newspaper titles ‘Choropleth’ map with R and the sf package",
    "text": "Drawing a newspaper titles ‘Choropleth’ map with R and the sf package\nAnother type of map is known as a ‘choropleth’. This is where the data is visualised by a certain polygon area rather than a point. Typically these represent areas like parishes, counties or countries. Using the library sf a choropleth map can be made quite quickly.\nTo do this, we will use the sf package to merge the information at the coordinate level with information on the geographic county borders. Next, we’ll count the number of titles within each county, and use this total to color or shade the map. The good thing about this method is that once you have a set of coordinates, they can be situated within any shapefile - a historic map, for example. This is particularly useful for anything to do with English counties, which have changed several times throughout history.\nThis section uses data from .visionofbritain.co.uk, which needs to be downloaded separately. You could also use the free boundary data here: https://www.ordnancesurvey.co.uk/business-government/products/boundaryline, which contains boundaries for both modern and historic counties.\nThis is an excellent source, and the file includes a range of boundaries including counties but also districts and constituencies, under an ‘Open Government Licence’.\n\nChoropleth map steps\nThe steps to create this type of map:\n\nDownload shapefiles for England and scotland from here\nTurn into sf object Download list of points, turn into sf object\nUse st join to get county information\nJoin to the title list and deselect everything except county and titles - maybe 19th century only..\nJoin that to the sf object Plot using geom_sf()\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(sf)\nsf::sf_use_s2(FALSE)\n\n\n\nGet county information from the title list\nNext, download (if you haven’t already) the title list from the British Library open repository.\n\ntitle_df = read_csv('data/BritishAndIrishNewspapersTitleList_20191118.csv')\n\n\n\nDownload shapefiles\nFirst, download the relevant shapefiles. These don’t necessarily have to be historic ones. Use st_read() to read the file, specifying its path. Do this for England, Wales and Scotland (we don’t have points for Ireland)."
  },
  {
    "objectID": "map-title-list.html#transform-from-utm-to-latlong-using-st_transform",
    "href": "map-title-list.html#transform-from-utm-to-latlong-using-st_transform",
    "title": "7  Working with Metadata: Mapping the British Library Newspaper Collection",
    "section": "Transform from UTM to lat/long using st_transform()",
    "text": "Transform from UTM to lat/long using st_transform()\nThese shapefiles use points system known as UTM, which stands for ‘Universal Transverse Mercator’. According to wikipedia,\n\nit differs from global latitude/longitude in that it divides earth into 60 zones and projects each to the plane as a basis for its coordinates.\n\nIt needs to be transformed into lat/long coordinates, because the coordinates we have are in that format. This is easy with st_transform():\n\neng_1851 = st_transform(eng_1851, crs = 4326)\n\n\nscot_1851  = st_transform(scot_1851, crs = 4326)\n\nBind them both together, using rbind() to make one big shapefile for Great Britain.\n\ngb1851 = rbind(eng_1851, scot_1851 %&gt;%\n                 select(-UL_AUTH))\n\n\nDownload and merge the title list with a set of coordinates.\nNext, load and pre-process the set of coordinates:\n\ngeocorrected = read_csv('data/geocorrected.csv')\n\nChange the column names:\n\nlibrary(snakecase)\ncolnames(geocorrected) = to_snake_case(colnames(geocorrected))\n\nChange some column names further, select just the relevant columns, change the NA values and get rid of any empty entries.\n\ncolnames(geocorrected)[6:8] = c('wikititle', 'lat', 'lng')\n\ngeocorrected = geocorrected %&gt;% select(-1, -9,-10, -11, -12)\n\ngeocorrected = geocorrected %&gt;% \n  mutate(country_of_publication = replace(country_of_publication, \n                                          country_of_publication == 'na', NA)) %&gt;% mutate(general_area_of_coverage = replace(general_area_of_coverage,\n                                                                                                                             general_area_of_coverage == 'na', NA)) %&gt;% \n  mutate(coverage_city = replace(coverage_city, \n                                 coverage_city == 'na', NA))\n\ngeocorrected = geocorrected %&gt;%\n  mutate(lat = as.numeric(lat)) %&gt;% \n  mutate(lng = as.numeric(lng)) %&gt;% \n  filter(!is.na(lat)) %&gt;% \n  filter(!is.na(lng))\n\nNext, join these points to the title list, so that every title now has a set of lat/long coordinates.\n\ntitle_df = title_df %&gt;% \n  left_join(geocorrected) %&gt;% \n  filter(!is.na(lat)) %&gt;% \n  filter(!is.na(lng))\n\n\n\nUsing st_join to connect the title list to the shapefile\nTo join this to the shapefile, we need to turn it in to an simple features item. To do this we need to specify the coordinates and the CRS. The resulting file will contain a new column called ‘geometry’, containing the lat/long coordaintes in the correct simple features format.\n\nst_title = st_as_sf(title_df, coords = c('lng', 'lat'))\n\nst_title = st_title  %&gt;% st_set_crs(4326)\n\nNow, we can use a special kind of join, which will join the points in the title list, if they are within a particular polygon. The resulting dataset now has the relevant county, as found in the shapefile.\n\nst_counties = st_join(st_title, gb1851)\n\nMake a new dataframe, containing just the counties and their counts.\n\ncounty_tally = st_counties %&gt;% \n  select(G_NAME) %&gt;% \n  group_by(G_NAME) %&gt;% \n  tally() %&gt;%\n  st_drop_geometry()\n\n\n\nDraw using ggplot2 and geom_sf()\nJoin this to the shapefile we made earlier, which gives a dataset with the relevant counts attached to each polygon. This can then be visualised using the geom_sf() function from ggplot2, and all of ggplot2’s other features can be used.\n\ngb1851 %&gt;% \n  left_join(county_tally) %&gt;% \n  ggplot() + \n  geom_sf(lwd = .5,color = 'black', aes(fill = n)) + \n  theme_void() +\n  lims(fill = c(10,4000)) + \n  scale_fill_viridis_c(option = 'plasma') + \n  labs(title = \"British Library Newspaper\\nTitles, by County\", \n       fill = 'No. of Titles:') + \n  theme(legend.position = 'left') +\n  theme(title = element_text(size = 12), \n        legend.title = element_text(size = 8))\n\n\n\n\nChoropleth Made with geom_sf()"
  },
  {
    "objectID": "map-title-list.html#recommended-reading",
    "href": "map-title-list.html#recommended-reading",
    "title": "7  Working with Metadata: Mapping the British Library Newspaper Collection",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nThis article uses both of these metadata files (the title list and the press directories) as its historical evidence:\nKaspar Beelen and others, Bias and representativeness in digitized newspaper collections: Introducing the environmental scan, Digital Scholarship in the Humanities, Volume 38, Issue 1, April 2023, Pages 1–22, https://doi.org/10.1093/llc/fqac037\nThe book ‘Geocomputation with R’ is a fantastic resource for learning about mapping: https://geocompr.robinlovelace.net.\n\n\n\n\nRyan, Yann, and Luke McKernan. 2021. “Converting the British Library’s Catalogue of British and Irish Newspapers into a Public Domain Dataset: Processes and Applications.” Journal of Open Humanities Data 7. https://doi.org/10.5334/johd.23."
  },
  {
    "objectID": "download-and-unzip.html#shared-research-repository",
    "href": "download-and-unzip.html#shared-research-repository",
    "title": "8  Accessing Newspaper Data from the Shared Research Repository",
    "section": "Shared Research Repository",
    "text": "Shared Research Repository\nThe titles released so far are available on the British Library’s Shared Research Repository.\nThe items in the repository are organised into collections. All the newspaper-related data released on to the repository can be found within the British Library News Datasets collection. Clicking on this link will bring up a list of all the items collected under this headings. There are also three sub-headings: Title Lists, Newspapers, and Press Directories. Clicking on the first of these, Newspapers, will display just the newspaper data items.\n\n\n\nScreenshot showing the Shared Research Repository maintained by the British Library, on the newspaper collections page.\n\n\n\nNewspaper File Structure\nEach separate title (if a newspaper changed title, they are combined together) is listed here as a dataset. Clicking into one of these, you’ll see that each year of that title is available as a separate downloadable .zip file.\nIf you download one of these and decompress it, you’ll see the structure of the title. It contains a root folder, containing the name and year of the title, as well as a unique title code. Contained within this folder are further folders, one for each day of the newspaper. These folders are named using the month and day of publication.\nWithin this folder are the actual newspaper files: one .xml file for each page of that day’s paper, plus one more METS file.\n\n\n\nScreenshot showing the folder structure of a decompressed newspaper-year file"
  },
  {
    "objectID": "download-and-unzip.html#downloading-titles-in-bulk",
    "href": "download-and-unzip.html#downloading-titles-in-bulk",
    "title": "8  Accessing Newspaper Data from the Shared Research Repository",
    "section": "Downloading Titles in Bulk",
    "text": "Downloading Titles in Bulk\nAcquiring a dataset on which to work can be cumbersome if you have to manually download each file. The first part of this tutorial will show you how to bulk download all or some of the available titles. This was heavily inspired by the method found here. If you want to bulk download titles using a more robust method (using the command line, so no need for R), then I really recommend checking out that repository.\nTo do this there are three basic steps:\n\nMake a list of all the links in the repository collection for each title\nGo into each of those title pages, and get a list of all links for each zip file download.\nOptionally, you can specify which titles you’d like to download, or even which year.\nDownload all the relevant files to your machine.\n\n\nCreate a dataframe of all the newspaper files in the repository\nFirst, load the libraries needed:\n\nlibrary(tidyverse)\nlibrary(XML)\nlibrary(xml2)\nlibrary(rvest)\n\nNext, grab all the pages of the collection:\n\nurls = paste0(\"https://bl.iro.bl.uk/collections/9a6a4cdd-2bfe-47bb-8c14-c0a5d100501f?locale=en&page=\",1:6)\n\nUse lapply to go through the list, use the function read_html to read the page into R, and store each as an item in a list:\n\nlist_of_pages &lt;- lapply(urls, read_html)\n\nNext, write a function that takes a single html page (as downloaded with read_html), extracts the links and newspaper titles, and puts it into a dataframe.\n\nmake_df = function(x){\n  \n  all_collections =  x %&gt;% \n  html_nodes(\".search-result-title\") %&gt;% \n  html_nodes('a') %&gt;% \n  html_attr('href') %&gt;% \n  paste0(\"https://bl.iro.bl.uk\",.)\n\nall_collections_titles = x %&gt;% \n  html_nodes(\".search-result-title\") %&gt;% \n  html_text()\n\nall_collections_df = tibble(all_collections, all_collections_titles) %&gt;% \n  filter(str_detect(all_collections, \"concern\\\\/datasets\"))\n\nall_collections_df\n\n}\n\nRun this function on the list of html pages. This will return a list of dataframes. Merge them into one with rbindlist from data.table.\n\nl = purrr::map(list_of_pages, make_df)\n\nl = data.table::rbindlist(l)\nl %&gt;% knitr::kable('html')\n\nl = l %&gt;% mutate(pages = paste0(all_collections, \"&page=\")) \n\nsequence &lt;- 1:10\n\n# Expand the dataframe and concatenate with the sequence\nexpanded_df &lt;- l %&gt;%\n  crossing(sequence) %&gt;%\n  mutate(pages = paste(pages, sequence, sep = \"\"))\n\nNow we have a dataframe containing the url for each of the titles in the collection. The second stage is to go to each of these urls and extract the relevant download links.\nWrite another function. This takes a url, extracts all the links and IDs within it, and turns it into a dataframe. It then filters to just the relevant links (which have the ID ‘file_download’).\n\nget_collection_links = function(c){\n  tryCatch({\ncollection = c  %&gt;%  read_html()\n  \nlinks = collection%&gt;% html_nodes('a') %&gt;% html_attr('href')\n\nid = collection %&gt;% html_nodes('a') %&gt;% html_attr('id')\n\ntext = collection%&gt;% html_nodes('a') %&gt;% html_attr('title')\n\nlinks_df = tibble(links, id, text)\n\n}, error = function(e) {\n    # Action to perform when an error occurs\n    result &lt;- NA\n  })\n\nreturn(links_df)\n\n\n\n}\n\nUse lapply to run this function on the column of urls from the previous step,and merge it with rbindlist. Keep just links which contain the text Download BLNewspapers.\n\nt = pbapply::pblapply(expanded_df$pages, get_collection_links)\n\nnames(t) = expanded_df$all_collections_titles[1]\n\nt_df = t %&gt;%\n  data.table::rbindlist(idcol = 'title') \n\nt_df = t_df %&gt;% \n  filter(str_detect(text, \"BLNewspapers\"))\n\nThe new dataframe needs a bit of tidying up. To use the download.file() function in R we need to also specify the full filename and location where we’d like the file to be put. At the moment the ‘text’ column is what we want but it needs some alterations. First, remove the ‘Download’ text from the beginning.\nNext, separate the text into a series of columns, using either _ or . as the separator. Create a new ‘filename’ column which pastes the different bits of the text back together without the long code.\nAdd /newspapers/ to the beginning of the filename, so that the files can be downloaded into that folder.\n\nt_df = t_df %&gt;% distinct(text, .keep_all = TRUE) %&gt;% \n  mutate(year = str_extract(text, \"(?&lt;=_)[0-9]{4}(?=[_.])\")) %&gt;% \n  mutate(nid = str_extract(text, \"[0-9]{7}\")) %&gt;% mutate(filename = str_extract(text, '(?&lt;=Download \")[^\"]+'))\n\nt_df = t_df %&gt;% mutate(links = paste0(\"https://bl.iro.bl.uk\",links )) %&gt;% \n  mutate(destination = paste0('/Users/Yann/Documents/non-Github/r-newspaper-quarto/newspapers/', filename))\n\nThe result is a dataframe which can be used to download either all or some of the files.\n\n\nFilter the download links by date or title\nYou can now filter this dataframe to produce a list of titles and/or years you’re interested in. For example, if you just want all the newspapers for 1855:\n\nfiles_of_interest = t_df %&gt;% filter(as.numeric(year) == 1855)\nfiles_of_interest%&gt;% knitr::kable('html')\n\nTo download these we use the Map function, which will apply the function download.file to the vector of links, using the dest colum we created as the file destination. download.file by default times out after 100 seconds, but these downloads will take much longer. Increase this using options(timeout=9999).\nBefore this step, you’ll need to create a new folder called ‘newspapers’, within the working directory of the R project.\n\noptions(timeout=9999)\n\nMap(function(u, d) download.file(u, d, mode=\"wb\"), files_of_interest$links, files_of_interest$dest)\n\n\n\nFolder structure\nOnce these have downloaded, you can quickly unzip them using R. First it’s worth understanding a little about the folder structure you’ll see once they’re unzipped.\nEach file will have a filename like this:\nBLNewspapers_TheSun_0002194_1850.zip\nThis is made from the following pieces of information:\nBLNewspapers - this identifies the file as coming from the British Library\nTheSun - this is the title of the newspaper, as found on the Library’s catalogue.\n0002194 - This is the NLP, a unique code given to each title. This code is also found on the Title-level list, in case you want to link the titles from the repository to that dataset.\n1850 - The year."
  },
  {
    "objectID": "download-and-unzip.html#contruct-a-corpus",
    "href": "download-and-unzip.html#contruct-a-corpus",
    "title": "8  Accessing Newspaper Data from the Shared Research Repository",
    "section": "Contruct a Corpus",
    "text": "Contruct a Corpus\nAt this point, and for the rest of the tutorials in the book, you might want to construct a ‘corpus’ of newspapers, using whatever criteria you see fit. Perhaps you’re interested in a longitudinal study, and would like to download a small sample of years spread out over the century, or maybe you’d like to look at all the issues in a single newspaper, or perhaps all of a single year across a range of titles.\nThe tutorials will make most sense and produce similar results if your corpus is the same as above: all newspapers in the repository from the year 1855. You can also download a single .zip file with the extracted text from these titles here."
  },
  {
    "objectID": "download-and-unzip.html#bulk-extract-the-files-using-unzip-and-a-for-loop",
    "href": "download-and-unzip.html#bulk-extract-the-files-using-unzip-and-a-for-loop",
    "title": "8  Accessing Newspaper Data from the Shared Research Repository",
    "section": "Bulk extract the files using unzip() and a for() loop",
    "text": "Bulk extract the files using unzip() and a for() loop\nR can be used to unzip the files in bulk, which is particularly useful if you have downloaded a large number of files. It’s very simple, there’s just two steps. This is useful if you’re using windows and have a large number of files to unzip.\nFirst, use list.files() to create a vector, called zipfile containing the full file paths to all the zip files in the ‘newspapers’ folder you’ve just created.\n\nzipfiles = list.files(\"/Volumes/T7/zipfiles/\", full.names = TRUE)\nzipfiles\n\nNow, use this in a loop with unzip().\nLoops in R are very useful for automating simple tasks. The below takes each file named in the ‘zipfiles’ vector, and unzips it. It takes some time.\n\npurrr::map(zipfiles, unzip)\n\nOnce this is done, you’ll have a new (or several new) folders in the project directory (not the newspapers directory). These are named using a numeric code, called the ‘NLP’, so they should look like this in your project directory:\nTo tidy up, put these back into the newspapers folder.\nThese files contain the METS/ALTO .xml files with the newspaper text. If you have followed the above and downloaded all newspapers for the year 1855, you should have seven different titles and a few hundred newspaper issues. In the next chapter, you’ll extract this text from the .xml and save it in a more convenient format. These final files will form the basis for the following tutorials which process and analyse the text of the newspapers."
  },
  {
    "objectID": "extract-text.html#folder-structure",
    "href": "extract-text.html#folder-structure",
    "title": "9  Make a Text Corpus",
    "section": "Folder structure",
    "text": "Folder structure\nDownload and extract the newspapers you’re interested in, and put them in the same folder as the project you’re working on in R.\nThe folder structure of the newspapers is [nlp]-&gt;year-&gt;issue month and day-&gt; xml files. The nlp is a unique code given to each digitised newspaper. This makes it easy to find an individual issue of a newspaper.\nLoad some libraries: all the text extraction is done using tidyverse and furrr for some parallel programming.\n\nrequire(furrr)\nrequire(tidyverse)\nlibrary(tidytext)\nlibrary(purrr)\n\nThere are two main functions: get_page(), which extracts words and their corresponding textblock, and make_articles(), which extracts a table of the textblocks and corresponding articles, and joins them to the words from get_page(). get_page() also cleans up the text, removing words in super and sub-script, for example. This is because within the .xml, these words are duplicated so can be safely removed. It also replaces the .xml which indicates split words, with a hyphen.\nHere’s get_page():\n\nget_page = function(alto){\n page = alto %&gt;%  read_file() %&gt;%\n        str_split(\"\\n\", simplify = TRUE) %&gt;% \n        keep(str_detect(., \"CONTENT|&lt;TextBlock ID=\")) %&gt;% \n        str_extract(\"(?&lt;=CONTENT=\\\")(.*?)(?=WC)|(?&lt;=&lt;TextBlock ID=)(.*?)(?= HPOS=)\")%&gt;% \n        discard(is.na) %&gt;% \n    as.tibble() %&gt;%\n    mutate(pa = ifelse(str_detect(value, \"pa[0-9]{7}\"), \n                       str_extract(value, \"pa[0-9]{7}\"), NA)) %&gt;% \n    fill(pa) %&gt;%\n    filter(str_detect(pa, \"pa[0-9]{7}\")) %&gt;% \n    filter(!str_detect(value, \"pa[0-9]{7}\"))%&gt;% \n   mutate(value = str_remove_all(value, \n                                 \"STYLE=\\\"subscript\\\" \")) %&gt;% \n   mutate(value = str_remove_all(value, \n                                 \"STYLE=\\\"superscript\\\" \"))%&gt;% \n   mutate(value = str_remove_all(value,\n                                 \"\\\"\")) %&gt;%\n   mutate(value = str_replace_all(value,\n                                 ' SUBS_TYPE=HypPart1 SUBS_CONTENT=.*', '-'))%&gt;%\n   mutate(value = str_remove_all(value,\n                                 ' SUBS_TYPE=HypPart2 SUBS_CONTENT=.*'))\n}\n\nIf you want to understand how it works, I have broken the function down into components below.\nFirst read the alto page, which should be an argument to the function. Here’s one page to use as an example:\n\nalto = \"newspapers/0002194/1855/0101//0002194_18550101_0001.xml\"\n\naltofile = alto %&gt;%  read_file()\n\nSplit the file on each new line, resulting in a character vector of the length of the number of lines in the page:\n\naltofile = altofile %&gt;%\n        str_split(\"\\n\", simplify = TRUE)\n\naltofile %&gt;% glimpse()\n\nJust keep lines which contain either a CONTENT or TextBlock tag. This\n\naltofile = altofile %&gt;% keep(str_detect(., \"CONTENT|&lt;TextBlock ID=\"))\n\naltofile %&gt;% glimpse()\n\nTurn it into a dataframe (a tibble in this case):\n\naltofile = altofile %&gt;% \n  str_extract(\"(?&lt;=CONTENT=\\\")(.*?)(?=WC)|(?&lt;=&lt;TextBlock ID=)(.*?)(?= HPOS=)\") %&gt;% \n        #discard(is.na) %&gt;% \n  as_tibble()\n\naltofile %&gt;% head(20)\n\nThis dataframe has a single column, containing every textblock, textline and word in the ALTO file. Now we need to extract the textblock IDs, put them in a separate column, and then fill() each textblock ID down until it reaches the next one.\n\naltofile = altofile %&gt;% \n  mutate(pa = ifelse(str_detect(value,\n                                \"pa[0-9]{7}\"),\n                     str_extract(value, \"pa[0-9]{7}\"), NA)) %&gt;% \n    fill(pa)\n\nThe final step removes the textblock IDs from the column which should contain only words, and cleans up some .xml tags we don’t want:\n\naltofile = altofile %&gt;%\n    filter(str_detect(pa, \"pa[0-9]{7}\")) %&gt;% \n    filter(!str_detect(value, \"pa[0-9]{7}\"))%&gt;% \n   mutate(value = str_remove_all(value, \n                                 \"STYLE=\\\"subscript\\\" \")) %&gt;% \n   mutate(value = str_remove_all(value, \n                                 \"STYLE=\\\"superscript\\\" \"))%&gt;% \n   mutate(value = str_remove_all(value,\n                                 \"\\\"\")) %&gt;%\n   mutate(value = str_replace_all(value,\n                                 ' SUBS_TYPE=HypPart1 SUBS_CONTENT=.*', '-'))%&gt;%\n   mutate(value = str_remove_all(value,\n                                 ' SUBS_TYPE=HypPart2 SUBS_CONTENT=.*'))\n\nThe final output is a dataframe with individual words on one side and the text block IDs on the other.\n\nhead(altofile)\n\nThis is the second function:\n\nmake_articles &lt;- function(foldername){\n    \n  files &lt;- list.files(foldername, full.names = TRUE)\n  \n  csv_files_exist &lt;- any(xfun::file_ext(files) == \"csv\")\n  \n  if (!csv_files_exist) {\n  \n   metsfilename =  str_match(list.files(path = foldername, \n                                        all.files = TRUE, \n                                        recursive = TRUE, \n                                        full.names = TRUE),\n                             \".*mets.xml\") %&gt;%\n     discard(is.na)\n    \n    csvfoldername = metsfilename %&gt;% str_remove(\"_mets.xml\")\n    \n    metsfile = read_file(metsfilename)\n    \n    page_list =  str_match(list.files(path = foldername, \n                                      all.files = TRUE, \n                                      recursive = TRUE, \n                                      full.names = TRUE), \n                           \".*[0-9]{4}.xml\") %&gt;%\n    discard(is.na)\n    \n    \n    \n        metspagegroups = metsfile %&gt;% \n          str_split(\"&lt;mets:smLinkGrp&gt;\")%&gt;%\n    flatten_chr() %&gt;%\n    as_tibble() %&gt;% \n          filter(str_detect(value, '#art[0-9]{4}')) %&gt;% \n          mutate(articleid = str_extract(value,\"[0-9]{4}\")) \n\n    \n     t = future_map(page_list, get_page) \n     t = t[sapply(t, nrow) &gt; 0]\n     t %&gt;% \n       bind_rows()  %&gt;%\n       left_join(extract_page_groups(metspagegroups$value) %&gt;% \n                                    unnest() %&gt;% \n        mutate(art = ifelse(str_detect(id, \"art\"), \n                            str_extract(id, \"[0-9]{4}\"), NA)) %&gt;% \n        fill(art) %&gt;% \n          filter(!str_detect(id, \n                             \"art[0-9]{4}\")),\n        by = c('pa' = 'id')) %&gt;% \n      group_by(art) %&gt;% \n      summarise(text = paste0(value, collapse = ' ')) %&gt;% \n       mutate(issue_name = metsfilename ) %&gt;%\n       write_csv(path = paste0(csvfoldername, \".csv\"))\n     \n  } else {\n    message(cat(\"Skipping folder:\", foldername, \"- .csv files already exist.\\n\"))\n  }\n\n\n}\n\nIt’s a bit more complicated, and a bit of a fudge. Because there are multiple ALTO pages for one METS file, we need to read in all the ALTO files, run our get_pages() function on them within this function, bind them altogether, and then join that to a METS file which contains an article ID and all the corresponding textBlocks. Again, if you’re interested, the function has been broken down into components below. You can ignore this section if you just want to run the function and extract text from your own files.\nThe function takes an argument called ‘foldername’. This folder should correspond to the folders within the downloaded newspaper files from the BL repository. Later, we can pass a list of folder names to the function using lapply() or future_map(), and it will run the function on each folder in turn.\nThis is how it works with a single folder:\n\nfoldername = \"newspapers/0002194/1855/0101/\"\n\nUsing the folder name as the last part of the file path, and then a regular expression to get only a file ending in mets.xml, this will get the correct METS file name and read it into memory:\n\nmetsfilename =  str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), \".*mets.xml\") %&gt;%\n    discard(is.na)\n\nmetsfilename\n\n\nmetsfile = read_file(metsfilename)\n\nWe also need to call the .csv (which we’re going to have as an output) a unique name:\n\ncsvfoldername = metsfilename %&gt;% str_remove(\"_mets.xml\")\n\nNext we have to grab all the ALTO files in the same folder, using the same method:\n\npage_list =  str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), \".*[0-9]{4}.xml\") %&gt;%\n    discard(is.na)\n\nNext we need the file which lists all the pagegroups and corresponding articles.\n\nmetspagegroups = metsfile %&gt;% \n  str_split(\"&lt;mets:smLinkGrp&gt;\") %&gt;%\n    flatten_chr() %&gt;%\n    as_tibble() %&gt;% \n  filter(str_detect(value, '#art[0-9]{4}')) %&gt;% \n  mutate(articleid = str_extract(value,\"[0-9]{4}\"))\n\nThe next bit uses a function written by brodrigues called extractor()\n\nextractor &lt;- function(string, regex, all = FALSE){\n    if(all) {\n        string %&gt;%\n            str_extract_all(regex) %&gt;%\n            flatten_chr() %&gt;%\n            str_extract_all(\"[:alnum:]+\", simplify = FALSE) %&gt;%\n            purrr::map(paste, collapse = \"_\") %&gt;%\n            flatten_chr()\n    } else {\n        string %&gt;%\n            str_extract(regex) %&gt;%\n            str_extract_all(\"[:alnum:]+\", simplify = TRUE) %&gt;%\n            paste(collapse = \" \") %&gt;%\n            tolower()\n    }\n}\n\nWe also need another function which extracts the correct pagegroups:\n\nextract_page_groups &lt;- function(article){\n\n    id &lt;- article %&gt;%\n        extractor(\"(?&lt;=&lt;mets:smLocatorLink xlink:href=\\\"#)(.*?)(?=\\\" xlink:label=\\\")\", \n                  all = TRUE)\n\n    type = \n    tibble::tribble(~id,\n                    id) \n}\n\nNext this takes the list of ALTO files, and applies the get_page() function to each item, then binds the four files together vertically. I’ll give it a random variable name, even though it doesn’t need one in the function because we just pipe it along to the csv.\n\nt = future_map(page_list, get_page)\nt = t[sapply(t, nrow) &gt; 0]\nt = t %&gt;% \n  bind_rows()\n\nhead(t)\n\nThis extracts the page groups from the mets dataframe we made, and turns it into a dataframe with the article ID as a number, again extracting and filtering using regular expressions, and using fill(). The result is a dataframe of every word, plus their article and text block.\n\nt = t %&gt;%\n  left_join(extract_page_groups(metspagegroups$value) %&gt;% \n                                    unnest() %&gt;% \n        mutate(art = ifelse(str_detect(id, \"art\"), \n                            str_extract(id, \n                                        \"[0-9]{4}\"), NA))%&gt;% \n          fill(art), \n        by = c('pa' = 'id')) %&gt;% \n  fill(art)\n        \nhead(t, 50)\n\nNext we use summarise() and paste() to group the words into the individual articles, and add the mets filename so that we also can extract the issue date afterwards.\n\n t = t %&gt;% \n    group_by(art) %&gt;% \n  summarise(text = paste0(value, collapse = ' ')) %&gt;% \n       mutate(issue_name = metsfilename ) \n\nhead(t, 10)\n\nAnd finally write to .csv using the csvfoldername we created:\n\nt %&gt;%\n       write_csv(path = paste0(csvfoldername, \".csv\"))\n\nTo run it on a bunch of folders, you’ll need to make a list of paths to all the issue folders you want to process. You can do this using list_dirs. You only want these final-level issue folders, otherwise it will try to work on an empty folder and give an error. This means that if you want to work on multiple years or issues, you’ll need to figure out how to pass a list of just the issue level folder paths.\nIn this case, I used the package fs:\n\nlibrary(fs)\n\n\nget_all_deepest_folders &lt;- function(folder_path) {\n  if (!file.exists(folder_path) || !file.info(folder_path)$isdir) {\n    stop(\"Invalid folder path or folder does not exist.\")\n  }\n\n  find_deepest_folders_recursive &lt;- function(dir_path) {\n    subdirs &lt;- list.dirs(dir_path, full.names = TRUE, recursive = FALSE)\n    \n    if (length(subdirs) == 0) {\n      return(dir_path)\n    }\n    \n    deepest_subdirs &lt;- character(0)\n    for (subdir in subdirs) {\n      deepest_subdirs &lt;- c(deepest_subdirs, find_deepest_folders_recursive(subdir))\n    }\n    \n    return(deepest_subdirs)\n  }\n  \n  deepest_folders &lt;- find_deepest_folders_recursive(folder_path)\n  return(unique(deepest_folders))\n}\n\n\n\n\n\n\nstarting_folder &lt;- \"../../../Downloads/TheSun_sample/\"\ndeepest_folders &lt;- get_all_deepest_folders(starting_folder)\n\nFinally, this applies the function make_articles() to everything in the folderslist vector. It will write a new .csv file into each of the folders, containing the article text and codes. You can add whatever folders you like to a vector called folderlist, and it will generate a csv of articles for each one.\n\nfuture_map(deepest_folders, make_articles, .progress = TRUE)\n\nIt’s not very fast (I think it can take 10 or 20 seconds per issue, so bear that in mind), but eventually you should now have a .csv file in each of the issue folders, with a row per line.\nThese .csv files can be re-imported and used for text mining tasks such as:\n\nword frequency count\ntf-idf scores\nsentiment analysis\ntopic modelling\ntext reuse"
  },
  {
    "objectID": "term-freq.html#load-the-news-dataframe-and-relevant-libraries",
    "href": "term-freq.html#load-the-news-dataframe-and-relevant-libraries",
    "title": "10  N-gram Analysis",
    "section": "Load the news dataframe and relevant libraries",
    "text": "Load the news dataframe and relevant libraries\nFor this tutorial, you’ll need a set of .csv files containing newspaper article text in a specific format. Chapter 8 and Chapter 9 walk through the processing of downloading and creating these files. If you want to construct your own newspaper corpus and use it for this chapter, I recommend going back and checking those out. Alternatively, if you want to use a ready-made corpus, you can download a compressed file containing all the articles from a single year on the repository: 1855. This file is available on Zenodo. Once you have downloaded it, decompress it and make a note of where it is stored on your local machine.\nThe first step is to load all these files and turn them into a single dataframe. The command list.files(), with the parameters set below, will list all the files with the text csv in them: Swap the path= parameter for the location you have saved the .csv files, if appropriate.\n\nnews_sample_dataframe = list.files(path = \"newspaper_text\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\nNext, import the files themselves. The function lapply is a bit like a loop: it will take list or a vector (in this case, a vector containing the file names we want to import), and run the same function over them all, storing the result as a list. In this case, we’ll run the function fread, which will read a single file into R as a dataframe. Passing it a list of filenames means it will read all of them into dataframes, and store them as a list of dataframes.\nWith this list of dataframes, we’ll use another function rbindlist to transform them from a list of dataframes to a single, long dataframe. Essentially, merging them together.\n\nall_files = lapply(news_sample_dataframe, data.table::fread) \n\nnames(all_files) = news_sample_dataframe\n\nall_files_df = data.table::rbindlist(all_files, idcol = 'filename')\n\nMake a new object, news_df, which takes the information about the newspaper found in the filename, and uses it as metadata, stored in columns.\nOne first step is to get the actual title names for the titles. The information in the .csv only contains a unique code, the NLP. To do this, we’ll make a small dataframe with the title names and NLP codes, and then join this to the data.\nThe result is a dataframe with a column for the issue date, the article number, and the full article text.\n\ntitle_names_df = tibble(newspaper_id = c('0002090', '0002194', '0002244', '0002642', '0002645', '0003089', '0002977'), newspaper_title = c('The Liverpool Standard And General Commercial Advertiser', 'The Sun', 'Colored News', 'The Express', 'The Press', 'Glasgow Courier', 'Swansea and Glamorgan Herald'))\n\nnews_df = all_files_df %&gt;% \n  mutate(filename = basename(filename))\n\n\nnews_df = news_df %&gt;% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # turn the date column into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% \n  left_join(title_names_df, by = 'newspaper_id')# select all columns but with the article code first"
  },
  {
    "objectID": "term-freq.html#text-analysis-with-tidytext",
    "href": "term-freq.html#text-analysis-with-tidytext",
    "title": "10  N-gram Analysis",
    "section": "Text Analysis with Tidytext",
    "text": "Text Analysis with Tidytext\nThe package we’re going to use for analysing the text is called ‘tidytext’. This package has many features for understanding and working with text, such as tokenising (splitting into words) and calculating the tf-idf scores of words in a group of documents. The authors of the package have published an entire book, Text Mining with R, which is a very good introduction to text analysis with R.\nIt’s a little different to the approach taken by, say, python, because it works on the principle of turning text into dataframes, in a format which is easy to work with in R. When we tokenise, for example, tidytext will create a dataframe which contains one row for each token. This is then easy to count, sort, filter, and so forth, using standard tidyverse tools.\nSimple word counts as a method are increasingly outdated, and modern text analysis is much more likely to use more sophisticated metrics and take the context into account. But a simple statistical analysis of text is still a useful and quick way of understanding a group of documents and getting an overview of their contents. It also is often the first step in further analysis, such as topic modelling or word embeddings.\nAs a first step, take a look at the dataframe, using glimpse():\n\nnews_df %&gt;% mutate(text = str_trunc(text, 500)) %&gt;% # show just the first part of the text for displaying in a dataframe\n  head(5) %&gt;% # show the first 5 rows \n  kableExtra::kbl()\n\n\n\n\narticle_code\nnewspaper_id\ndate\nart\ntext\nnewspaper_title\n\n\n\n\n1\n0002090\n1855-06-19\n1\nesP e allall J o Po&#34;' .d. o f f t/' ea tice 11, P ate° s' ide r ice F r tl e i e'l.fr 't 'e ' cVi c° P tc rj t , irit'tbhe el., j t 0 P , te O P. o3g e r ti 2b111„,1,P\nThe Liverpool Standard And General Commercial Advertiser\n\n\n2\n0002090\n1855-06-19\n2\nEqp t , I , WILLIAMSON-SQUARE. i'lle . AL, --- elott,,t . in e „ - . 417 FOR TFVO NIGHTS ONLY. . 'itilltiste, Miss CUSHMAN, will appear in her rt,, shed Im Personation of u . ite lr(a , MEG MERRILIES, t 4 iiednesdaythe 20th instant and in the New Play of ACTRESS OF , PADUA, On Fainmr next, the 22nd instant. 0441 eltkertilielts.7.- T0.k 0 ,, 11 (Wednes.day). the 20th instant, Ilezr the Ncewl G UY MANNERING ' Miss ...\nThe Liverpool Standard And General Commercial Advertiser\n\n\n3\n0002090\n1855-06-19\n3\nTri C 3JI all 1101 Tau atl) AND GENERAL COMMERCIAL ADYERTIS'EI? A CARD. ATR. ENSOR, DENTIST, 171 Has REMOVED from SEEL-STRENT tO No. 51, RODNEY-STREET. 316 EMBROIDERED COLLARS at 2s. 11d., worth 4s. 3d. 170 Ditto Ditto at 2s. 6d., worth 3s. 9d. 253 Ditto Ditto at Is. 71d.. „ 2s. 3d. Also, several Cheap Lots of SLEEVES AND HABIT-SHIRTS. HARRISON BROTHERS, 60 &amp; 62, CHURCH-STREET, Corner of Hanover-street. FLANAGAN'S EOL ...\nThe Liverpool Standard And General Commercial Advertiser\n\n\n4\n0002090\n1855-06-19\n4\nA NNIVERSARY OF THE NATIONAL SCHOOLS. The ANNIVERSARY of the NATIONAL SCHOOLS connected with the Established Church will be held THIS DAY (Tuesday), the 19th instant, when a SERMON will be preached in St. Peter's Church, by the Rev. THOMAS NOLAN, M.A., Vicar of Acton ; after which a Collection will be made in aid of the Funds of the different Schools. Divine Service will commence at Eleven o'clock. THE RUSSIANS. WHAT KIND...\nThe Liverpool Standard And General Commercial Advertiser\n\n\n5\n0002090\n1855-06-19\n5\nA LOT OF THE VERY BEST F RENCH PRINTED MUSLINS AT 12s. 9d. Full Dress, usually sold at 255. 6d. ; also a LARGE REGULAR STOCK Of FRENCH AND TOWN PRINTED MUSLIN'S, in all the New Designs, Fast Colours, commencing at 64. 9d. the Dress HARRISON BROTHERS, 63 &amp; G 2, CHURCH-STREET, Corner of Hanover-street. A splendid Assortment of LONDON BRONZED TEA URNS tjr SWING KETTLES, FENDERS, FIRE IRONS, PAPER TRAYS, HIP, SPONGING, S...\nThe Liverpool Standard And General Commercial Advertiser\n\n\n\n\n\n\n\nWe can see that it has a number of rows (about 97,000 if you’re using the data from the previous tutorials) and 5 columns. Each row is a different article, and the fifth column text, contains the full text of that article. You’ll also probably notice that the text itself is pretty garbled, because of OCR errors. It’s worth pointing out that because we’re looking at the first few articles, which are usually advertisements, the OCR is likely to be much worse than with ordinary articles within the paper.\nAs a first task, we’ll simply use the tidytext package to make a count of the words found in the data."
  },
  {
    "objectID": "term-freq.html#tokenise-the-text-using-unnest_tokens",
    "href": "term-freq.html#tokenise-the-text-using-unnest_tokens",
    "title": "10  N-gram Analysis",
    "section": "Tokenise the text using unnest_tokens()",
    "text": "Tokenise the text using unnest_tokens()\nThe first step is to tokenise the text. This is the starting point for many of the basic text analyses which will follow. Tokenising simply divides the text into ‘tokens’: smaller, equally-sized ‘units’ of some text. A unit is often a word, but could be a bigram (a sequence of two consecutive words), or a trigram, a sequence of three consecutive words.\nTo do this using the library tidytext, we will pass the dataframe of text to the function unnest_tokens(). This function takes a column of text in a dataframe and splits it into tokens. The function has a set of default values, but, as we will see, we can change the function to create other types of functions.\nTo understand what tokenising is doing, I’ll make a dataframe containing a single ‘article’, which in this case is a single sentence.\n\ndf = tibble(article_code = 1, text = \"The quick brown fox jumped over the lazy dog.\")\n\ndf %&gt;% \n  kableExtra::kbl()\n\n\n\n\narticle_code\ntext\n\n\n\n\n1\nThe quick brown fox jumped over the lazy dog.\n\n\n\n\n\n\n\nTo tokenise this dataframe, we’ll use the unnest_tokens function. The two most important parameters to unnest_tokens are output and input. This is fairly self-explanatory. Pass the name of the column you’d like to tokenise as the input, and the name you would like to give the tokenised words as the output.\n\ndf %&gt;% \n  unnest_tokens(output = word, input = text) %&gt;% \n  kableExtra::kbl()\n\n\n\n\narticle_code\nword\n\n\n\n\n1\nthe\n\n\n1\nquick\n\n\n1\nbrown\n\n\n1\nfox\n\n\n1\njumped\n\n\n1\nover\n\n\n1\nthe\n\n\n1\nlazy\n\n\n1\ndog\n\n\n\n\n\n\n\nRun this code, and you’ll see that the dataframe has been transformed. Each word in the sentence is now on a separate row, in a new column called word. Also note that the other data (in this case the article code) is kept and duplicated.\nYou can also specify an argument for token, allowing you to split the text into sentences, characters, lines, or n-grams. If you split into n-grams, you need to use the argument n= to specify how many consecutive words you’d like to use.\nLike this:\n\ndf %&gt;% \n  unnest_tokens(output = word, \n                input = text, \n                token = 'ngrams', \n                n =3) %&gt;% \n  kableExtra::kbl()\n\n\n\n\narticle_code\nword\n\n\n\n\n1\nthe quick brown\n\n\n1\nquick brown fox\n\n\n1\nbrown fox jumped\n\n\n1\nfox jumped over\n\n\n1\njumped over the\n\n\n1\nover the lazy\n\n\n1\nthe lazy dog\n\n\n\n\n\n\n\nYou can also use other tokenizers such as character shingles, or supply your own method for splitting the text, such as on new lines, if you have them in your text:\n\ndf = tibble(article_code = 1, text = \"The quick brown fox\\njumped over the lazy dog.\")\n\ndf %&gt;% \n  unnest_tokens(output = word, \n                input = text, token = stringr::str_split, pattern = \"\\n\") %&gt;% \n  kableExtra::kbl()\n\n\n\n\narticle_code\nword\n\n\n\n\n1\nthe quick brown fox\n\n\n1\njumped over the lazy dog.\n\n\n\n\n\n\n\nNow, it’s time to do this to our article text. Create a new object, news_tokens, using unnest_tokens(), passing the text column as the input column:\n\nnews_tokens = news_df %&gt;% unnest_tokens(output = word, input = text)\n\nnews_tokens %&gt;% head(10) %&gt;% \n  kableExtra::kbl()\n\n\n\n\narticle_code\nnewspaper_id\ndate\nart\nnewspaper_title\nword\n\n\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\nesp\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\ne\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\nallall\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\nj\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\no\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\npo\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\n34\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\nd\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\no\n\n\n1\n0002090\n1855-06-19\n1\nThe Liverpool Standard And General Commercial Advertiser\nf\n\n\n\n\n\n\n\nThe result is a very large dataset of words - one row for each word in the dataset, a total of about 66 million using the tutorial data.\n\nSpeeding things up with {Tidytable}\nThe next step is to use tidyverse commands to count and analyse the data. However, doing this with a dataframe of 66 million rows is not ideal. For this, we’ll introduce a new package, called tidytable. Tidytable allows us to use tidyverse verbs, but it translates them into another package, data.table, behind the scenes. Data.table is much faster in most cases.\nTo use the tidytable equivalent to a tidyverse verb, add a period (.) just before the parentheses. For example, mutate() becomes mutate.()\nNote that tidytable does not have a group_by command. Instead, you’ll use the parameter .by = within another command to specify the group you want it to apply to.\nOnce this is done, it is relatively easy to count and analyse the data using standard tidyverse verbs. The following will count the instances of each word and show them in descending order:\n\nnews_tokens %&gt;% \n  summarise.(n = n.(), .by = word) %&gt;% \n  arrange.(desc.(n)) %&gt;% head(20) %&gt;% \n  kableExtra::kbl()\n\n\n\n\nword\nn\n\n\n\n\nthe\n4483357\n\n\nof\n2473323\n\n\nto\n1674183\n\n\nand\n1625027\n\n\na\n1097019\n\n\nin\n977684\n\n\nthat\n593476\n\n\nat\n530306\n\n\ni\n529302\n\n\nfor\n471661\n\n\nbe\n467150\n\n\nwas\n456650\n\n\non\n454623\n\n\nis\n453208\n\n\nby\n415020\n\n\nit\n413959\n\n\nwith\n341337\n\n\ne\n334563\n\n\nt\n332604\n\n\n1\n329737\n\n\n\n\n\n\n\nAs you can see, the top words are entirely made up of short, common words such as the, of, i, and so forth. These are unlikely to tell us much about the text or reveal patterns about the content (though they may have other uses, for example for identifying authors, but let’s ignore that for now).\nTo get something more meaningful out of these top results, it’s probably best to do some text cleaning.\nWhen doing your own research, particularly using sources such as newspapers which will often look quite different and have messy OCR, you’ll often need to go back and forth, checking and adding additional cleaning and pre-processing steps to get something meaningful.\nIn this case, we’ll remove these short, common words (known as ‘stop words’), and also, later, do some more text cleaning."
  },
  {
    "objectID": "term-freq.html#removing-stop-words",
    "href": "term-freq.html#removing-stop-words",
    "title": "10  N-gram Analysis",
    "section": "Removing stop words",
    "text": "Removing stop words\nTo do this, we load a dataframe of stopwords, which is included in the tidytext package:\n\ndata(\"stop_words\")\n\nThis will load a dataframe called stop_words into the R environment. This dataframe contains a column called word. We want to merge this to our tokenised data, and remove any matches.\nNext use the function anti_join(). This basically removes any word in our word list which is also in the stop words list:\n\nnews_tokens = news_tokens %&gt;% \n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\nLet’s take another look at the top words, using the same code as above:\n\nnews_tokens %&gt;% \n  summarise.(n = n.(), .by = word) %&gt;% \n  arrange.(desc.(n)) %&gt;% head(20)%&gt;% \n  kableExtra::kbl()\n\n\n\n\nword\nn\n\n\n\n\n1\n329737\n\n\n4\n205696\n\n\n0\n200869\n\n\namp\n152582\n\n\n3\n99579\n\n\n6\n84694\n\n\n11\n84643\n\n\nday\n84447\n\n\n2\n81151\n\n\n5\n80543\n\n\nlord\n78383\n\n\nstreet\n75711\n\n\n10\n73835\n\n\ntime\n68863\n\n\nst\n68110\n\n\n7\n67775\n\n\nlondon\n67363\n\n\nhouse\n59095\n\n\nwar\n56102\n\n\nde\n53433\n\n\n\n\n\n\n\nNow the stopwords are removed, we can use the filter() command to remove any other unwanted tokens: numbers are also particularly common in newspapers (these titles often publish lists of stocks and shipping information, for example) and don’t tell us anything about the texts. Furthermore, ssome horizontal lines have been picked up by the OCR as punctuation. Let’s filter both of these out:\n\nnews_tokens = news_tokens %&gt;% \n  filter(!str_detect(word, \"[0-9]\")) %&gt;% \n  filter(!str_detect(word, \"__\"))\n\nThe command str_detect() within `filter() removes any word which matches a given regular expressions pattern. In this case, the pattern is simply any occurence of a number. That’s a bit blunt, but it will be effective at least.\nLet’s check the top words again:\n\nnews_tokens %&gt;% \n  summarise.(n = n.(), .by = word) %&gt;% \n  arrange.(desc.(n)) %&gt;% head(20) %&gt;% \n  kableExtra::kbl()\n\n\n\n\nword\nn\n\n\n\n\namp\n152582\n\n\nday\n84447\n\n\nlord\n78383\n\n\nstreet\n75711\n\n\ntime\n68863\n\n\nst\n68110\n\n\nlondon\n67363\n\n\nhouse\n59095\n\n\nwar\n56102\n\n\nde\n53433\n\n\nsir\n53288\n\n\njohn\n49003\n\n\nsad\n48236\n\n\nroyal\n45438\n\n\ncent\n43548\n\n\ngovernment\n41093\n\n\npublic\n40450\n\n\nesq\n38063\n\n\ncountry\n37544\n\n\narmy\n37313\n\n\n\n\n\n\n\nThe list looks a bit more sensible now, with words which are plausibly found often particularly within news sources."
  },
  {
    "objectID": "term-freq.html#visualising-using-ggplot",
    "href": "term-freq.html#visualising-using-ggplot",
    "title": "10  N-gram Analysis",
    "section": "Visualising using ggplot",
    "text": "Visualising using ggplot\nThe next step is to visualise this, which is very easy to do using ggplot2.\n\nnews_tokens %&gt;% \n  summarise.(n = n.(), .by = word) %&gt;% \n  arrange.(desc.(n)) %&gt;% head(20) %&gt;% \n  ggplot() + geom_col(aes(x = reorder(word, n),y = n)) + coord_flip()\n\n\n\n\nAs well as a basic count of everything, we can use tidyverse/tidytable to do some more specific counts and visualisations.\nFor example, a count of the top five words for each month in the data. For this, we need to create a ‘month’ column:\n\nnews_tokens %&gt;% \n  mutate(month = as.character(cut(date, 'month')))%&gt;% \n  summarise.(n = n.(), .by = c(month, word)) %&gt;% \n  group_by(month) %&gt;% \n  slice_max(order_by = n, n = 10) %&gt;% \n  ggplot() + \n  geom_col(aes(x = reorder(word, n), y = n)) + \n  coord_flip() + facet_wrap(~month, scales = 'free_y')\n\n\n\n\nWe can also do it by issue code:\n\nnews_tokens %&gt;% \n  summarise.(n = n.(), .by = c(newspaper_id, word)) %&gt;% \n  group_by(newspaper_id) %&gt;% \n  slice_max(order_by = n, n = 10) %&gt;% \n  ggplot() + geom_col(aes(x = reorder(word, n), y = n)) + coord_flip() + facet_wrap(~newspaper_id, scales = 'free')\n\n\n\n\nWe can see some differences between the titles, although many of the word lists are quite similar. Many of these are typical words which appear in advertisements or report - words related to times or places (street, day, clock, etc.). There are also regional differences, with the place associated with the title (glasgow, swansea, liverpool etc.) also showing up. One title, 0002645, seems to have more words related to ‘serious’ news (war, government). Some further cleaning or adding words to the stop word list would be helpful too, clearly.\n\nChange over time\nAnother thing to look at is the change in individual words over time. ‘War’ is a common word: did its use change over the year? To do this, we first filter the token dataframe, using filter() to keep only the word (or words) we’re interested in.\nIn many cases (as definitely here), the spread of data over the entire period is not even - some months have many more words than others. For an analysis to be in any way meaningful, you should think of some way of normalising the results, so that the number is of a percentage of the total words in that title, for example. The raw numbers may just indicate a change in the total volume of text.\nWe’ll do this with an extra step. First, make a count of the total number of each word, per week. Second, make a new column which divides the total per word, by the total number of words per week. This number is the frequency - basically what proportion of the total words for that week is a particular word. Lastly, filter to just the word of interest:\n\n\nWords over time\n\nnews_tokens %&gt;%\n    mutate.(week = ymd(cut(date, 'week'))) %&gt;% \n    summarise.(n = n(), .by = c(word, week))  %&gt;%\n    mutate.(freq = n/sum(n), .by = week) %&gt;% \n    filter.(word == 'war') %&gt;% \n    ggplot() + geom_col(aes(x = week, y = freq))\n\n\n\n\nChart of the Word ‘ship’ over time\n\n\n\n\nWe can also look at very seasonal words, to test whether it really makes sense:\n\nnews_tokens %&gt;%\n    mutate.(week = ymd(cut(date, 'week'))) %&gt;% \n    summarise.(n = n(), .by = c(word, week))  %&gt;%\n    mutate.(freq = n/sum(n), .by = week) %&gt;% \n    filter.(word == 'christmas') %&gt;% \n    ggplot() + geom_col(aes(x = week, y = freq))\n\n\n\n\nUnsurprisingly, there is a seasonal pattern to the word christmas in the dataset.\nCounting tokens like this in many cases says more about the dataset and its collection than anything about the content or the historical context. In fact, many of the words seem to be coming from advertisements rather than news articles. In a future chapter, we’ll build a classifier to detect these and remove them."
  },
  {
    "objectID": "term-freq.html#tf-idf",
    "href": "term-freq.html#tf-idf",
    "title": "10  N-gram Analysis",
    "section": "Tf-idf",
    "text": "Tf-idf\nThis section deals uses R and tidytext to do another very typical word frequency analysis, known as the tf-idf score. This is a measurement of how ‘unique’ a word is in a given document, by comparing its frequency in one document to its frequency overall. Counting tokens, as above, will generally result in a listen of very popular words, which occur very often in all newspapers, and so don’t really give any interesting information. Using a metric such as tf-idf can be a way to understand the most ‘significant’ words within a given document.\nIn this case, the word document can be misleading. It could be a single issue or article, or it could be something completely different, depending on our needs. A document can be any way of splitting up the text. For instance, we could consider all articles from a given month as a single ‘document’, and then calculate the words most unique to that month. This might give us a better understanding of what unique topics were being discussed at a particular time in the newspapers.\nTo do this, we use a function from tidytext called bind_tf_idf. This function expects a list of words per document and a raw count. We’ll do this as a first step. To keep things simple, we’ll use the newspaper ID as the ‘document’, meaning the metric should find words unique to each title. First, create a new object news_tokens_count, which contains the counts of the words in each newspaper ID:\n\nnews_tokens_count = news_tokens %&gt;% \n  count(newspaper_id, word)\n\nnews_tokens_count %&gt;% head(10) %&gt;% \n  kableExtra::kbl()\n\n\n\n\nnewspaper_id\nword\nn\n\n\n\n\n0002090\n_a\n45\n\n\n0002090\n_a_\n1\n\n\n0002090\n_aa\n2\n\n\n0002090\n_aaa\n1\n\n\n0002090\n_aacislain\n1\n\n\n0002090\n_aad\n1\n\n\n0002090\n_abo\n1\n\n\n0002090\n_about\n2\n\n\n0002090\n_acaltitash\n1\n\n\n0002090\n_accept\n1\n\n\n\n\n\n\n\nNext, use the bind_tf_idf function from tidytext. This needs to be passed the word column (term), the document column, and the column with the word counts (n)\n\nnews_tfidf = news_tokens_count %&gt;% \n  bind_tf_idf(term = word, document = newspaper_id, n = n)\n\nnews_tfidf %&gt;% head(10) %&gt;% \n  kableExtra::kbl()\n\n\n\n\nnewspaper_id\nword\nn\ntf\nidf\ntf_idf\n\n\n\n\n0002090\n_a\n45\n1.73e-05\n0.0000000\n0e+00\n\n\n0002090\n_a_\n1\n4.00e-07\n0.5596158\n2e-07\n\n\n0002090\n_aa\n2\n8.00e-07\n0.8472979\n7e-07\n\n\n0002090\n_aaa\n1\n4.00e-07\n1.9459101\n7e-07\n\n\n0002090\n_aacislain\n1\n4.00e-07\n1.9459101\n7e-07\n\n\n0002090\n_aad\n1\n4.00e-07\n1.2527630\n5e-07\n\n\n0002090\n_abo\n1\n4.00e-07\n1.9459101\n7e-07\n\n\n0002090\n_about\n2\n8.00e-07\n0.8472979\n7e-07\n\n\n0002090\n_acaltitash\n1\n4.00e-07\n1.9459101\n7e-07\n\n\n0002090\n_accept\n1\n4.00e-07\n1.9459101\n7e-07\n\n\n\n\n\n\n\nNow, we have a new object with new columns. The first is tf, which is simply the frequency of that term as a proportion of all words in the document. Next is idf, which is the inverse of the frequency of the word over all the documents. The less frequent a word is overall, the larger the number in this column. Third is tf_idf, which multiples one by the other.\nTo make use of this, we want to find the words with the highest tf-idf scores for each of the documents. Let’s do this and plot the results. In order to have the results show correctly, we make use of the function reorder_within from the tidytext package. This is to ensure that the words are properly ordered within each facet, rather than overall.\n\nnews_tfidf %&gt;% \n  group_by(newspaper_id) %&gt;% \n  slice_max(order_by = tf_idf, n = 10) %&gt;% \n  ungroup() %&gt;%\n    mutate(newspaper_id = as.factor(newspaper_id),\n           word = reorder_within(word, tf_idf, newspaper_id)) %&gt;%\n  ggplot() + geom_col(aes(word, tf_idf)) + \n  facet_wrap(~newspaper_id, scales = 'free') +\n    scale_x_reordered()+\n    scale_y_continuous(expand = c(0,0))+ coord_flip()\n\n\n\n\nThere’s one final problem worth considering with using tf-idf. A word can score very highly if it occurs just a couple of times in one document and not at all in others. This may mean that the highest tf-idf words are not actually significant but just extremely rare. One solution to this is to filter so that we only consider words that occur at least a few times in the documents:\n\nnews_tfidf %&gt;% \n  filter(n&gt;3) %&gt;% \n  group_by(newspaper_id) %&gt;% \n  slice_max(order_by = tf_idf, n = 10) %&gt;% \n  ungroup() %&gt;%\n    mutate(newspaper_id = as.factor(newspaper_id),\n           word = reorder_within(word, tf_idf, newspaper_id)) %&gt;%\n  ggplot() + geom_col(aes(word, tf_idf)) + \n  facet_wrap(~newspaper_id, scales = 'free') +\n    scale_x_reordered()+\n    scale_y_continuous(expand = c(0,0))+ coord_flip()\n\n\n\n\nThe results are still not very satisfactory - at least at first glance, it’s hard to get anything about the"
  },
  {
    "objectID": "term-freq.html#sec-bigram",
    "href": "term-freq.html#sec-bigram",
    "title": "10  N-gram Analysis",
    "section": "Small case study",
    "text": "Small case study\nTo demonstrate how counting frequencies can be used as a form of analysis, this section is a small case study looking at bigrams, that is, pairs of words found in the data. Looking at the immediate context of a word can give some clue as to how it is being used. For example, the word ‘board’ in the bigram ‘board game’ has a different meaning to the word board in the bigram ‘board meeting’.\nIn this case study, we’ll count bigrams containing the word ‘liberal’, to see how the meaning of that word changed over time.\nBecause we want to look at temporal, or diachronic change, we’ll need a different datatset to the year 1855 used in the chapter so far. Instead, we’ll use a dataset containing all the issues of a single title The Sun, for the years 1802 and 1870. These are the earliest and latest years in the data, and using one title means it’s more likely we’ll have at least slightly consistent results.\nI have already extracted the text from these years and they are available as .zip file here. Once you have downloaded this, decompress and put the path to the folder name in the code below. Otherwise, follow the same steps as in the code at the beginning of the chapter.\n\ntheSun = list.files(path = \"../../../Downloads/TheSun_sample/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\ntheSunall_files =   lapply(theSun, data.table::fread) \n\nnames(theSunall_files) = theSun\n\ntheSunall_files_df = data.table::rbindlist(theSunall_files, idcol = 'filename')\n\ntheSunall_files_df = theSunall_files_df %&gt;% \n  mutate(filename = basename(filename))\n\ntheSunall_files_df = theSunall_files_df %&gt;% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # turn the date column into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% \n  left_join(title_names_df, by = 'newspaper_id')# select all columns but with the article code first \n\nUse unnest_tokens to tokenise the data. Set the n parameter to 2, which will divide the text into bigrams:\n\ntheSunNgrams = theSunall_files_df %&gt;% \n  unnest_tokens(word, text, token = 'ngrams', n =2)\n\nWith this new dataset, filter to include only bigrams which contain the word liberal. Count these and visualise the top ten results:\n\ntheSunNgrams %&gt;%\n  filter(str_detect(word, \"^liberal \")) %&gt;% \n  mutate(year = year(date)) %&gt;%\n  count(word, year) %&gt;% \n  group_by(year) %&gt;% \n  slice_max(order_by = n, n = 10) %&gt;%\n    mutate(year = as.factor(year),\n           word = reorder_within(word, n, year)) %&gt;%\n  ggplot() + geom_col(aes(word, n)) + \n  facet_wrap(~year, scales = 'free') +\n    scale_x_reordered()+\n    scale_y_continuous(expand = c(0,0))+ coord_flip()\n\n\n\n\nThe results point to a huge change in the way that the word liberal is used in these two years of the newspaper. At the beginning of the century, the most common bigrams point to the general meaning of the word liberal, but by the end, the words are all related to liberal as a political ideology and party."
  },
  {
    "objectID": "term-freq.html#further-reading",
    "href": "term-freq.html#further-reading",
    "title": "10  N-gram Analysis",
    "section": "Further reading",
    "text": "Further reading\nThe best place to learn more is by reading the ‘Tidy Text Mining’ book available at https://www.tidytextmining.com. This book covers a whole range of text mining topics, including those used in the next few chapters."
  },
  {
    "objectID": "topic-modelling.html#methods",
    "href": "topic-modelling.html#methods",
    "title": "11  Topic Modelling",
    "section": "Methods",
    "text": "Methods\nThere are numerous ways to extract topics from documents. One which has been used extensively in digital humanities is Latent Dirichlet Allocation (LDA). The basic aim of the LDA algorithm is to figure out the mixture of words in each topic. It starts out by randomly assigning all the words into topics, and then, for each word in the data:\n\nIt assumes that all other words are in the correct topics.\nIt moves the word in question to each topic, and calculates the probability that each is the correct topic, based on the other words currently in that topic, and their relationship to the word and the other documents it appears within.\nIt does this lots of times until it reaches an optimal, stable state.\n\nLDA topic modelling has its limitations, for example it treats documents as ‘bag of words’, meaning their order is not taken into account. Often, the resulting topics (which are represented simply as a group of keywords) are not easy to interpret. Newer methods of topic modelling, such as contextual topic modelling (CTM) are available which take into account context and word order but these are not, as of now, implemented within any R packages."
  },
  {
    "objectID": "topic-modelling.html#topic-modelling-with-the-library-topicmodels",
    "href": "topic-modelling.html#topic-modelling-with-the-library-topicmodels",
    "title": "11  Topic Modelling",
    "section": "Topic modelling with the library ‘topicmodels’",
    "text": "Topic modelling with the library ‘topicmodels’\nThis chapter uses three libraries: tidyverse, tidytext, and topicmodels. If you are missing any of these, you can install them with the following code:\n\ninstall.packages('tidyverse')\ninstall.packages('tidytext')\ninstall.packages('topicmodels')\n\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(topicmodels)"
  },
  {
    "objectID": "topic-modelling.html#load-the-news-dataframe-and-relevant-libraries",
    "href": "topic-modelling.html#load-the-news-dataframe-and-relevant-libraries",
    "title": "11  Topic Modelling",
    "section": "Load the news dataframe and relevant libraries",
    "text": "Load the news dataframe and relevant libraries\nTopic modelling can be quite computationally-intensive. To speed things up, we’ll just look at a single newspaper title for the year 1855.\nEither construct your own corpus by following Chapter 8 and Chapter 9, or download and open the ready-made .zip file with all issues from 1855. Next, get these articles into the correct format. See Chapter 10 for an explanation of this code:\n\nnews_sample_dataframe = list.files(\"newspaper_text/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\n\nall_files = lapply(news_sample_dataframe, data.table::fread) \n\nnames(all_files) = news_sample_dataframe\n\nall_files_df = data.table::rbindlist(all_files, idcol = 'filename')\n\n\nnews_df = all_files_df %&gt;% \n  mutate(filename = basename(filename))\n\ntitle_names_df = tibble(newspaper_id = c('0002090', '0002194', '0002244', '0002642', '0002645', '0003089', '0002977'), newspaper_title = c('The Liverpool Standard And General Commercial Advertiser', 'The Sun', 'Colored News', 'The Express', 'The Press', 'Glasgow Courier', 'Swansea and Glamorgan Herald'))\n\nnews_df = news_df %&gt;% separate(filename, \n                               into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into some arbitrarily named colums\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% \n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # make a date column, and turn it into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% \n  left_join(title_names_df, by = 'newspaper_id')\n\nMake a new dataset of only one title: the Swansea and Glamorgan Herald, using its unique ID:\n\nnews_for_tm = news_df %&gt;% \n  filter(newspaper_id == '0003089')\n\nNext, use unnest_tokens to tokenise the data. For topic modelling, it can be good to remove stop words, and words which don’t occur very frequently. We’ll also remove any words made up of numbers, as an additional cleaning step. This is done using anti_join() and filter() with a regular expression to match any number.\n\ndata(\"stop_words\")\n\nnews_for_tm = news_for_tm %&gt;% \n  unnest_tokens(output = word, input = text) %&gt;% \n  anti_join(stop_words) %&gt;% \n  filter(!str_detect(word, \"[0-9]\"))\n\nJoining with `by = join_by(word)`"
  },
  {
    "objectID": "topic-modelling.html#create-a-dataframe-of-word-counts-with-tf_idf-scores",
    "href": "topic-modelling.html#create-a-dataframe-of-word-counts-with-tf_idf-scores",
    "title": "11  Topic Modelling",
    "section": "Create a dataframe of word counts with tf_idf scores",
    "text": "Create a dataframe of word counts with tf_idf scores\nThe LDA algorithm expects a count of the words found in each document. We will generate the necessary statistics using the tidytext package, as used in the previous chapter. First, make a dataframe of the words in each document, with the count and the tf-idf score. This will be used to filter and weight the text data.\nFirst, get the word counts for each article:\n\nissue_words = news_for_tm %&gt;% \n  group_by(article_code, word) %&gt;%\n  tally() %&gt;% \n  arrange(desc(n))\n\nNext, use bind_tf_idf() to get the tf_idf scores:\n\nissue_words_tf_idf = issue_words %&gt;% \n  bind_tf_idf(word, article_code, n)"
  },
  {
    "objectID": "topic-modelling.html#make-a-document-term-matrix",
    "href": "topic-modelling.html#make-a-document-term-matrix",
    "title": "11  Topic Modelling",
    "section": "Make a ‘document term matrix’",
    "text": "Make a ‘document term matrix’\nUsing the function cast_dtm() from the topicmodels package, make a document term matrix. This is a matrix with all the documents on one axis, all the words on the other, and the number of times that word appears as the value. We’ll also filter out words with a low tf-idf score, and only include words that occur at least 5 times.\n\ndtm_long &lt;- issue_words_tf_idf %&gt;% \n    filter(tf_idf &gt; 0.00006) %&gt;% \n  filter(n&gt;5) %&gt;%\n    cast_dtm(article_code, word, n)\n\nUse the LDA() functionfrom the topicmodels package to compute the model. For this function we need to specify the number of topics in advance, using the argument k, and we’ll set the random seed to a set number for reproducibility. It can take some time to run the model, depending on the size of the corpus.\n\nlda_model_long_1 &lt;- LDA(dtm_long,k =12, control = list(seed = 1234))\n\nThe object lda_model_long_1 is a list containing, amongst other things, a list of the topics and the words which make them up, and a list of the documents with the mixture of topics within them. To view this object easily, we can use the tidy function from the tidytext package. The tidy function is a generic, meaning it can be used in different ways depending on the input. In this case, the tidytext package contains a method specifically created for turning the outputs of the LDA topic model into a readable format. We can choose to look either at the beta (the mixture of terms in topics) or gamma (the mixture of topics in documents).\n\nbeta_result &lt;- tidytext::tidy(lda_model_long_1,matrix = 'beta')\n\ngamma_result &lt;- tidytext::tidy(lda_model_long_1,matrix = 'gamma')\n\nWhat information do these contain? Well, let’s look first at the mixture of keywords within a certain topic, using the ‘beta’ matrix:\n\nbeta_result %&gt;% filter(topic ==1) %&gt;% \n  slice_max(order_by = beta, n = 10) %&gt;% \n  kableExtra::kbl()\n\n\n\n\ntopic\nterm\nbeta\n\n\n\n\n1\nstreet\n0.1591182\n\n\n1\namp\n0.1189573\n\n\n1\nglasgow\n0.0875793\n\n\n1\nesq\n0.0351185\n\n\n1\nst\n0.0316020\n\n\n1\njohn\n0.0266981\n\n\n1\nsale\n0.0241553\n\n\n1\nbuchanan\n0.0209430\n\n\n1\njames\n0.0198344\n\n\n1\npublic\n0.0127428\n\n\n\n\n\n\n\nWe can plot the top words which make up each of the topics, to get an idea of how the articles have been categorised as a whole. Some of these make sense: there’s a topic which seems to be about university and education, one with words relating to poor laws, and a couple about disease in the army, as well as some more which contain words probably related to the Crimean war.\n\nbeta_result %&gt;% \n  group_by(topic) %&gt;% \n  slice_max(order_by = beta, n = 10) %&gt;%\n    ungroup()%&gt;%\n    mutate(topic = as.factor(topic),\n           term = reorder_within(term, beta, topic)) %&gt;%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\")+\n    scale_x_reordered() + \n    coord_flip()\n\n\n\n\nSample Topics\n\n\n\n\nThe result seems to have picked some sensible topics: Each is a mixture of words, and the most important words seem to be sensibly connected to each other. It looks like there is a topic of government news, one on the Crimean war, one on markets and the economy/trade, and so forth.\nWe can also look at the other view, and look at how each article has been assigned to topics. Let’s take a look at the document which has been signed the highest probability for topic 4, which seems to be about the Crimean War:\n\ngamma_result &lt;- tidytext::tidy(lda_model_long_1, 'gamma')\n\ngamma_result %&gt;% filter(topic == 4) %&gt;% arrange(desc(gamma)) %&gt;% head(10) %&gt;%\n  kableExtra::kbl()\n\n\n\n\ndocument\ntopic\ngamma\n\n\n\n\n94868\n4\n0.9996461\n\n\n90659\n4\n0.9995358\n\n\n90839\n4\n0.9986704\n\n\n89320\n4\n0.9984336\n\n\n89587\n4\n0.9978388\n\n\n89378\n4\n0.9978062\n\n\n94037\n4\n0.9977378\n\n\n94766\n4\n0.9977020\n\n\n92196\n4\n0.9976650\n\n\n95635\n4\n0.9976268\n\n\n\n\n\n\n\nWe can now read this document:\n\ntop_article = gamma_result %&gt;% filter(topic == 4) %&gt;% slice_max(order_by = gamma, n = 1) %&gt;% pull(document)\n\nnews_df %&gt;% filter(article_code ==top_article ) %&gt;% \n  mutate(text = str_trunc(text, 5000)) %&gt;%\n  kableExtra::kbl()\n\n\n\n\narticle_code\nnewspaper_id\ndate\nart\ntext\nnewspaper_title\n\n\n\n\n94868\n0003089\n1855-09-29\n2\nTILE ASSAULT ON SEBASTOPOL. STORMING AND CAPTURE or THU MALAKOFF, AND RETREAT or rue nussan. ( From the nmee Special arrespontkht) - FOURTH DIVISION CAMP, MONDAY, SIFT. JO.— Tho contest on which the eyes of Europe have been turned on long is nearly decided—the mat On which the hopes of so many mighty empires de- pended is all but determined. Sebastopol is in flames! The fleet, the object of so much dil;lomatic controversy, and so many bloody struggles, has dis- appeared in the deep! One more great act of car- nage has been added to the tremendous, but glorious tragedy, of which the whole world. from the roost civilised nations down to the most barbarous hordes of the East, has been the anxious and excited audience. To every one out hero the occurrences of the last few days seem prodigious, startling, and momentous. Time will show whether we duly ap- preciate them. On Saturday we felt that the great success of our valiant allies was somewhat tarnished by our own failure, and it ssas doubtful whether the Russians would abandon all hope of retaking the Malakoff. On Sunday, ere noon, we were walking about the streets of Sebastopol and gazing on its ruins. The army is now in suspense as to its future. The south side of the city is in the hands of the allies. On the north side the great eitadel and numerous regular forU, backed by enormous earthworks, and defended by a numerous army, bids us defiance across is narrow strip of water, and Russia may boaelthet she has not yet lost Sebastopol. Thu al- lied fieet remains outside, paralysed by Fort Con- stantine and its dependencies, and every one is going I about asking, What are we to do now? The last and decisive cannonade was, as the world knows ever so long ago, begun on the morning of Wednesday, Sept. .5, by the French, against the Rus. hien right, consisting of tho Quarantine Batteries, the Bastion Centrale, and the Bastion du Mat, with great vigour and effect, and at night began a devas- tating bombardment, in which all the allied batteries joined. A frigate was fired by a French shell and sunk at night. On the morning of the 6th the Eng- lish and French together opened the cannonade, be- neath which the Russian batteries were almost broken to pieces, and which they did not dare to answer. In Ilw evening the bombardment was renewed and kept up all night; a fire appeared behind the Redan, and the enemy seemed by their constant signalling to be in much uneasiness. It was observed that great quantities of forage were being sent across the bridge from the north to the south side, although there were no cavalry in the latter. On the 7th the cannonade was continued in salvoes as before, and it was remarked that the town showed, inanumuistake- able manner, the terrible energy of the nightly bom- bardment. Nearly every house within range was split and in ruins. The bridge between the north and the south side was much crowded all day with men and carts passing to and fro,and large convoys were seen ' entering and leaving the town at the north side. To- wards evening the head of the great dockyard shears, so long a prominent object from our bat- tries, caught fire, and burned fiercely in the high \\sititl, which was raging all day. A two-decker was set on fire by the French shells, and was de- stroyed, and a steamer was busily employed towing a large dismasted frigate to the dockyard, out of range. In the middle of the day there was a council of Generals, and at 2 o'clock it became generally known that the allies would assault the place at noon on the Bth, after a vigorous cannonade and bombardment. The hour was well selected, as it was certain that the Russians are accustomed to indulge in a siesta about that time. In the course of the night there was an explosion behind the Redan. And now comes the memorable DAT OF TUE ASSAULT. i'STIaI.Ay, SEPT. 8.--The weather changed sod- denly yesterday. This morning it became bitterly cold. A biting wind right from the north side of Sebastopol blew intolerable clouds of harsh dust into i our faces. The sun was obscured; the sky was of a leaden wintry grey. Early in the morning a strong force of cavalry, under the command of Col. Hodge, I was moved up to the front and formed a chain of sentries in front of Cathca...\nGlasgow Courier\n\n\n\n\n\n\n\nLooking at this, it looks like it might be a good way of understanding and categorising the articles in the newspaper. Let’s take another perspective. Take the documents which have as their highest-probability topic number 4, and see how they distribute over time. This might tell us something about how the war was reported over the year.\n\ngamma_result %&gt;% group_by(document) %&gt;% \n  slice_max(order_by = gamma, n = 1) %&gt;% \n  filter(topic == 4) %&gt;% \n  left_join(news_df %&gt;% \n              mutate(article_code = as.character(article_code)), \n            by =c('document' = 'article_code'))  %&gt;%\n    mutate(week = ymd(cut(date, 'month'))) %&gt;% ungroup() %&gt;% \n  mutate(article_word_count = str_count(text)) %&gt;% \n  count(week, wt = article_word_count) %&gt;% \n  ggplot() + geom_col(aes(x = week, y = n))\n\n\n\n\nThis gives us some sense of when reporting about the Crimean War may have peaked. September 1855 was an important time, because of the battle of Sevastepol. But it also looks like news about the war was fairly consistent across the year.\nYou can also group the articles by their percentage of each ‘topic’, and use this to find common thread between them - for more on this, see here:"
  },
  {
    "objectID": "topic-modelling.html#recommended-reading",
    "href": "topic-modelling.html#recommended-reading",
    "title": "11  Topic Modelling",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nMarjanen, Jani, Elaine Zosa, Simon Hengchen, Lidia Pivovarova, and Mikko Tolonen. “Topic Modelling Discourse Dynamics in Historical Newspapers,” 2020. https://doi.org/10.48550/ARXIV.2011.10428.\nhttps://www.tidytextmining.com/topicmodeling.html"
  },
  {
    "objectID": "word2vec.html#what-are-word-embeddings",
    "href": "word2vec.html#what-are-word-embeddings",
    "title": "12  Word Embeddings",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nThe use of word embeddings is a valuable tool for understanding the meaning of words within texts. They can be used to understand semantic change within vocabularies - how the meaning of words changed over time. Word embeddings have been heavily used in the context of historical newspapers, for example to examine the changing biases around gender in Dutch newspapers (Wevers 2019), or to understand concepts Wevers and Koolen (2020). In a nutshell, word embeddings are a mathematical representation of words in a corpus, taking into account how that word is used and its relationship to other words. This representation can be used for further tasks, such as measuring semantic change, as a form of text search, or as the input for machine learning models, for classification for example.\nThe basic premise of word embeddings is to assign a mathematical value to each word in a corpus. An embedding is a ultimately a point in Euclidean space. For example, a word in two-dimensional Euclidean space might be represented by the point c(2,2). A second word might be represented by the point c(1,1). If we draw these on a graph it would look like this:\n\n\n\n\n\nWhen represented on a graph like this, it’s easy to do mathematical calculations such as measuring the distance between the words. We can also easily calculate whether a word is closer to one or another. If we introduce a third word to the above:\n\n\n\n\n\nWe can easily tell (and also calculate, using a simple distance formula) that word 3 is closer to word 2 than it is to word 1.\nWe can also represent the words in more than two dimensions. It becomes more difficult to draw them, but the mathematics stays the same.\nWord embedding methods, then, essentially turn each word in a corpus into a vector - meaning a series of numbers representing its position in multi-dimensional space. In the example above, the words are represented by a vector of length two, such as c(1,1).\nThere are lots of ways of determining these word embeddings, and there is no single ‘true’ representation. A popular method is to look at the context of words and use this information as as way to determine the most appropriate vector by which each word can be represented. A model can be trained which means a set of embeddings can be developed where words which appear often in context with each other will be closer together within the multi-dimensional space of that corpus.\nImagine the above word 1 is something like cat, word 2 is a colour, say yellow, and word 3 is another colour, say red. Red and yellow are much more likely to be used in the same context (I have a red/yellow raincoat) than the word cat. The sentence I have a cat raincoat is less likely to occur in a corpus, though not impossible… This means that red and yellow are likely to be semantically similar. A good contextual word embedding model will mean they will be most likely be placed closer together in multi-dimensional space."
  },
  {
    "objectID": "word2vec.html#word-embedding-algorithms",
    "href": "word2vec.html#word-embedding-algorithms",
    "title": "12  Word Embeddings",
    "section": "Word Embedding Algorithms",
    "text": "Word Embedding Algorithms\nThere are a number of algorithms available for doing this. Two popular ones are word2vec, created in 2013 by researchers at Google, and the GloVe algorithm, created at Stanford in 2014. Both of these look at the context of words, and iterate over a process which tries to maximise in some way the relationship between the vectors and the context of the words, using neural networks.\nGloVe takes into account the overall word co-occurrence of the words in the data, alongside the local co-occurrence, which can be more efficient. Jay Alammar has an excellent explainer of word2vec, much of which also applies to GloVe.\nEssentially, these use neural networks to learn the best set of embeddings which are as good as possible at predicting the next word in the sentences found in the data - a form of unsupervised learning."
  },
  {
    "objectID": "word2vec.html#creating-word-embeddings-with-r-and-text2vec.",
    "href": "word2vec.html#creating-word-embeddings-with-r-and-text2vec.",
    "title": "12  Word Embeddings",
    "section": "Creating Word Embeddings with R and text2vec.",
    "text": "Creating Word Embeddings with R and text2vec.\nIn R, we can access these algorithms through a package called text2vec. Text2vec is a package which can do a number of NLP tasks, including word vectorisation. The package has a vignette which explains how to use the GloVe algorithm. I also recommend reading this tutorial by Michael Clark, which also uses text2vec.\nOn a practical level, the steps to use this package to generate embeddings are the following:\n\nConstruct the input data from the full texts of the newspapers extracted in previous chapters. This involves tokenising the data, and creating a count of the appearances of words in the data.\nNext, create a term co-occurrence matrix. This is a large matrix which holds information on how often words occur together. For the gloVe algorithm, we pick a ‘window’. The co-occurrence statistics count all instances of terms occurring together within this window, over the whole dataset.\nRun the gloVe algorithm on this co-occurrence matrix.\nConstruct the vectors from the resulting output.\n\n\nLoad libraries\nThis tutorial uses libraries used previously, plus a new one called text2vec. If you haven’t installed these (or some of them), you can do so with the following:\n\ninstall.packages('text2vec')\ninstall.packages('tidyverse')\ninstall.packages('tidytext')\n\nOnce you’re done, load the packages:\n\n#| warning: false\n#| message: false\n\nlibrary(text2vec)\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\n\nLoad and create the newspaper dataset\nEither construct your own corpus by following [Chapter -@sec-download] and [Chapter -@sec-extract], or download and open the ready-made .zip file with all issues from 1855. Next, get these articles into the correct format. See [Chapter -@sec-count] for an explanation of this code:\n\nnews_sample_dataframe = list.files(path = \"newspaper_text/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\n\nall_files = lapply(news_sample_dataframe, data.table::fread) \n\nnames(all_files) = news_sample_dataframe\n\nall_files_df = data.table::rbindlist(all_files, idcol = 'filename')\n    \n\ntitle_names_df = tibble(newspaper_id = c('0002090', '0002194', '0002244', '0002642', '0002645', '0003089', '0002977'), newspaper_title = c('The Liverpool Standard And General Commercial Advertiser', 'The Sun', 'Colored News', 'The Express', 'The Press', 'Glasgow Courier', 'Swansea and Glamorgan Herald'))\n\nnews_df = all_files_df %&gt;% \n  mutate(filename = basename(filename))\n\n\nnews_df = news_df %&gt;% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # turn the date column into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% # select all columns but with the article code first \n  left_join(title_names_df, by = 'newspaper_id') # join the titles \n\n\n\nCreate the correct input data\nThe first step is to create the vocabulary which will be used to later construct the term co-occurrence matrix.\nFirst, use unnest_tokens() (see the previous chapter) to get a list of the tokens within the data. Store the tokens column (called word) as a list:\n\nnews_tokens = news_df %&gt;% \n  unnest_tokens(output = word, input = text )\n\nnews_words_ls = list(news_tokens$word)\n\nNext, create the ‘iterator’ using a function from the text2vec package, itoken(). An iterator is an object which will tell our function how to move through the list of words.\n\nit = itoken(news_words_ls, progressbar = FALSE)\n\nCreate the vocabulary list by passing the iterator to the function create_vocabulary(). Furthermore, use prune_vocabulary() to remove very infrequent words, which won’t contain much information and in many cases may be OCR artefacts. You can experiment with this value, depending on the size of your dataset.\n\nnews_vocab = create_vocabulary(it)\n\nnews_vocab = prune_vocabulary(news_vocab, term_count_min = 10)\n\nConstruct the term co-occurrence matrix. To begin with, create a vectorizer using the function vocab_vectorizer(). As with iterator above, this creates an object whose job is to describe how to do something - in this case, how to map words in the correct way for the term co-occurrence matrix.\nUsing this, use the function create_tcm() to create the term co-occurrence matrix, using the iterator and the vectorizer created above. Specify the skip_grams_window paramter, which defines how large a context to consider when calculating the co-occurrence. The result, if you look at it, is a large sparse matrix, containing each pair of co-occurring words, and the number of times they co-occur in the data.\n\nvectorizer = vocab_vectorizer(news_vocab)\n\n# use window of 10 for context words\nnews_tcm = create_tcm(it, vectorizer, skip_grams_window = 10)\n\n\n\nRun the GloVe algorithm\nNext, run the GloVe algorithm. This is done by creating an R6 class object, rather than simply running a function. This is an example of using object-orientated programming. The process is slightly different. First, create an empty object of the GlobalVectors class using GlobalVectors$new() . Next, run the neural network using glove$fit_transform, specifying how many iterations and threads (processors) it should use. This will take some time to run.\n\nglove = GlobalVectors$new(rank = 50, x_max = 10)\nwv_main = glove$fit_transform(news_tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 5)\n\nINFO  [12:52:04.451] epoch 1, loss 0.1578\nINFO  [12:53:53.251] epoch 2, loss 0.1246\nINFO  [12:55:41.643] epoch 3, loss 0.1148\nINFO  [12:57:30.750] epoch 4, loss 0.1099\nINFO  [12:59:16.637] epoch 5, loss 0.1067\nINFO  [13:01:01.389] epoch 6, loss 0.1044\nINFO  [13:02:50.749] epoch 7, loss 0.1028\nINFO  [13:04:37.363] epoch 8, loss 0.1015\nINFO  [13:06:25.861] epoch 9, loss 0.1004\nINFO  [13:08:12.319] epoch 10, loss 0.0996\nINFO  [13:08:12.320] Success: early stopping. Improvement at iterartion 10 is less then convergence_tol\n\n\nOn the advice from the package vignette, the following takes the average of the main and context vectors, which usually produces higher-quality embeddings.\n\nnews_wv_context = glove$components\n\nnews_word_vectors = wv_main + t(news_wv_context)\n\nWith this, using the following we can extract the embeddings for a single word, and find its cosine similarity to all other words in the data, using the function sim2\n\nking = news_word_vectors[\"king\", , drop = FALSE]\n\ncos_sim = sim2(x = news_word_vectors, y = king, method = \"cosine\", norm = \"l2\")\n\n\nhead(sort(cos_sim[,1], decreasing = TRUE), 10) \n\n     king     queen  napoleon   emperor    prince   brother      duke  sardinia \n1.0000000 0.8156325 0.7556608 0.7513469 0.7023357 0.6931232 0.6873137 0.6798223 \n  majesty    albert \n0.6777707 0.6686118 \n\n\n\n\nLimitations of Word Embeddings\nAs a warning, word embeddings are going to be very closely related to the specific context of the corpus you are using. This can be a problem, but could also easily be exploited to find out interesting things about a particular corpus.\nNewspapers, like all texts, have their own particular history and way of writing. This means in some cases we may get surprising or unexpected results. To show how this works in practice, we’ll compare the most-similar embeddings for two European cities: Paris and Madrid.\nWe might assume that these two would be very similar and have similar ‘interchangeable’ words. They’re both European capital cities. The words used in a similar context to a city are, generally, other cities. Newspapers tend to talk about cities in a very similar way, after all. In terms of the news, one city is to a certain extent interchangeable with any other.\nTo test this, let’s extract the vectors for these two cities, using a similar method to above. First, Madrid:\n\nmadrid = news_word_vectors[\"madrid\", , drop = FALSE]\n\ncos_sim = sim2(x = news_word_vectors, y = madrid, method = \"cosine\", norm = \"l2\")\n\n\nhead(sort(cos_sim[,1], decreasing = TRUE), 20)\n\n    madrid      cadiz    trieste petersburg   portugal  barcelona      spain \n 1.0000000  0.7882828  0.7619828  0.7531655  0.7179562  0.7158912  0.7100895 \n   hamburg     lisbon marseilles      genoa     brings     naples      dated \n 0.7061993  0.6685570  0.6644486  0.6595788  0.6411570  0.6319918  0.6287576 \n    states    advices  gibraltar despatches   despatch      dates \n 0.6274122  0.6161796  0.6130921  0.6076462  0.6053111  0.6051016 \n\n\nAs expected, the most-similar words are other cities, Trieste, Cadiz, and so forth. Semantically, one city is sort of interchangeable for any other. Now let’s look at Paris:\n\nparis = news_word_vectors[\"paris\", , drop = FALSE]\n\ncos_sim = sim2(x = news_word_vectors, y = paris, method = \"cosine\", norm = \"l2\")\n\n\nhead(sort(cos_sim[,1], decreasing = TRUE), 20)\n\n        paris    petersburg        berlin        vienna      received \n    1.0000000     0.7040589     0.6968068     0.6879835     0.6697019 \n       france       emperor          news        london          says \n    0.6692741     0.6617469     0.6519818     0.6450786     0.6413042 \n     moniteur correspondent          here          rome     following \n    0.6400405     0.6330811     0.6217484     0.6148711     0.6119747 \n       french       hamburg     yesterday       letters       evening \n    0.6112651     0.6106243     0.6097684     0.6075599     0.6049715 \n\n\nThe result is a list which is much more mixed, semanttically. In this list there are a few cities (Vienna, Rome, London), but there are also words relating to the transmission of news (News, says, gazette, despatch, daily…).\nThis suggests that Paris is not just an interchangeable city from where news is reported, but has perhaps a more important role, as a key relay place from where news is being sent. News is not just reported from Paris, but it is a place where news is gathered from across Europe to be sent onwards across the Channel. This is reflected in the contextual word embeddings."
  },
  {
    "objectID": "word2vec.html#case-study---semantic-shifts-in-the-word-liberal-over-time-in-the-sun-newspaper",
    "href": "word2vec.html#case-study---semantic-shifts-in-the-word-liberal-over-time-in-the-sun-newspaper",
    "title": "12  Word Embeddings",
    "section": "Case study - semantic shifts in the word ‘liberal’ over time in The Sun Newspaper",
    "text": "Case study - semantic shifts in the word ‘liberal’ over time in The Sun Newspaper\nAs a final case study demonstrating how word embeddings might be used, we’ll look at how a particular concept shifts in a single newspaper over time. By looking at the most-similar words to a target word, in this case ‘liberal’, we can capture changes in the dominant meaning of the word. In Chapter 10, we did a bigram analysis (Section 10.7), which pointed to a change in the way the word liberal was used between 1802 and 1870.\nTo do so, we’ll use another dataset, this time, all issues of The Sun newspaper from two years: 1802 (the first full year in the repository data) and 1870 (the last full year). We’ll follow the same workflow as above, except create two entirely different set of word vectors for each time period. We can then look at the most similar words in each and make some conclusions. We can also compare the similarity of two words in each time period.\nYou can do this with any selection of newspapers from any dates. Here, I have already provided the full-text files for the two years of The Sun. If you want to use other titles or years, follow the steps outlined in chapters x and y.\nParticularly because we are looking at a single title, and, therefore, we might expect it to be more consistent in its editorial and writing practices, looking at shifts might tell us something about how the concept or word use of the word liberal was treated differently in the press over time.\nAt the same time, there may be many other hidden reasons for the change in the word. Perhaps it is used in a popular advertisement which ran at one time and not another? You should attempt to understand, for example through close reading or secondary sources, why the semantics of a word might look as they do. And of course, this may not reflect anything deeper about how the concept or ideology changed over time. But understanding how a word was represented in a title and how that changed might be a starting-point for further analysis.\nAs before, we will load in the files, create the token list, and the word embeddings using GloVe. The steps here are exactly as above, just repeated for each year in the data.\n\ntheSun = list.files(path = \"../../../Downloads/TheSun_sample/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\n\ntheSunall_files =   lapply(theSun, data.table::fread) \n\nnames(theSunall_files) = theSun\n\ntheSunall_files_df = data.table::rbindlist(theSunall_files, idcol = 'filename')\n\ntheSunall_files_df = theSunall_files_df %&gt;% \n  mutate(filename = basename(filename))\n\ntheSunall_files_df = theSunall_files_df %&gt;% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # turn the date column into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% \n  left_join(title_names_df, by = 'newspaper_id')# select all columns but with the article code first \n\n\ntheSunTokens = theSunall_files_df %&gt;% \n  unnest_tokens(word, text, token = 'words')\n\n\ntokens_1802 = theSunTokens %&gt;% \n  mutate(year = year(date)) %&gt;% \n  filter(year == 1802)\n\nwords_ls_1802 = list(tokens_1802$word)\n\nit_1802 = itoken(words_ls_1802, progressbar = FALSE)\n\nvocab_1802 = create_vocabulary(it_1802)\n\nvocab_1802 = prune_vocabulary(vocab_1802, term_count_min = 10)\n\n\ntokens_1870 = theSunTokens %&gt;% mutate(year = year(date)) %&gt;% \n  filter(year == 1870)\n\nwords_ls_1870 = list(tokens_1870$word)\n\nit_1870 = itoken(words_ls_1870, progressbar = FALSE)\n\nvocab_1870 = create_vocabulary(it_1870)\n\nvocab_1870 = prune_vocabulary(vocab_1870, term_count_min = 10)\n\n\nvectorizer_1802 = vocab_vectorizer(vocab_1802)\n\n# use window of 10 for context words\ntcm_1802 = create_tcm(it_1802, vectorizer_1802, skip_grams_window = 10)\n\nvectorizer_1870 = vocab_vectorizer(vocab_1870)\n\n# use window of 10 for context words\ntcm_1870 = create_tcm(it_1870, vectorizer_1870, skip_grams_window = 10)\n\n\nglove1802 = GlobalVectors$new(rank = 50, x_max = 10)\n\nwv_main_1802 = glove1802$fit_transform(tcm_1802, n_iter = 10, convergence_tol = 0.01, n_threads = 8)\n\nINFO  [13:10:46.640] epoch 1, loss 0.1605\nINFO  [13:11:00.082] epoch 2, loss 0.1128\nINFO  [13:11:13.368] epoch 3, loss 0.1000\nINFO  [13:11:26.602] epoch 4, loss 0.0927\nINFO  [13:11:40.156] epoch 5, loss 0.0878\nINFO  [13:11:53.250] epoch 6, loss 0.0843\nINFO  [13:12:06.721] epoch 7, loss 0.0816\nINFO  [13:12:20.455] epoch 8, loss 0.0796\nINFO  [13:12:34.263] epoch 9, loss 0.0779\nINFO  [13:12:47.824] epoch 10, loss 0.0765\n\nglove1870 = GlobalVectors$new(rank = 50, x_max = 10)\n\nwv_main_1870 = glove1870$fit_transform(tcm_1870, n_iter = 10, convergence_tol = 0.01, n_threads = 8)\n\nINFO  [13:13:15.975] epoch 1, loss 0.1982\nINFO  [13:13:44.104] epoch 2, loss 0.1426\nINFO  [13:14:09.805] epoch 3, loss 0.1271\nINFO  [13:14:38.126] epoch 4, loss 0.1187\nINFO  [13:15:06.659] epoch 5, loss 0.1134\nINFO  [13:15:33.960] epoch 6, loss 0.1096\nINFO  [13:16:02.349] epoch 7, loss 0.1068\nINFO  [13:16:30.317] epoch 8, loss 0.1046\nINFO  [13:16:57.710] epoch 9, loss 0.1028\nINFO  [13:17:25.649] epoch 10, loss 0.1014\n\n\n\nwv_context_1802 = glove1802$components\n\nword_vectors_1802 = wv_main_1802 + t(wv_context_1802)\n\nwv_context_1870 = glove1870$components\n\nword_vectors_1870 = wv_main_1870 + t(wv_context_1870)\n\nFinally, we can compare the two sets of embeddings for the word ‘liberal’.\n\nliberal_1802 = word_vectors_1802[\"liberal\", , drop = FALSE]\n\ncos_sim_liberal_1802 = sim2(x = word_vectors_1802, y = liberal_1802, method = \"cosine\", norm = \"l2\")\n\n\nliberal_1870 = word_vectors_1870[\"liberal\", , drop = FALSE]\n\ncos_sim_liberal_1870 = sim2(x = word_vectors_1870, y = liberal_1870, method = \"cosine\", norm = \"l2\")\n\n\nhead(sort(cos_sim_liberal_1802[,1], decreasing = TRUE), 10)\n\n  liberal    became    advice   support  sanction      thus      your     terms \n1.0000000 0.5570342 0.5376675 0.5272473 0.5238372 0.5071123 0.4978900 0.4969419 \n  highest suffrages \n0.4842931 0.4793520 \n\nhead(sort(cos_sim_liberal_1870[,1], decreasing = TRUE), 10)\n\n      liberal  conservative         party     candidate       members \n    1.0000000     0.7829457     0.6714207     0.6631544     0.6532358 \n     majority participation      feelings    candidates      friendly \n    0.6356385     0.5968122     0.5882970     0.5848294     0.5711921"
  },
  {
    "objectID": "word2vec.html#word-similarity-changes",
    "href": "word2vec.html#word-similarity-changes",
    "title": "12  Word Embeddings",
    "section": "Word similarity changes",
    "text": "Word similarity changes\nLet’s use the same embeddings for a slightly different perspective on semantic shift. Using the same methods, we can take any two embeddings and calculate a similarity score. Looking at how this score changed over time can be informative in understanding how a word’s meaning (or the relationship between two words) changed over time.\nDo this by again calculating the vectors for liberal, plus the word conservative. Do this for both 1802 and 1870.\nUse the same method to calculate similarity, but this time instead of calculate one against all, we calculate one against the other. This returns a single similarity score.\n\nliberal_1802 = word_vectors_1802[\"liberal\", , drop = FALSE]\nconservative_1802 = word_vectors_1802[\"conservative\", , drop = FALSE]\n\nsim2(x = liberal_1802, y = conservative_1802, method = \"cosine\", norm = \"l2\")\n\n        conservative\nliberal  -0.06736419\n\nliberal_1870 = word_vectors_1870[\"liberal\", , drop = FALSE]\nconservative_1870 = word_vectors_1870[\"conservative\", , drop = FALSE]\n\nsim2(x = liberal_1870, y = conservative_1870, method = \"cosine\", norm = \"l2\")\n\n        conservative\nliberal    0.7829457\n\n\nThe words go from being very dissimilar (rarely used in the same context) to being very similar - both are used in terms of party politics by 1870.\nA further, easy step, would be to create such a set of embeddings for sets of five years, and look at the change from one period to the next. This would help us to understand exactly when this shift occurred. A similar method has been used by researchers\nAs with the ngram analysis in a previous chapter, this points to a huge change in the semantic shift of the word liberal, in the newspapers.\nMention newer methods such as BERT which also produce word embeddings. Also mention this new LwM paper: https://muse.jhu.edu/pub/1/article/903976\nBERT uses a technology called a transformer, which itself uses what is known as ‘attention mechanism’. Rather than simply look at co-occurring words in a given sentence, transformers allow each word to share information with each other, meaning that important and related words within a sentence or chunk of text can be found. This means that the resulting embedding can take into account the specific context and even word order of a given phrase. Furthermore, BERT generates an embedding not at the corpus-level but at the word-level, meaning that each individual utterance of a word can be represented differently, according to the way it is used in a particular sentence. This is powerful tool which really tease apart the way words, particularly those which might have multiple meanings, semantically shift over time.\nAs an alternative - try doing the same analysis, but first remove all the articles labelled as advertisements from the previous tutorial.\nAlso, download all years of the Sun. Divide into 5-year chunks, compare the vector between them.\n\n\n\n\nWevers, Melvin. 2019. “Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990.” https://doi.org/10.48550/ARXIV.1907.08922.\n\n\nWevers, Melvin, and Marijn Koolen. 2020. “Digital Begriffsgeschichte: Tracing Semantic Change Using Word Embeddings.” Historical Methods: A Journal of Quantitative and Interdisciplinary History 53 (4): 226–43. https://doi.org/10.1080/01615440.2020.1760157."
  },
  {
    "objectID": "tidymodels.html#machine-learning",
    "href": "tidymodels.html#machine-learning",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Machine Learning",
    "text": "Machine Learning\nMachine learning is the name for a group of techniques which take input data of some kind, and learn how to achieve some particular goal. The ‘deep learning’ used by neural networks and in particular things like ChatGPT are one type, but the field has been around for much longer than that.\nMachine learning itself can be divided into subsets: Machine learning done with neural networks, and what we might call ‘classical’ machine learning, which use algorithms.\nA very simple form of machine learning is linear regression. Linear regression attempts to find the best fitting line through a dataset. Take this dataset of the flipper size and body mass of a group of observed penguins (from the R package palmerpenguins:\n\n\n\n\n\nBy visually inspecting this, we can guess that there is a statistical relationship between the length and mass: as one value gets higher, the other does too. We can use geom_smooth() and a linear model to predict the best line of fit through this dataset:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIf we measured the slope of this line (using simple geometry), its angle would tell us about the relationship between the two values.In this way, our model helps us to understand some underlying pattern in the dataset - that these two values are highly correlated. We could also use it to predict new values: if given the body mass for a new penguin, by using this line, we could predict its most likely flipper length, and in most cases be fairly accurate.\nThis is a form of simple machine learning. We give an algorithm (here, linear regression) a bunch of input data, in this case the body mass and bill flipper length for a group of penguins, and it provides a model (in this case, a line with a certain slope), which hopefully helps us to explain the existing data and predict unknown parts. Models like this try in some way to minimise a loss function. In this case, finding the line where the error for each point (how far away it is from the line) is as small as possible.\nThe machine learning here will use basically the same principle. We’ll give it data (text, in the form of a mathematical representation of the text, and its assigned category), and an algorithm (in this case, a ‘Random Forest’ model), and use that to predict the category of unseen texts. The model itself may also tell us something about the existing data.\nMuch of the AI work at the cutting edge of text analysis today uses neural networks, and in particular the ‘transformer’ mechanism which allows neural networks to better understand context and word order. But this kind of machine learning still has a very important role to play, it’s well-established, relatively easy to run, and the results can be quite good depending on the problem. It is also a good way to get started with understanding the Tidymodels framework."
  },
  {
    "objectID": "tidymodels.html#the-tidymodels-package",
    "href": "tidymodels.html#the-tidymodels-package",
    "title": "13  Machine Learning with Tidymodels",
    "section": "The Tidymodels Package",
    "text": "The Tidymodels Package\nTidymodels is an R ‘meta-package’ which allows you to interact with lots of different machine learning packages and engines in a consistent way. Using tidymodels, we can easily compare multiple models together and we can swap in one for another without having to re-do code. We also can make sure that our pre-processing steps are precisely consistent across any number of different models.\nMachine learning can be used for a number of different tasks. One key one is text classification.\nIn this tutorial, we will use tidymodels to classify text into articles and advertisements. It could easily be generalised to any number of categories, for example foreign news, court reporting, and so forth. You can provide your own spreadsheet of training data (as we’ll use below), and as long as it is in the same format and has similar information, you should be able to build your own classifier.\nOnce we have built the model, we will fine-tune it. The random forest algorithm we’ll use, like most machine learning algorithms, has a group of parameters which can be adjusted. To find the best values for these, we’ll evaluate the same data using many different combinations of parameters, and pick the best one. Using tidymodels, this can all be done consistently. We could even swap out any other model type, and otherwise reuse the exact same workflow.\nFinally, we’ll put the model to some use: we’ll use the model to predict the class of the rest of the articles in the newspaper dataset, and do some analysis on this.\nThe model is only as good as the training data, and in this case, we don’t have many examples, and it won’t be terribly accurate in many cases. But it will show how a machine learning model can be operationalised for this task.\n\nBasic Steps\nIn this chapter we’ll create a model which can label newspaper as either articles or advertisements. We’ll do some further steps to explore improving the model, and also to look at the most important ‘features’ used by the model to do its predicting. The steps are:\n\nDownload a labelled dataset, containing examples of our two classes (articles and advertisements).\nCreate a ‘recipe’ which is a series of pre-processing steps. This same recipe can be reused in different contexts and for different models.\nSplit the labelled data into testing and training sets. The training data is used to fit the best model. The test set is used at the end, to see how well it performs on unseen data.\nRun an initial classifier and evaluate the results\n‘Tune’ the model, by re–running the classifier with different parameters, selecting the best one.\nRun the ‘best’ model over the full news dataset, predicting whether or not an articles is news or an advertisement."
  },
  {
    "objectID": "tidymodels.html#installload-the-packages",
    "href": "tidymodels.html#installload-the-packages",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Install/load the packages",
    "text": "Install/load the packages\nAs a first step, you’ll need to load the necessary packages for this tutorial. If you haven’t already done so, you can install them first using the below.\nIf you have installed them, make sure you update to the latest version, as they can change rapidly.\n\ninstall.packages('tidyverse')\ninstall.packages('tidymodels')\ninstall.packages('textrecipes')\ninstall.packages('tidytext')\ninstall.packages('ranger')\ninstall.packages('vip')\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(tidytext)\nlibrary(ranger)\nlibrary(vip)"
  },
  {
    "objectID": "tidymodels.html#import-data",
    "href": "tidymodels.html#import-data",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Import data",
    "text": "Import data\nAs a first step, load some pre-labelled data. This contains a number of newspaper articles, and their ‘type’: whether it is an advertisement or an article.\nIf we take a look at the dataframe once it is loaded, you’ll see it’s quite simple structure: it’s got a ‘filename’ column, the full text of the article store in ‘text’, and the type stored in a column called ‘type’.\n\nadvertisements_labelled = read_csv('advertisements_labelled.csv')\n\nadvertisements_labelled = advertisements_labelled %&gt;% filter(!is.na(text))"
  },
  {
    "objectID": "tidymodels.html#set-up-the-machine-learning-model",
    "href": "tidymodels.html#set-up-the-machine-learning-model",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Set up the Machine Learning Model",
    "text": "Set up the Machine Learning Model\nIn this step, we begin preparing the data for machine learning. We set a seed for reproducibility to ensure consistent results when randomization is involved. The target variable ‘type’ in the advertisements data is converted to a factor as it represents categorical classes (‘advertisement’ and ‘article’).\nThe data is then split into training and testing sets using the initial_split function. This separation is crucial for evaluating the performance of the machine learning model and preventing overfitting.\n\nset.seed(9999)\nadvertisements_labelled = advertisements_labelled %&gt;% \n  mutate(type = factor(type))\n\nadvertisements_split &lt;- initial_split(advertisements_labelled, strata = type)\n\nadvertisements_train &lt;- training(advertisements_split)\nadvertisements_test &lt;- testing(advertisements_split)"
  },
  {
    "objectID": "tidymodels.html#create-recipe-for-text-data",
    "href": "tidymodels.html#create-recipe-for-text-data",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Create Recipe for Text Data",
    "text": "Create Recipe for Text Data\nTo prepare the text data for modeling, we create a recipe using the recipe function. The textrecipes package provides essential tools for text preprocessing and feature extraction in machine learning. In this recipe, we tokenize the text, remove stop words, and apply a term frequency transformation to represent the text data as numerical features. These transformations convert the raw text data into a format suitable for machine learning algorithms, enabling them to process and understand textual information.\n\nadvertisement_rec &lt;-\n  recipe(type ~ text, data = advertisements_train)\n\nadvertisement_rec &lt;- advertisement_rec %&gt;%\n  step_tokenize(text, token = \"words\") %&gt;%\n  step_tokenfilter(text, max_tokens = 1000, min_times = 5 )  %&gt;% \n  step_tf(text)\n\nNext, we set up a workflow() object, which will store the recipe and later the model instructions, and make it easier to reuse.\n\nadvertisement_wf &lt;- workflow() %&gt;%\n  add_recipe(advertisement_rec)\n\nCreate the model. In this case, we’ll use a random forest model, from the package ranger. Setting the importance = \"impurity\" parameter means we’ll be able to see what words the model used to make its decisions.\n\nrf_spec &lt;- rand_forest(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\nTake the workflow object we made above, and add the model to it. After this, use fit to run the model, specifying it should use the advertisements_train dataset we created above.\nFinally, we use predict on this fitted model, specifying advertisements_test as the dataset. Add the true labels from advertisements_test as a new column, and use accuracy() to compare the true label with the prediction to get an accuracy score.\n\n advertisement_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit(data = advertisements_train)%&gt;%\n  predict(new_data = advertisements_test) %&gt;%\n  mutate(truth = advertisements_test$type) %&gt;%\n  accuracy(truth, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.811"
  },
  {
    "objectID": "tidymodels.html#perform-cross-validation",
    "href": "tidymodels.html#perform-cross-validation",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Perform Cross-Validation",
    "text": "Perform Cross-Validation\nTo evaluate the model further, we’ll use cross-validation. Cross-validation is a crucial step in model evaluation. It helps assess the model’s generalization performance on unseen data and reduces the risk of overfitting.\nIn this step, we set up cross-validation folds using the vfold_cv function, which creates multiple training and testing sets from the training data. The model will be trained and evaluated on each fold separately, providing a more robust estimate of its performance.\n\nset.seed(234)\nadvertisements_folds &lt;- vfold_cv(advertisements_train)"
  },
  {
    "objectID": "tidymodels.html#train-the-random-forest-model-with-cross-validation",
    "href": "tidymodels.html#train-the-random-forest-model-with-cross-validation",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Train the Random Forest Model with Cross-Validation",
    "text": "Train the Random Forest Model with Cross-Validation\nNow, we train the Random Forest model using cross-validation. The fit_resamples function fits the model to each fold created during cross-validation, allowing us to evaluate its performance across different subsets of the training data. The control_resamples function is used to control various settings during the resampling process.\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(advertisement_rec) %&gt;%\n  add_model(rf_spec)\n\nrf_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\n\nrf_rs &lt;- fit_resamples(\n  rf_wf,\n  advertisements_folds,\n  control = control_resamples(save_pred = TRUE)\n)"
  },
  {
    "objectID": "tidymodels.html#evaluate-the-models-performance",
    "href": "tidymodels.html#evaluate-the-models-performance",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Evaluate the Model’s Performance",
    "text": "Evaluate the Model’s Performance\nIn this step, we collect the evaluation metrics and predictions from the cross-validation process. The collected metrics will help us assess the model’s performance, while the predictions on each fold will be used for further analysis and comparison. By evaluating the model on multiple subsets of the data, we can gain insights into its robustness and reliability.\n\nrf_rs_metrics &lt;- collect_metrics(rf_rs)\nrf_rs_predictions &lt;- collect_predictions(rf_rs)"
  },
  {
    "objectID": "tidymodels.html#visualize-the-confusion-matrix",
    "href": "tidymodels.html#visualize-the-confusion-matrix",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Visualize the Confusion Matrix",
    "text": "Visualize the Confusion Matrix\nThe confusion matrix is a useful visualization for evaluating the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. The autoplot function from the yardstick package allows us to visualize the confusion matrix as a heatmap. This visualization aids in understanding the model’s classification accuracy and any potential misclassifications.\n\nconf_mat_resampled(rf_rs, tidy = FALSE) %&gt;%\n  autoplot(type = \"heatmap\")\n\n\n\n\nWe can see that there are very few true advertisements which are misclassified as articles - but there are some some articles misclassified as advertisements."
  },
  {
    "objectID": "tidymodels.html#tune-the-random-forest-model",
    "href": "tidymodels.html#tune-the-random-forest-model",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Tune the Random Forest Model",
    "text": "Tune the Random Forest Model\nIn machine learning, hyperparameter tuning is essential for optimizing model performance. In this step, we define a tuning grid using the rand_forest function. The grid specifies different combinations of hyperparameters, such as the number of variables randomly sampled for splitting (mtry) and the minimum number of samples per leaf node (min_n). We aim to find the best combination of hyperparameters that yields the highest performance.\n\ntune_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = 1000,\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n\ntune_wf &lt;- workflow() %&gt;%\n  add_recipe(advertisement_rec) %&gt;%\n  add_model(tune_spec)"
  },
  {
    "objectID": "tidymodels.html#tune-the-random-forest-model-with-cross-validation",
    "href": "tidymodels.html#tune-the-random-forest-model-with-cross-validation",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Tune the Random Forest Model with Cross-Validation",
    "text": "Tune the Random Forest Model with Cross-Validation\nNow, we perform hyperparameter tuning using cross-validation. The tune_grid function uses the tuning grid specified earlier and fits the model on each fold of the data to identify the optimal hyperparameters. This process helps us identify the best hyperparameters for the Random Forest model, leading to improved performance and better generalization.\n\nset.seed(555)\ntrees_folds = vfold_cv(advertisements_train)\n\n\ndoParallel::registerDoParallel()\n\nset.seed(666)\ntune_res = tune_grid(\n  tune_wf,\n  resamples = trees_folds,\n  grid = 20\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntune_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits          id     .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [95/11]&gt; Fold01 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [95/11]&gt; Fold02 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [95/11]&gt; Fold03 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [95/11]&gt; Fold04 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [95/11]&gt; Fold05 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [95/11]&gt; Fold06 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [96/10]&gt; Fold07 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [96/10]&gt; Fold08 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [96/10]&gt; Fold09 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [96/10]&gt; Fold10 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "tidymodels.html#visualize-the-tuning-results",
    "href": "tidymodels.html#visualize-the-tuning-results",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Visualize the Tuning Results",
    "text": "Visualize the Tuning Results\nTo visualize the tuning results, we plot the average Area Under the Receiver Operating Characteristic Curve (ROC AUC) against different values of mtry and min_n. ROC AUC is a common metric for assessing the model’s ability to discriminate between classes. The plot provides insights into how different hyperparameter values affect the model’s performance.\n\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, min_n, mtry) %&gt;%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %&gt;%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n\n\n\n\nIt’s a little difficult to interpret, but it looks like the highest values are between about 40 and 250 for the mtry value, and around 10 and 30 for the min_n value. We can do another grid search, this time just looking between these values.\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(40, 250)),\n  min_n(range = c(10,30)),\n  levels = 5\n)\n\n\nset.seed(999)\nregular_res &lt;- tune_grid(\n  tune_wf,\n  resamples = trees_folds,\n  grid = rf_grid\n)\n\n\nregular_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  mutate(min_n = factor(min_n)) %&gt;%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"AUC\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nbest_auc &lt;- select_best(regular_res, \"roc_auc\")\n\nfinal_rf &lt;- finalize_model(\n  tune_spec,\n  best_auc\n)\n\nLastly, evaluate the accuracy using the same method as before:\n\nadvertisement_wf %&gt;%\n  add_model(final_rf) %&gt;%\n  fit(data = advertisements_train)%&gt;%\n  predict(new_data = advertisements_test) %&gt;%\n  mutate(truth = advertisements_test$type) %&gt;%\n  accuracy(truth, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.865\n\n\nAs you can see, we have improved the model’s performance, from about .80 to about .86. There are other ways you could try to improve the performance further. With text, the pre-processing steps can often make a huge difference. You could experiment with the text ‘recipe’, for instance adjusting step_tokenfilter to include or remove more tokens. You could also use step_word_embeddings, using the output of the word embeddings in Chapter 12."
  },
  {
    "objectID": "tidymodels.html#build-the-final-random-forest-model",
    "href": "tidymodels.html#build-the-final-random-forest-model",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Build the Final Random Forest Model",
    "text": "Build the Final Random Forest Model\nIn this step, we build the final Random Forest model using the best hyperparameters obtained from tuning. The model is then fitted on the entire training dataset to capture the relationships between features and target classes optimally. This final model is the one that we will use for making predictions on new data.\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(advertisement_rec) %&gt;%\n  add_model(final_rf)\n\nfinal_res &lt;- final_wf %&gt;%\n  last_fit(advertisements_split)\n\nfinal_res %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.865 Preprocessor1_Model1\n2 roc_auc  binary         0.888 Preprocessor1_Model1"
  },
  {
    "objectID": "tidymodels.html#what-features-is-the-model-using",
    "href": "tidymodels.html#what-features-is-the-model-using",
    "title": "13  Machine Learning with Tidymodels",
    "section": "What features is the model using?",
    "text": "What features is the model using?\nTo understand how the model is using the text to make decisions, we can look at the most important features (in this case, text frequency counts) used by the random forest algorithm. We’ll use the package vip to extract the most important features, using the function extract_fit_parsnip(), and plotting it using ggplot2.\nFor the random forest method, we can only see the overall top features, and not which were more important for the prediction of the different categories. The most important feature is the frequency of the word ‘street’: at a guess, this is used more often in advertisements, which very often contain an address to a business or service. Interestingly, the second most important feature is the word ‘was’. Is this perhaps because news articles are more likely to use the past tense than advertisements? Interestingly, the frequency of certain function words such as the, and, to, of (which would often be filtered out as ‘stop words’) also seems to be important to the model. One explanation is that the use of these words is quite different in prose and in the kind of text used in advertisements.\n\ncomplaints_imp &lt;- extract_fit_parsnip(final_res$.workflow[[1]]) %&gt;%\n  vi(lambda = choose_acc$penalty)\n\ncomplaints_imp %&gt;% \n  mutate(Variable = str_remove(Variable, \"tfidf_text_\")) %&gt;% \n  head(20) %&gt;% \n  ggplot() + \n  geom_col(aes(x = reorder(Variable,Importance), y= Importance)) + coord_flip()"
  },
  {
    "objectID": "tidymodels.html#using-the-model",
    "href": "tidymodels.html#using-the-model",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Using the Model",
    "text": "Using the Model\nOnce we are happy with the final model, we can use it to label unseen data as either articles or advertisements. For this, we’ll need a newspaper corpus. As in previous chapters, either construct your own corpus by following Chapter 8 and Chapter 9, or download and open the ready-made .zip file with all issues from 1855. Next, get these articles into the correct format. See Chapter 10 for an explanation of this code:\n\nnews_sample_dataframe = list.files(path = \"newspaper_text/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\n\nall_files = lapply(news_sample_dataframe, data.table::fread) \n\nnames(all_files) = news_sample_dataframe\n\nall_files_df = data.table::rbindlist(all_files, idcol = 'filename')\n    \n\ntitle_names_df = tibble(newspaper_id = c('0002090', '0002194', '0002244', '0002642', '0002645', '0003089', '0002977'), newspaper_title = c('The Liverpool Standard And General Commercial Advertiser', 'The Sun', 'Colored News', 'The Express', 'The Press', 'Glasgow Courier', 'Swansea and Glamorgan Herald'))\n\nnews_df = all_files_df %&gt;% \n  mutate(filename = basename(filename))\n\n\nnews_df = news_df %&gt;% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # turn the date column into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% # select all columns but with the article code first \n  left_join(title_names_df, by = 'newspaper_id') # join the titles"
  },
  {
    "objectID": "tidymodels.html#make-predictions-on-newspaper-articles",
    "href": "tidymodels.html#make-predictions-on-newspaper-articles",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Make Predictions on Newspaper Articles",
    "text": "Make Predictions on Newspaper Articles\nWith the final Random Forest model trained, we proceed to make predictions on the newspaper articles’ text data. This involves applying the text preprocessing steps (e.g., tokenization, TF-IDF) used during training to transform the new data into the same format. The model then predicts whether each article is an advertisement or not.\n\nnew_ads_to_check = final_wf %&gt;%\n  fit(data = advertisements_train)%&gt;%\n  predict(new_data = news_df)\n\nall_files_df = all_files_df %&gt;% \n  mutate(prediction =new_ads_to_check$.pred_class)"
  },
  {
    "objectID": "tidymodels.html#analyze-the-top-words-in-advertisements",
    "href": "tidymodels.html#analyze-the-top-words-in-advertisements",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Analyze the Top Words in Advertisements",
    "text": "Analyze the Top Words in Advertisements\nAfter making predictions, we perform text analysis to identify the top words associated with advertisements. We tokenize the text data, count the occurrences of each word for each prediction, and select the most frequent words in advertisements. This analysis allows us to gain insights into the language patterns characteristic of advertisements.\n\ndata('stop_words')\n\n\nall_files_df %&gt;% \n  head(10000) %&gt;% \n  unnest_tokens(word, text) %&gt;% \n  filter(!str_detect(word, \"[0-9]\")) %&gt;% \n  anti_join(stop_words) %&gt;% \n  count(prediction, word) %&gt;% \n  group_by(prediction) %&gt;% \n  slice_max(order_by = n, n = 10) %&gt;% ungroup() %&gt;% \n    mutate(prediction = as.factor(prediction),\n           word = reorder_within(word, n, prediction)) %&gt;%\n  ggplot() + geom_col(aes(word, n)) + \n  facet_wrap(~prediction, scales = 'free') +\n    scale_x_reordered()+\n    scale_y_continuous(expand = c(0,0))+ coord_flip()\n\nJoining with `by = join_by(word)`\n\n\n\n\n\nAdvertisements often contain an address, including the word ‘street’, unlike articles. This matches the findings from the top features, above. Advertisements also often specify an exact time and date, resulting in higher counts of the word ‘o’clock’."
  },
  {
    "objectID": "tidymodels.html#analyze-the-proportion-of-advertisements-in-each-newspaper-issue",
    "href": "tidymodels.html#analyze-the-proportion-of-advertisements-in-each-newspaper-issue",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Analyze the Proportion of Advertisements in Each Newspaper Issue",
    "text": "Analyze the Proportion of Advertisements in Each Newspaper Issue\nFinally, we analyze the proportion of advertisements in each newspaper issue. We group the articles by their newspaper_id and date, calculate the proportion of advertisements in each issue, and then compute the mean proportion for each newspaper. This analysis helps us understand the prevalence of advertisements in different newspapers over time.\n\nnews_df$type = new_ads_to_check$.pred_class\n\nnews_df =news_df  %&gt;% mutate(count = str_count(text))\n\nnews_df =news_df  %&gt;% mutate(issue_code = paste0(newspaper_id, \"_\", date))\n\nnews_df %&gt;% \n  group_by(newspaper_title, issue_code, type) %&gt;%\n  summarise(n = sum(count)) %&gt;%\n  mutate(prop = n/sum(n)) %&gt;% \n  group_by(newspaper_title, type) %&gt;% \n  summarise(mean_prop = mean(prop)) %&gt;% \n  ggplot() + \n  geom_col(aes(x = str_trunc(newspaper_title,30), y = mean_prop, fill = type)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n`summarise()` has grouped output by 'newspaper_title', 'issue_code'. You can\noverride using the `.groups` argument.\n`summarise()` has grouped output by 'newspaper_title'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nInterestingly, some of the regional titles (Liverpool, Glasgow, Swansea and Glamorgan) seem to have much higher proportions of advertisements - provided the labelling by the machine learning model is largely correct, of course."
  },
  {
    "objectID": "tidymodels.html#recommended-reading",
    "href": "tidymodels.html#recommended-reading",
    "title": "13  Machine Learning with Tidymodels",
    "section": "Recommended Reading",
    "text": "Recommended Reading\nSupervised Machine Learning for Text Analysis in R\nBroersma, Marcel, and Frank Harbers. “Exploring Machine Learning to Study the Long-Term Transformation of News.” In Journalism History and Digital Archives, edited by Henrik Bødker, 1st ed., 38–52. Routledge, 2020. https://doi.org/10.4324/9781003098843-4."
  },
  {
    "objectID": "text-reuse.html#find-reused-text-with-the-package-textreuse",
    "href": "text-reuse.html#find-reused-text-with-the-package-textreuse",
    "title": "14  Text Reuse",
    "section": "Find reused text with the package textreuse",
    "text": "Find reused text with the package textreuse\nThere are many methods by which reused text can be found. Usually these involve finding overlapping chunks of text. It is complicated when the OCR is not perfect - it’s very unlikely that long strings of exactly the same text will be found, because of small differences. One package to do this in R is the textreuse package.\nIn this chapter, we’ll demonstrate how to use this package and view and interpret the results from a sample of newspaper issues covering a single month."
  },
  {
    "objectID": "text-reuse.html#background",
    "href": "text-reuse.html#background",
    "title": "14  Text Reuse",
    "section": "Background",
    "text": "Background\nFor this tutorial, we’ll use a subset of the 1855 newspaper text data, taken from the Shared Research Repository. In 1855, despite the invention and spread of the telegraph, much of the circulation of news from London to regional hubs relied on the copies of newspapers or reports travelling by railway. News from overseas might arrive first in London where it was printed in papers there, and then sent onwards via the next train to Liverpool, Edinburgh, and so forth. This was particularly true of non-urgent or timely reports.\nComputationally detecting text reuse will show some of this pattern, and is a way of understanding more about the circulation of news. By looking at the source newspapers for text printed in regional papers, we can get an idea of the patterns of news flow from one place to another. The results will show a variety of reuse: not just reused articles, but repeated advertisements, and quotations from third parties."
  },
  {
    "objectID": "text-reuse.html#method",
    "href": "text-reuse.html#method",
    "title": "14  Text Reuse",
    "section": "Method",
    "text": "Method\nThe textreuse package expects a corpus in the form of a folder of text documents. To get the data in this format, the first step is to take the dataframe of articles we have been using throughout the book, filter to a single month, and export the text each article to a separate text document, in a new folder.\nFollowing this, we’ll use functions from the textreuse package to generate a datasetset of pairs of articles and a similarity score.\nThe final step is to merge this set of similarity scores with the text and metadata of the newspaper articles, so we can explore, read, and visualise the results.\n\nLoad necessary libraries\nAs in previous chapters, the first step is to load the libraries used throughout the tutorial If they are not installed, you can do so using the following:\n\ninstall.packages('tidyverse')\ninstall.packages('textreuse')\ninstall.packages('data.table')\ninstall.packages('ggtext')\n\n\nlibrary(tidyverse)\nlibrary(textreuse)\nlibrary(data.table)\nlibrary(ggtext)\n\n\n\nLoad the dataframe and preprocess\nIn the extract text chapter @ref(label), you created a dataframe, with one row per article. The first step is to reload that dataframe into memory, and do some minor preprocessing.\n\nCombine Newspaper Articles into One Dataframe\nAs in previous chapters, either construct your own corpus by following Chapter 8 and Chapter 9, or download and open the ready-made .zip file with all issues from 1855. Next, get these articles into the correct format. See Chapter 10 for an explanation of this code:\n\nnews_sample_dataframe = list.files(path = \"newspaper_text/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\n\nall_files = lapply(news_sample_dataframe, data.table::fread) \n\nnames(all_files) = news_sample_dataframe\n\nall_files_df = data.table::rbindlist(all_files, idcol = 'filename')\n    \n\ntitle_names_df = tibble(newspaper_id = c('0002090', '0002194', '0002244', '0002642', '0002645', '0003089', '0002977'), newspaper_title = c('The Liverpool Standard And General Commercial Advertiser', 'The Sun', 'Colored News', 'The Express', 'The Press', 'Glasgow Courier', 'Swansea and Glamorgan Herald'))\n\nnews_df = all_files_df %&gt;% \n  mutate(filename = basename(filename))\n\n\nnews_df = news_df %&gt;% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %&gt;% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %&gt;% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %&gt;% \n  mutate(date = ymd(date)) %&gt;% # turn the date column into date format\n  mutate(article_code = 1:n()) %&gt;% # give every article a unique code\n  select(article_code, everything()) %&gt;% # select all columns but with the article code first \n  left_join(title_names_df, by = 'newspaper_id') # join the titles \n\n\n\n\nAdd a unique article code to be used in the text files\nMake a more useful code to use as an article ID. First use str_pad() to add leading zeros up to a maximum of three digits.\n\nnews_df$article_code = str_pad(news_df$article_code, \n                                             width = 3, \n                                             pad = '0')\n\nUse paste0() to add the prefix ‘article’ to this number.\n\nnews_df$article_code = paste0('article_',\n                                            news_df$article_code)\n\n\n\nFilter to a single month\nLet’s limit the newspapers to articles from a single month. When text is being reused in newspapers, it would be reasonable to assume that it is more likely that the reusing text will do so shortly after the source was originally published. Comparing all the articles for a single month may be a good way to pick up some interesting reuse.\nUse filter() to restrict the data to a single month, and save this as a new object:\n\nsample_for_text_reuse = news_df %&gt;% \n  mutate(month = as.character(cut(date, 'month'))) %&gt;% \n  filter(month == '1855-09-01')\n\n\n\nExport each article as a separate text file\nThe code below loops over each row in the news_sample_dataframe, writes the fifth cell (which is where the text of the article is stored) to a text file, using a function from a library called data.table called fwrite(), stores it in a folder called textfiles/, and makes a filename from the article code concatenated with ‘.txt’.\nBefore you do this, create an empty folder first, in the project directory, called textfiles\nOnce this has finished running, you should have a folder in the project folder called textfiles, with a text document for each article within it.\n\nfor(i in 1:nrow(sample_for_text_reuse)){\n    \n    \n  filename = paste0(\"textfiles/\", sample_for_text_reuse[i,1],\".txt\")\n  \n  writeLines(sample_for_text_reuse[i,5],con = filename)\n}"
  },
  {
    "objectID": "text-reuse.html#load-the-files-as-a-textreusecorpus",
    "href": "text-reuse.html#load-the-files-as-a-textreusecorpus",
    "title": "14  Text Reuse",
    "section": "Load the files as a TextReuseCorpus",
    "text": "Load the files as a TextReuseCorpus\nAfter this, we’ll import the files in this textfiles folder, and store them as an object from the textreuse package: a TextReuseCorpus.\n\nGenerate a minhash\nThe textreuse package uses minhashes to more efficiently store and compare the text documents (more information here). Before we generate the TextReuseCorpus, use the minhash_generator to specify the number of minhashes you want to represent each document. Set a random seed to make it reproducible:\n\nminhash &lt;- minhash_generator(n = 200, seed = 1234)\n\n\n\nCreate the TextReuseCorpus\nNext, create the TextReuseCorpus\nThe function TextReuseCorpus() expects a number of parameters:\ndir = is the directory where all the text files are stored.\ntokenizer is the function which tokenises the text. Here we’ve used tokenize_ngrams, but you could use characters, or build your own.\nn sets the length of the ngrams for the tokenizer. n = 2 means the documents will be split into bigrams (sequences of two tokens).\nminhash_func = is the parameters set using minhash_generator() above.\nkeep_tokens = Whether or not you keep the actual tokens, or just the hashes. There’s no real point keeping the tokens as we use the hashes to make the comparisons.\nThis function can take a long time to run with a large number of documents.\n\nreusecorpus &lt;- TextReuseCorpus(dir = \"textfiles/\", \n                               tokenizer = tokenize_ngrams, \n                               n = 2,\n                          minhash_func = minhash, \n                          keep_tokens = FALSE, \n                          progress = FALSE)\n\nNow each document is represented by a series of hashes, which are substitutes for small sequences of text. For example, this is the first ten minhashes for the first article:\n\nhead(minhashes(reusecorpus[[1]]),10)\n\n [1] -1509664909 -1764812607 -1837533555 -1776955188 -1785365283 -1551982906\n [7] -1409803751 -2137549872 -1559991213 -2094002709\n\n\nAt this point, you could compare any single document’s sequences of hashes to any other, and get its Jacquard Similarity score, which counts the number of shared hashes in the documents. The more shared hashes, the higher the similarity.\nTo do this over the entire corpus more efficiently, a local Sensitivity Hashing algorithm is used to solve this problem. This groups the representations together, and finds pairs of documents that should be compared for similarity. More details can be found here:\n\nLSH breaks the minhashes into a series of bands comprised of rows. For example, 200 minhashes might broken into 50 bands of 4 rows each. Each band is hashed to a bucket. If two documents have the exact same minhashes in a band, they will be hashed to the same bucket, and so will be considered candidate pairs. Each pair of documents has as many chances to be considered a candidate as their are bands, and the fewer rows there are in each band, the more likely it is that each document will match another.\n\nThe first step in this is to create the buckets. You can try other values for the bands.\n\nbuckets &lt;- lsh(reusecorpus, bands = 50, progress = FALSE)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the textreuse package.\n  Please report the issue at &lt;https://github.com/ropensci/textreuse/issues&gt;.\n\n\nNext, use lsh_candidates() to compare each bucket, and generate a list of candidates.\n\ncandidates &lt;- lsh_candidates(buckets)\n\nNext, go back to the full corpus, and calculate the similarity score for these pairs, using lsh_compare(). The first argument is the candidates, the second is the full corpus, the third is the method (other similarity functions could be used). The output (the first 10 lines are below this code) is a dataframe with three columns, one for each pair of matched documents: the first two columns store the unique IDs for the documents, and the last the similarity score.\n\njacsimilarity_both = lsh_compare(candidates, \n                                 reusecorpus, \n                                 jaccard_similarity, \n                                 progress = FALSE) %&gt;% \n  arrange(desc(score))\n\njacsimilarity_both %&gt;% head(10) %&gt;% kableExtra::kbl()\n\n\n\n\na\nb\nscore\n\n\n\n\narticle_3092\narticle_3249\n1\n\n\narticle_3092\narticle_3291\n1\n\n\narticle_3092\narticle_3321\n1\n\n\narticle_3092\narticle_3369\n1\n\n\narticle_3092\narticle_3393\n1\n\n\narticle_3092\narticle_3407\n1\n\n\narticle_3092\narticle_3533\n1\n\n\narticle_3092\narticle_3666\n1\n\n\narticle_3092\narticle_3706\n1\n\n\narticle_3092\narticle_3711\n1\n\n\n\n\n\n\n\nIn order to see what this means in practice, there are a few more data wrangling steps to carry out.\nFirstly, we’ll merge the matches dataframe to the original sample of articles, using the article code. This means we can see the text and the metadata (the date and newspaper source) for each pair of articles. We can use this information to clean the data and get some more meaningful pairs.\nThe method compares all articles against all other articles, meaning articles from the same newspapers are compared to each other. It also doesn’t have a minimum length, so sometimes very short articles (often just a few letters or punctuation marks, because of OCR and segmentation errors) will be matched with a very high score. We then select just the relevant columns, and filter out those where both documents are from the same newspaper.\n\nmatchedtexts = jacsimilarity_both %&gt;% \n  left_join(sample_for_text_reuse, by = c('a' = 'article_code')) %&gt;% \n  left_join(sample_for_text_reuse, by = c('b' = 'article_code')) %&gt;% \n  select(a,b,score, text.x, text.y, newspaper_id.x, newspaper_id.y, date.x, date.y, newspaper_title.x, newspaper_title.y) %&gt;% \n  filter(!is.na(newspaper_id.x)) %&gt;% \n  mutate(count = str_count(text.x)) %&gt;% \n  filter(count&gt;500) %&gt;% \n  mutate(text.x = paste0(\"&lt;b&gt;\",newspaper_title.x, \"&lt;/b&gt;&lt;br&gt;\", text.x))%&gt;% \n  mutate(text.y = paste0(\"&lt;b&gt;\",newspaper_title.y, \"&lt;/b&gt;&lt;br&gt;\", text.y))\n\nThe resulting file is a dataframe of every matched pair, including the titles and dates."
  },
  {
    "objectID": "text-reuse.html#analysis-of-the-results",
    "href": "text-reuse.html#analysis-of-the-results",
    "title": "14  Text Reuse",
    "section": "Analysis of the results",
    "text": "Analysis of the results\nAs a first step in understanding what exactly has been found, we could take a ‘bird’s eye’ view of the patterns of reuse. If we filter to just include matches with high overlap, and then organise the data so that we only look at matches where one is published at least a day later than the other. With this, we can assume that one text is the ‘source’ text for an article published by the other.\nThis can be visualised using network analysis software. We’ll count the total reuse from one title to another. Each title will be treated as a ‘node’ in a network, and the amount of reuse, going from older to newer, can be visualised as a link, with an arrow pointing from the amount of reuse from the source to the ‘target’. The thickness of the line will be mapped to the volume of reused text. We’ll also filter so that only reuse with a similarity score of at least .3 is considered.\n\n\n\n\n\nLooking at the visualisation, one particularly clear pattern seems to be the reusing of text from The Sun in the Glasgow Courier.\nIn order to compare the two versions, we’ll isolate a single pair, and use ggplot2 to render it in a nicely-formatted text box.\n\ndata = matchedtexts %&gt;% \n  slice(1) %&gt;% \n  pivot_longer(cols = c(text.x, text.y)) %&gt;% \n  mutate(value = str_trunc(value, 1000))\n\nThis code, using the package ggtext, will display the two articles side-by-side:\n\n# Create the plot\nggplot(data, aes(x = name, y = 1, label = value)) +\n  geom_textbox(width = 0.4,\n               box.padding = unit(0.5, \"lines\"), halign = 0.5, size = 3) +\n  coord_cartesian(ylim = c(0, 2)) +\n  theme_void()\n\n\n\n\nWe can see that these two texts are indeed the same, with some small differences attributable to OCR errors.\n\nCheck Local Alignment\nTo check the specific overlap of two documents, use another function from textreuse to check the ‘local alignment’. This is like comparing two documents in Microsoft Word: it finds the bit of the text with the most overlap, and it points out where in this overlap there are different words, replacing them with ######\nFirst turn the text in each cell into a string:\n\na = paste(matchedtexts$text.x[1], sep=\"\", collapse=\"\")\n\nb =  paste(matchedtexts$text.y[1], sep=\"\", collapse=\"\")\n\nCall the align_local() function, giving it the two strings to compare.\n\naligned = align_local(a, b)\n\nAs before, we can use ggplot2 to render the result so that it is easier to read:\n\naligned_df = tibble(texts = c(aligned$a_edits, aligned$b_edits)) %&gt;% \n  mutate(texts = str_trunc(texts, 1000)) %&gt;% \n  mutate(name = c('The Sun', 'Glasgow Courier'))\n\n# Create the plot\nggplot(aligned_df, aes(x = name, y = 1, label = texts)) +\n  geom_textbox(width = 0.4,\n               box.padding = unit(0.5, \"lines\"), halign = 0.5, size = 3) +\n  coord_cartesian(ylim = c(0, 2)) +\n  theme_void()"
  },
  {
    "objectID": "text-reuse.html#next-steps",
    "href": "text-reuse.html#next-steps",
    "title": "14  Text Reuse",
    "section": "Next steps",
    "text": "Next steps\nText reuse is complex, because ultimately it is difficult to know whether is is ‘genuine’ text reuse, or something else such as reuse from a third party, boilerplate text, commonplaces, or famous quotations. In the above example, it’s likely that the true pattern is that The Sun published this news in London along with many other titles at the same time, and the Glasgow Courier may have picked it up from any of them. Perhaps even more so than with other forms of text analysis, you’ll want to really understand the sources and do close reading to fully understand the results.\nYou should experiment with the parameters, mostly the n parameter in the minihash generator, the n in the tokenizer, and the bands in the candidates function."
  },
  {
    "objectID": "text-reuse.html#further-reading",
    "href": "text-reuse.html#further-reading",
    "title": "14  Text Reuse",
    "section": "Further reading",
    "text": "Further reading\nDavid A. Smith and others, Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers, American Literary History, Volume 27, Issue 3, Fall 2015, Pages E1–E15, https://doi.org/10.1093/alh/ajv029\nRyan Cordell, Reprinting, Circulation, and the Network Author in Antebellum Newspapers, American Literary History, Volume 27, Issue 3, Fall 2015, Pages 417–445, https://doi.org/10.1093/alh/ajv028\n\n\n\n\nSmith, David A., Ryan Cordell, and Abby Mullen. 2015. “Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers.” American Literary History 27 (3): E1–15. https://doi.org/10.1093/alh/ajv029."
  },
  {
    "objectID": "further-reading.html#further-reading",
    "href": "further-reading.html#further-reading",
    "title": "15  Final Thoughts",
    "section": "Further Reading",
    "text": "Further Reading\nThere is a huge volume of literature on R, text analysis and newspaper digitisation. This is a small collection of recommended reading.\nA useful list of coding resources: https://scottbot.net/teaching-yourself-to-code-in-dh/\nA book on R specifically for digital humanities: http://dh-r.lincolnmullen.com\nGeocomputation with R - a fantastic introduction to advanced mapping and spatial analysis. https://bookdown.org/robinlovelace/geocompr/\nUse R to write blog posts: https://bookdown.org/yihui/blogdown/\nR-Studio cheatsheets, which are really useful to print out and keep while you’re coding, particularly at the beginning: https://rstudio.com/resources/cheatsheets/\nText mining with R - lots of the examples in this book are based on lessons from here: https://www.tidytextmining.com\nThe best introduction to R and the Tidyverse: https://r4ds.had.co.nz\nA recent report on newspaper digitisation and metadata standards: https://melissaterras.files.wordpress.com/2020/01/selectioncriterianewspapers_hauswedell_nyhan_beals_terras_bell-3.pdf"
  },
  {
    "objectID": "final-thoughts.html",
    "href": "final-thoughts.html",
    "title": "16  Final Thoughts",
    "section": "",
    "text": "Techniques that can be done elsewhere\nNewspapers have very particular makeup, particular type of text (temporality etc)\nHopefully will get more useful as more texts become available."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ahnert, Ruth, Emma Griffin, Mia Ridge, and Giorgia Tolfo. 2023.\n“Collaborative Historical Research in the Age of Big Data,”\nJanuary. https://doi.org/10.1017/9781009175548.\n\n\nBeelen, Kaspar, Jon Lawrence, Daniel C S Wilson, and David Beavan. 2022.\n“Bias and Representativeness in Digitized Newspaper Collections:\nIntroducing the Environmental Scan.” Digital Scholarship in\nthe Humanities 38 (1): 1–22. https://doi.org/10.1093/llc/fqac037.\n\n\nFyfe, Paul. 2016. “An Archaeology of Victorian Newspapers.”\nVictorian Periodicals Review 49 (4): 546–77. https://doi.org/10.1353/vpr.2016.0039.\n\n\nHarris, P. R. 1998. A History of the British\nMuseum Library, 1753-1973. London: British Library.\n\n\nKing, Ed. 2007. “Digitisation of British Newspapers\n1800-1900.” https://www.gale.com/intl/essays/ed-king-digitisation-of-british-newspapers-1800-1900.\n\n\n———. 2008. “British Library Digitisation: Access and\nCopyright.”\n\n\nMussell, James. 2014. “Elemental Forms: Elemental Forms: The\nNewspaper as Popular Genre in the Nineteenth Century.” Media\nHistory 20 (1): 4–20. https://doi.org/10.1080/13688804.2014.880264.\n\n\nPrescott, Andrew. 2018. “Travelling Chronicles: News and\nNewspapers from the Early Modern Period to the Eighteenth\nCentury.” In, edited by Siv Gøril Brandtzæg, Paul Goring, and\nChristine Watson, 51–71. Leiden, The Netherlands: Brill.\n\n\nRyan, Yann, and Luke McKernan. 2021. “Converting the British\nLibrary’s Catalogue of British and Irish Newspapers into a\nPublic Domain Dataset: Processes and Applications.” Journal\nof Open Humanities Data 7. https://doi.org/10.5334/johd.23.\n\n\nShaw, Jane. 2005. “10 Billion Words: The British Library British\nNewspapers 1800-1900 Project: Some Guidelines for Large-Scale Newspaper\nDigitisation.” https://archive.ifla.org/IV/ifla71/papers/154e-Shaw.pdf.\n\n\n———. 2007. “Selection of Newspapers.” British Library\nNewspapers. https://www.gale.com/intl/essays/jane-shaw-selection-of-newspapers.\n\n\nSmith, David A., Ryan Cordell, and Abby Mullen. 2015.\n“Computational Methods for Uncovering\nReprinted Texts in Antebellum\nNewspapers.” American Literary History 27\n(3): E1–15. https://doi.org/10.1093/alh/ajv029.\n\n\nSmits, Thomas. 2016. “Making the News\nNational: Using Digitized\nNewspapers to Study the\nDistribution of the Queen’s\nSpeech by W. H.\nSmith & Son, 1846–1858.”\nVictorian Periodicals Review 49 (4): 598–625. https://doi.org/10.1353/vpr.2016.0041.\n\n\nTolfo, Giorgia, Olivia Vane, Kaspar Beelen, Kasra Hosseini, Jon\nLawrence, David Beavan, and Katherine McDonough. 2022. “Hunting\nfor Treasure: Living with Machines and the British Library Newspaper\nCollection.” In, 23–46. De Gruyter. https://doi.org/10.1515/9783110729214-002.\n\n\nWevers, Melvin. 2019. “Using Word Embeddings to Examine Gender\nBias in Dutch Newspapers, 1950-1990.” https://doi.org/10.48550/ARXIV.1907.08922.\n\n\nWevers, Melvin, and Marijn Koolen. 2020. “Digital\nBegriffsgeschichte: Tracing Semantic Change Using Word\nEmbeddings.” Historical Methods: A Journal of Quantitative\nand Interdisciplinary History 53 (4): 226–43. https://doi.org/10.1080/01615440.2020.1760157."
  }
]