{
  "hash": "41d9d8e7032b63f8f1df0eadb200f37c",
  "result": {
    "markdown": "# Machine Learning with Tidymodels {#sec-tidymodels}\n\nThis chapter looks at machine learning using historical newspapers and the 'tidymodels' framework. We'll build a classifier which learns, from some pre-labelled examples, to differentiate between advertisements and news articles. We'll evaluate the model, and fine-tune its parameters. This chapter is heavily indebted to existing tidymodels tutorials on the web, such as the one from [Supervised Machine Learning in R](https://smltar.com/mlclassification.html) by Emil Hvitfeldt and Julia Silge.\n\n## Machine Learning\n\nMachine learning is the name for a group of techniques which take input data of some kind, and learn how to achieve some particular goal. The 'deep learning' used by neural networks and in particular things like ChatGPT are one type, but the field has been around for much longer than that.\n\nMachine learning itself can be divided into subsets: Machine learning done with neural networks, and what we might call 'classical' machine learning, which use algorithms.\n\nA very simple form of machine learning is *linear regression*. Linear regression attempts to find the best fitting line through a dataset. Take this dataset of the flipper size and body mass of a group of observed penguins (from the R package `palmerpenguins`:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nBy visually inspecting this, we can guess that there is a statistical relationship between the length and mass: as one value gets higher, the other does too. We can use `geom_smooth()` and a linear model to predict the best line of fit through this dataset:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf we measured the slope of this line (using simple geometry), its angle would tell us about the relationship between the two values.In this way, our model helps us to understand some underlying pattern in the dataset - that these two values are highly correlated. We could also use it to *predict* new values: if given the body mass for a new penguin, by using this line, we could predict its most likely flipper length, and in most cases be fairly accurate.\n\nThis is a form of simple machine learning. We give an algorithm (here, linear regression) a bunch of input data, in this case the body mass and bill flipper length for a group of penguins, and it provides a model (in this case, a line with a certain slope), which hopefully helps us to explain the existing data and predict unknown parts. Models like this try in some way to minimise a loss function. In this case, finding the line where the error for each point (how far away it is from the line) is as small as possible.\n\nThe machine learning here will use basically the same principle. We'll give it data (text, in the form of a mathematical representation of the text, and its assigned category), and an algorithm (in this case, a 'Random Forest' model), and use that to predict the category of unseen texts. The model itself may also tell us something about the existing data.\n\nMuch of the AI work at the cutting edge of text analysis today uses neural networks, and in particular the 'transformer' mechanism which allows neural networks to better understand context and word order. But this kind of machine learning still has a very important role to play, it's well-established, relatively easy to run, and the results can be quite good depending on the problem. It is also a good way to get started with understanding the Tidymodels framework.\n\n## The Tidymodels Package\n\nTidymodels is an R 'meta-package' which allows you to interact with lots of different machine learning packages and engines in a consistent way. Using tidymodels, we can easily compare multiple models together and we can swap in one for another without having to re-do code. We also can make sure that our pre-processing steps are precisely consistent across any number of different models.\n\nMachine learning can be used for a number of different tasks. One key one is text classification.\n\nIn this tutorial, we will use tidymodels to classify text into articles and advertisements. It could easily be generalised to any number of categories, for example foreign news, court reporting, and so forth. You can provide your own spreadsheet of training data (as we'll use below), and as long as it is in the same format and has similar information, you should be able to build your own classifier.\n\nOnce we have built the model, we will fine-tune it. The random forest algorithm we'll use, like most machine learning algorithms, has a group of parameters which can be adjusted. To find the best values for these, we'll evaluate the same data using many different combinations of parameters, and pick the best one. Using tidymodels, this can all be done consistently. We could even swap out any other model type, and otherwise reuse the exact same workflow.\n\nFinally, we'll put the model to some use: we'll use the model to predict the class of the rest of the articles in the newspaper dataset, and do some analysis on this.\n\nThe model is only as good as the training data, and in this case, we don't have many examples, and it won't be terribly accurate in many cases. But it will show how a machine learning model can be operationalised for this task.\n\n### Basic Steps\n\nIn this chapter we'll create a model which can label newspaper as either articles or advertisements. We'll do some further steps to explore improving the model, and also to look at the most important 'features' used by the model to do its predicting. The steps are:\n\n1.  Download a labelled dataset, containing examples of our two classes (articles and advertisements).\n\n2.  Create a 'recipe' which is a series of pre-processing steps. This same recipe can be reused in different contexts and for different models.\n\n3.  Split the labelled data into testing and training sets. The training data is used to fit the best model. The test set is used at the end, to see how well it performs on unseen data.\n\n4.  Run an initial classifier and evaluate the results\n\n5.  'Tune' the model, by re--running the classifier with different parameters, selecting the best one.\n\n6.  Run the 'best' model over the full news dataset, predicting whether or not an articles is news or an advertisement.\n\n## Install/load the packages\n\nAs a first step, you'll need to load the necessary packages for this tutorial. If you haven't already done so, you can install them first using the below.\n\nIf you have installed them, make sure you update to the latest version, as they can change rapidly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages('tidyverse')\ninstall.packages('tidymodels')\ninstall.packages('textrecipes')\ninstall.packages('tidytext')\ninstall.packages('ranger')\ninstall.packages('vip')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(tidytext)\nlibrary(ranger)\nlibrary(vip)\n```\n:::\n\n\n## Import data\n\nAs a first step, load some pre-labelled data. This contains a number of newspaper articles, and their 'type': whether it is an advertisement or an article.\n\nIf we take a look at the dataframe once it is loaded, you'll see it's quite simple structure: it's got a 'filename' column, the full text of the article store in 'text', and the type stored in a column called 'type'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertisements_labelled = read_csv('advertisements_labelled.csv')\n\nadvertisements_labelled = advertisements_labelled %>% filter(!is.na(text))\n```\n:::\n\n\n## \n\n## Set up the Machine Learning Model\n\nIn this step, we begin preparing the data for machine learning. We set a seed for reproducibility to ensure consistent results when randomization is involved. The target variable 'type' in the advertisements data is converted to a factor as it represents categorical classes ('advertisement' and 'article').\n\nThe data is then split into training and testing sets using the `initial_split` function. This separation is crucial for evaluating the performance of the machine learning model and preventing overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9999)\nadvertisements_labelled = advertisements_labelled %>% \n  mutate(type = factor(type))\n\nadvertisements_split <- initial_split(advertisements_labelled, strata = type)\n\nadvertisements_train <- training(advertisements_split)\nadvertisements_test <- testing(advertisements_split)\n```\n:::\n\n\n## Create Recipe for Text Data\n\nTo prepare the text data for modeling, we create a recipe using the `recipe` function. The `textrecipes` package provides essential tools for text preprocessing and feature extraction in machine learning. In this recipe, we tokenize the text, remove stop words, and apply a term frequency transformation to represent the text data as numerical features. These transformations convert the raw text data into a format suitable for machine learning algorithms, enabling them to process and understand textual information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertisement_rec <-\n  recipe(type ~ text, data = advertisements_train)\n\nadvertisement_rec <- advertisement_rec %>%\n  step_tokenize(text, token = \"words\") %>%\n  step_tokenfilter(text, max_tokens = 1000, min_times = 5 )  %>% \n  step_tf(text)\n```\n:::\n\n\nNext, we set up a `workflow()` object, which will store the recipe and later the model instructions, and make it easier to reuse.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertisement_wf <- workflow() %>%\n  add_recipe(advertisement_rec)\n```\n:::\n\n\nCreate the model. In this case, we'll use a random forest model, from the package `ranger`. Setting the `importance = \"impurity\"` parameter means we'll be able to see what words the model used to make its decisions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(\"classification\") %>%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n```\n:::\n:::\n\n\nTake the workflow object we made above, and add the model to it. After this, use `fit` to run the model, specifying it should use the `advertisements_train` dataset we created above.\n\nFinally, we use `predict` on this fitted model, specifying `advertisements_test` as the dataset. Add the true labels from `advertisements_test` as a new column, and use `accuracy()` to compare the true label with the prediction to get an accuracy score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n advertisement_wf %>%\n  add_model(rf_spec) %>%\n  fit(data = advertisements_train)%>%\n  predict(new_data = advertisements_test) %>%\n  mutate(truth = advertisements_test$type) %>%\n  accuracy(truth, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.811\n```\n:::\n:::\n\n\n## Perform Cross-Validation\n\nTo evaluate the model further, we'll use cross-validation. Cross-validation is a crucial step in model evaluation. It helps assess the model's generalization performance on unseen data and reduces the risk of overfitting.\n\nIn this step, we set up cross-validation folds using the `vfold_cv` function, which creates multiple training and testing sets from the training data. The model will be trained and evaluated on each fold separately, providing a more robust estimate of its performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nadvertisements_folds <- vfold_cv(advertisements_train)\n```\n:::\n\n\n## Train the Random Forest Model with Cross-Validation\n\nNow, we train the Random Forest model using cross-validation. The `fit_resamples` function fits the model to each fold created during cross-validation, allowing us to evaluate its performance across different subsets of the training data. The `control_resamples` function is used to control various settings during the resampling process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_wf <- workflow() %>%\n  add_recipe(advertisement_rec) %>%\n  add_model(rf_spec)\n\nrf_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs <- fit_resamples(\n  rf_wf,\n  advertisements_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n\n## Evaluate the Model's Performance\n\nIn this step, we collect the evaluation metrics and predictions from the cross-validation process. The collected metrics will help us assess the model's performance, while the predictions on each fold will be used for further analysis and comparison. By evaluating the model on multiple subsets of the data, we can gain insights into its robustness and reliability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs_metrics <- collect_metrics(rf_rs)\nrf_rs_predictions <- collect_predictions(rf_rs)\n```\n:::\n\n\n## Visualize the Confusion Matrix\n\nThe confusion matrix is a useful visualization for evaluating the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. The `autoplot` function from the `yardstick` package allows us to visualize the confusion matrix as a heatmap. This visualization aids in understanding the model's classification accuracy and any potential misclassifications.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(rf_rs, tidy = FALSE) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWe can see that there are very few true advertisements which are misclassified as articles - but there are some some articles misclassified as advertisements.\n\n## Tune the Random Forest Model\n\nIn machine learning, hyperparameter tuning is essential for optimizing model performance. In this step, we define a tuning grid using the `rand_forest` function. The grid specifies different combinations of hyperparameters, such as the number of variables randomly sampled for splitting (`mtry`) and the minimum number of samples per leaf node (`min_n`). We aim to find the best combination of hyperparameters that yields the highest performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec <- rand_forest(\n  mtry = tune(),\n  trees = 1000,\n  min_n = tune()\n) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"ranger\", importance = \"impurity\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_wf <- workflow() %>%\n  add_recipe(advertisement_rec) %>%\n  add_model(tune_spec)\n```\n:::\n\n\n## Tune the Random Forest Model with Cross-Validation\n\nNow, we perform hyperparameter tuning using cross-validation. The `tune_grid` function uses the tuning grid specified earlier and fits the model on each fold of the data to identify the optimal hyperparameters. This process helps us identify the best hyperparameters for the Random Forest model, leading to improved performance and better generalization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(555)\ntrees_folds = vfold_cv(advertisements_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nset.seed(666)\ntune_res = tune_grid(\n  tune_wf,\n  resamples = trees_folds,\n  grid = 20\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n\n```{.r .cell-code}\ntune_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits          id     .metrics          .notes          \n   <list>          <chr>  <list>            <list>          \n 1 <split [95/11]> Fold01 <tibble [40 × 6]> <tibble [0 × 3]>\n 2 <split [95/11]> Fold02 <tibble [40 × 6]> <tibble [0 × 3]>\n 3 <split [95/11]> Fold03 <tibble [40 × 6]> <tibble [0 × 3]>\n 4 <split [95/11]> Fold04 <tibble [40 × 6]> <tibble [0 × 3]>\n 5 <split [95/11]> Fold05 <tibble [40 × 6]> <tibble [0 × 3]>\n 6 <split [95/11]> Fold06 <tibble [40 × 6]> <tibble [0 × 3]>\n 7 <split [96/10]> Fold07 <tibble [40 × 6]> <tibble [0 × 3]>\n 8 <split [96/10]> Fold08 <tibble [40 × 6]> <tibble [0 × 3]>\n 9 <split [96/10]> Fold09 <tibble [40 × 6]> <tibble [0 × 3]>\n10 <split [96/10]> Fold10 <tibble [40 × 6]> <tibble [0 × 3]>\n```\n:::\n:::\n\n\n## Visualize the Tuning Results\n\nTo visualize the tuning results, we plot the average Area Under the Receiver Operating Characteristic Curve (ROC AUC) against different values of `mtry` and `min_n`. ROC AUC is a common metric for assessing the model's ability to discriminate between classes. The plot provides insights into how different hyperparameter values affect the model's performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, min_n, mtry) %>%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nIt's a little difficult to interpret, but it looks like the highest values are between about 40 and 250 for the `mtry` value, and around 10 and 30 for the `min_n` value. We can do another grid search, this time just looking between these values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_grid <- grid_regular(\n  mtry(range = c(40, 250)),\n  min_n(range = c(10,30)),\n  levels = 5\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\nregular_res <- tune_grid(\n  tune_wf,\n  resamples = trees_folds,\n  grid = rf_grid\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nregular_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(min_n = factor(min_n)) %>%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"AUC\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_auc <- select_best(regular_res, \"roc_auc\")\n\nfinal_rf <- finalize_model(\n  tune_spec,\n  best_auc\n)\n```\n:::\n\n\nLastly, evaluate the accuracy using the same method as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertisement_wf %>%\n  add_model(final_rf) %>%\n  fit(data = advertisements_train)%>%\n  predict(new_data = advertisements_test) %>%\n  mutate(truth = advertisements_test$type) %>%\n  accuracy(truth, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.865\n```\n:::\n:::\n\n\nAs you can see, we have improved the model's performance, from about .80 to about .86. There are other ways you could try to improve the performance further. With text, the pre-processing steps can often make a huge difference. You could experiment with the text 'recipe', for instance adjusting `step_tokenfilter` to include or remove more tokens. You could also use `step_word_embeddings`, using the output of the word embeddings in [Chapter -@sec-word2vec].\n\n## Build the Final Random Forest Model\n\nIn this step, we build the final Random Forest model using the best hyperparameters obtained from tuning. The model is then fitted on the entire training dataset to capture the relationships between features and target classes optimally. This final model is the one that we will use for making predictions on new data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- workflow() %>%\n  add_recipe(advertisement_rec) %>%\n  add_model(final_rf)\n\nfinal_res <- final_wf %>%\n  last_fit(advertisements_split)\n\nfinal_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.865 Preprocessor1_Model1\n2 roc_auc  binary         0.888 Preprocessor1_Model1\n```\n:::\n:::\n\n\n## What features is the model using?\n\nTo understand how the model is using the text to make decisions, we can look at the most important features (in this case, text frequency counts) used by the random forest algorithm. We'll use the package `vip` to extract the most important features, using the function `extract_fit_parsnip()`, and plotting it using `ggplot2`.\n\nFor the random forest method, we can only see the overall top features, and not which were more important for the prediction of the different categories. The most important feature is the frequency of the word 'street': at a guess, this is used more often in advertisements, which very often contain an address to a business or service. Interestingly, the second most important feature is the word 'was'. Is this perhaps because news articles are more likely to use the past tense than advertisements? Interestingly, the frequency of certain function words such as `the`, `and`, `to`, `of` (which would often be filtered out as 'stop words') also seems to be important to the model. One explanation is that the use of these words is quite different in prose and in the kind of text used in advertisements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplaints_imp <- extract_fit_parsnip(final_res$.workflow[[1]]) %>%\n  vi(lambda = choose_acc$penalty)\n\ncomplaints_imp %>% \n  mutate(Variable = str_remove(Variable, \"tfidf_text_\")) %>% \n  head(20) %>% \n  ggplot() + \n  geom_col(aes(x = reorder(Variable,Importance), y= Importance)) + coord_flip()\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## Using the Model\n\nOnce we are happy with the final model, we can use it to label unseen data as either articles or advertisements. For this, we'll need a newspaper corpus. As in previous chapters, either construct your own corpus by following [Chapter -@sec-download] and [Chapter -@sec-extract], or [download](https://doi.org/10.5281/zenodo.8262355) and open the ready-made .zip file with all issues from 1855. Next, get these articles into the correct format. See [Chapter -@sec-count] for an explanation of this code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnews_sample_dataframe = list.files(path = \"newspaper_text/\", \n                                   pattern = \"csv\", \n                                   recursive = TRUE, \n                                   full.names = TRUE)\n\n\nall_files = lapply(news_sample_dataframe, data.table::fread) \n\nnames(all_files) = news_sample_dataframe\n\nall_files_df = data.table::rbindlist(all_files, idcol = 'filename')\n    \n\ntitle_names_df = tibble(newspaper_id = c('0002090', '0002194', '0002244', '0002642', '0002645', '0003089', '0002977'), newspaper_title = c('The Liverpool Standard And General Commercial Advertiser', 'The Sun', 'Colored News', 'The Express', 'The Press', 'Glasgow Courier', 'Swansea and Glamorgan Herald'))\n\nnews_df = all_files_df %>% \n  mutate(filename = basename(filename))\n\n\nnews_df = news_df %>% \n  separate(filename, \n           into = c('newspaper_id', 'date'), sep = \"_\") %>% # separate the filename into two columns\n  mutate(date = str_remove(date, \"\\\\.csv\")) %>% # remove .csv from the new data column\n  select(newspaper_id, date, art, text) %>% \n  mutate(date = ymd(date)) %>% # turn the date column into date format\n  mutate(article_code = 1:n()) %>% # give every article a unique code\n  select(article_code, everything()) %>% # select all columns but with the article code first \n  left_join(title_names_df, by = 'newspaper_id') # join the titles \n```\n:::\n\n\n## Make Predictions on Newspaper Articles\n\nWith the final Random Forest model trained, we proceed to make predictions on the newspaper articles' text data. This involves applying the text preprocessing steps (e.g., tokenization, TF-IDF) used during training to transform the new data into the same format. The model then predicts whether each article is an advertisement or not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_ads_to_check = final_wf %>%\n  fit(data = advertisements_train)%>%\n  predict(new_data = news_df)\n\nall_files_df = all_files_df %>% \n  mutate(prediction =new_ads_to_check$.pred_class)\n```\n:::\n\n\n## Analyze the Top Words in Advertisements\n\nAfter making predictions, we perform text analysis to identify the top words associated with advertisements. We tokenize the text data, count the occurrences of each word for each prediction, and select the most frequent words in advertisements. This analysis allows us to gain insights into the language patterns characteristic of advertisements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata('stop_words')\n\n\nall_files_df %>% \n  head(10000) %>% \n  unnest_tokens(word, text) %>% \n  filter(!str_detect(word, \"[0-9]\")) %>% \n  anti_join(stop_words) %>% \n  count(prediction, word) %>% \n  group_by(prediction) %>% \n  slice_max(order_by = n, n = 10) %>% ungroup() %>% \n    mutate(prediction = as.factor(prediction),\n           word = reorder_within(word, n, prediction)) %>%\n  ggplot() + geom_col(aes(word, n)) + \n  facet_wrap(~prediction, scales = 'free') +\n    scale_x_reordered()+\n    scale_y_continuous(expand = c(0,0))+ coord_flip()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nAdvertisements often contain an address, including the word 'street', unlike articles. This matches the findings from the top features, above. Advertisements also often specify an exact time and date, resulting in higher counts of the word 'o'clock'.\n\n## Analyze the Proportion of Advertisements in Each Newspaper Issue\n\nFinally, we analyze the proportion of advertisements in each newspaper issue. We group the articles by their `newspaper_id` and `date`, calculate the proportion of advertisements in each issue, and then compute the mean proportion for each newspaper. This analysis helps us understand the prevalence of advertisements in different newspapers over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnews_df$type = new_ads_to_check$.pred_class\n\nnews_df =news_df  %>% mutate(count = str_count(text))\n\nnews_df =news_df  %>% mutate(issue_code = paste0(newspaper_id, \"_\", date))\n\nnews_df %>% \n  group_by(newspaper_title, issue_code, type) %>%\n  summarise(n = sum(count)) %>%\n  mutate(prop = n/sum(n)) %>% \n  group_by(newspaper_title, type) %>% \n  summarise(mean_prop = mean(prop)) %>% \n  ggplot() + \n  geom_col(aes(x = str_trunc(newspaper_title,30), y = mean_prop, fill = type)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'newspaper_title', 'issue_code'. You can\noverride using the `.groups` argument.\n`summarise()` has grouped output by 'newspaper_title'. You can override using\nthe `.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nInterestingly, some of the regional titles (Liverpool, Glasgow, Swansea and Glamorgan) seem to have much higher proportions of advertisements - provided the labelling by the machine learning model is largely correct, of course.\n\n## Recommended Reading\n\n[Supervised Machine Learning for Text Analysis in R](https://smltar.com/)\n\nBroersma, Marcel, and Frank Harbers. \"Exploring Machine Learning to Study the Long-Term Transformation of News.\" In *Journalism History and Digital Archives*, edited by Henrik Bødker, 1st ed., 38--52. Routledge, 2020. https://doi.org/10.4324/9781003098843-4.\n",
    "supporting": [
      "tidymodels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}